
@misc{carrel_interpolatory_2025,
	title = {Interpolatory {Dynamical} {Low}-{Rank} {Approximation}: {Theoretical} {Foundations} and {Algorithms}},
	shorttitle = {Interpolatory {Dynamical} {Low}-{Rank} {Approximation}},
	url = {http://arxiv.org/abs/2510.19518},
	doi = {10.48550/arXiv.2510.19518},
	abstract = {Dynamical low-rank approximation (DLRA) is a widely used paradigm for solving large-scale matrix differential equations, as they arise, for example, from the discretization of time-dependent partial differential equations on tensorized domains. Through orthogonally projecting the dynamics onto the tangent space of a low-dimensional manifold, DLRA achieves a significant reduction of the storage required to represent the solution. However, the need for evaluating the velocity field can make it challenging to attain a corresponding reduction of computational cost in the presence of nonlinearities. In this work, we address this challenge by replacing orthogonal tangent space projections with oblique, data-sparse projections selected by a discrete empirical interpolation method (DEIM). At the continuous-time level, this leads to DLRA-DEIM, a well-posed differential inclusion (in the Filippov sense) that captures the discontinuities induced by changes in the indices selected by DEIM. We establish an existence result, exactness property and error bound for DLRA-DEIM that match existing results for DLRA. For the particular case of QDEIM, a popular variant of DEIM, we provide an explicit convex-polytope characterization of the differential inclusion. Building on DLRA-DEIM, we propose a new class of projected integrators, called PRK-DEIM, that combines explicit Runge--Kutta methods with DEIM-based projections. We analyze the convergence order of PRK-DEIM and show that it matches the accuracy of previously proposed projected Runge-Kutta methods, while being significantly cheaper. Extensions to exponential Runge--Kutta methods and low-order tensor differential equations demonstrate the versatility of our framework.},
	urldate = {2025-11-03},
	publisher = {arXiv},
	author = {Carrel, Benjamin and Kressner, Daniel and Lam, Hei Yin and Vandereycken, Bart},
	month = oct,
	year = {2025},
	note = {arXiv:2510.19518 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/WFE3HLLQ/Carrel et al. - 2025 - Interpolatory Dynamical Low-Rank Approximation Theoretical Foundations and Algorithms.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/8K5R65FA/2510.html:text/html;Snapshot:/Users/goessmann/Zotero/storage/8XJJIG5G/2510.html:text/html},
}

@article{cichocki_era_2014,
	title = {Era of {Big} {Data} {Processing}: {A} {New} {Approach} via {Tensor} {Networks} and {Tensor} {Decompositions}},
	shorttitle = {Era of {Big} {Data} {Processing}},
	abstract = {Many problems in computational neuroscience, neuroinformatics, pattern/image recognition, signal processing and machine learning generate massive amounts of multidimensional data with multiple aspects and high dimensionality. Tensors (i.e., multi-way arrays) provide often a natural and compact representation for such massive multidimensional data via suitable low-rank approximations. Big data analytics require novel technologies to efficiently process huge datasets within tolerable elapsed times. Such a new emerging technology for multidimensional big data is a multiway analysis via tensor networks (TNs) and tensor decompositions (TDs) which represent tensors by sets of factor (component) matrices and lower-order (core) tensors. Dynamic tensor analysis allows us to discover meaningful hidden structures of complex data and to perform generalizations by capturing multi-linear and multi-aspect relationships. We will discuss some fundamental TN models, their mathematical and graphical descriptions and associated learning algorithms for large-scale TDs and TNs, with many potential applications including: Anomaly detection, feature extraction, classification, cluster analysis, data fusion and integration, pattern recognition, predictive modeling, regression, time series analysis and multiway component analysis. Keywords: Large-scale HOSVD, Tensor decompositions, CPD, Tucker models, Hierarchical Tucker (HT) decomposition, low-rank tensor approximations (LRA), Tensorization/Quantization, tensor train (TT/QTT) - Matrix Product States (MPS), Matrix Product Operator (MPO), DMRG, Strong Kronecker Product (SKP).},
	urldate = {2019-03-16},
	journal = {arXiv:1403.2048 [cs]},
	author = {Cichocki, Andrzej},
	month = mar,
	year = {2014},
	keywords = {Check: Tensor Decomposition, Computer Science - Emerging Technologies},
	annote = {arXiv: 1403.2048},
}

@article{gels_multidimensional_2019,
	title = {Multidimensional {Approximation} of {Nonlinear} {Dynamical} {Systems}},
	volume = {14},
	issn = {1555-1415},
	doi = {10.1115/1.4043148},
	abstract = {A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.},
	number = {6},
	urldate = {2019-05-01},
	journal = {Journal of Computational and Nonlinear Dynamics},
	author = {Gelß, Patrick and Klus, Stefan and Eisert, Jens and Schütte, Christof},
	month = apr,
	year = {2019},
	pages = {061006--061006--12},
}

@article{chandrasekaran_convex_2012,
	title = {The {Convex} {Geometry} of {Linear} {Inverse} {Problems}},
	volume = {12},
	issn = {1615-3375, 1615-3383},
	doi = {10.1007/s10208-012-9135-7},
	language = {en},
	number = {6},
	urldate = {2019-08-01},
	journal = {Foundations of Computational Mathematics},
	author = {Chandrasekaran, Venkat and Recht, Benjamin and Parrilo, Pablo A. and Willsky, Alan S.},
	month = dec,
	year = {2012},
	pages = {805--849},
}

@article{de_silva_tensor_2008,
	title = {Tensor {Rank} and the {Ill}-{Posedness} of the {Best} {Low}-{Rank} {Approximation} {Problem}},
	volume = {30},
	issn = {0895-4798, 1095-7162},
	doi = {10.1137/06066518X},
	abstract = {There has been continued interest in seeking a theorem describing optimal low-rank approximations to tensors of order 3 or higher, that parallels the Eckart–Young theorem for matrices. In this paper, we argue that the naive approach to this problem is doomed to failure because, unlike matrices, tensors of order 3 or higher can fail to have best rank-r approximations. The phenomenon is much more widespread than one might suspect: examples of this failure can be constructed over a wide range of dimensions, orders and ranks, regardless of the choice of norm (or even Br`egman divergence). Moreover, we show that in many instances these counterexamples have positive volume: they cannot be regarded as isolated phenomena. In one extreme case, we exhibit a tensor space in which no rank-3 tensor has an optimal rank-2 approximation. The notable exceptions to this misbehavior are rank-1 tensors and order-2 tensors (i.e. matrices).},
	language = {en},
	number = {3},
	urldate = {2019-09-17},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {de Silva, Vin and Lim, Lek-Heng},
	month = jan,
	year = {2008},
	pages = {1084--1127},
}

@article{grasedyck_hierarchical_2010,
	title = {Hierarchical {Singular} {Value} {Decomposition} of {Tensors}},
	volume = {31},
	doi = {10.1137/090764189},
	abstract = {We define the hierarchical singular value decomposition (SVD) for tensors of order d ≥ 2. This hierarchical SVD has properties like the matrix SVD (and collapses to the SVD in d = 2), and we prove these. In particular, one can find low rank (almost) best approximations in a hierarchical format (H-Tucker) which requires only O((d - 1)k3 + dnk) parameters, where d is the order of the tensor, n the size of the modes, and k the (hierarchical) rank. The H-Tucker format is a specialization of the Tucker format and it contains as a special case all (canonical) rank k tensors. Based on this new concept of a hierarchical SVD we present algorithms for hierarchical tensor calculations allowing for a rigorous error analysis. The complexity of the truncation (finding lower rank approximations to hierarchical rank k tensors) is in O((d-1)k4+dnk2) and the attainable accuracy is just 2-3 digits less than machine precision.},
	journal = {SIAM J. Matrix Analysis Applications},
	author = {Grasedyck, Lars},
	month = jan,
	year = {2010},
	pages = {2029--2054},
}

@book{foucart_mathematical_2013,
	series = {Applied and {Numerical} {Harmonic} {Analysis}},
	title = {A {Mathematical} {Introduction} to {Compressive} {Sensing}},
	isbn = {978-0-8176-4947-0},
	url = {https://www.springer.com/de/book/9780817649470},
	abstract = {At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. With only moderate prerequisites, it is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject. A Mathematical Introduction to Compressive Sensing uses a mathematical perspective to present the core of the theory underlying compressive sensing.},
	language = {en},
	urldate = {2019-12-11},
	publisher = {Birkhäuser Basel},
	author = {Foucart, Simon and Rauhut, Holger},
	year = {2013},
	doi = {10.1007/978-0-8176-4948-7},
}

@article{beylkin_algorithms_2005,
	title = {Algorithms for {Numerical} {Analysis} in {High} {Dimensions}},
	volume = {26},
	issn = {1064-8275, 1095-7197},
	doi = {10.1137/040604959},
	abstract = {Nearly every numerical analysis algorithm has computational complexity that scales exponentially in the underlying physical dimension. The separated representation, introduced previously, allows many operations to be performed with scaling that is formally linear in the dimension. In this paper we further develop this representation by (i) discussing the variety of mechanisms that allow it to be surprisingly eﬃcient; (ii) addressing the issue of conditioning; (iii) presenting algorithms for solving linear systems within this framework; and (iv) demonstrating methods for dealing with antisymmetric functions, as arise in the multiparticle Schr¨odinger equation in quantum mechanics. Numerical examples are given.},
	language = {en},
	number = {6},
	urldate = {2019-12-11},
	journal = {SIAM Journal on Scientific Computing},
	author = {Beylkin, Gregory and Mohlenkamp, Martin J.},
	month = jan,
	year = {2005},
	pages = {2133--2159},
}

@article{holtz_manifolds_2012,
	title = {On manifolds of tensors of fixed {TT}-rank},
	volume = {120},
	issn = {0029-599X, 0945-3245},
	url = {http://link.springer.com/10.1007/s00211-011-0419-7},
	doi = {10.1007/s00211-011-0419-7},
	abstract = {Recently, the format of TT tensors (Hackbusch and Kühn in J Fourier Anal Appl 15:706–722, 2009; Oseledets in SIAM J Sci Comput 2009, submitted; Oseledets and Tyrtyshnikov in SIAM J Sci Comput 31:5, 2009; Oseledets and Tyrtyshnikov in Linear Algebra Appl 2009, submitted) has turned out to be a promising new format for the approximation of solutions of high dimensional problems. In this paper, we prove some new results for the TT representation of a tensor U ∈ Rn1×···×nd and for the manifold of tensors of TT-rank r . As a ﬁrst result, we prove that the TT (or compression) ranks ri of a tensor U are unique and equal to the respective separation ranks of U if the components of the TT decomposition are required to fulﬁl a certain maximal rank condition. We then show that the set T of TT tensors of ﬁxed rank r locally forms an embedded manifold in Rn1×···×nd , therefore preserving the essential theoretical properties of the Tucker format, but often showing an improved scaling behaviour. Extending a similar approach for matrices (Conte and Lubich in M2AN 44:759, 2010), we introduce certain gauge conditions to obtain a unique representation of the tangent space TU T of T and deduce a local parametrization of the TT manifold. The parametrisation of TU T is often crucial for an algorithmic treatment of high-dimensional time-dependent PDEs and minimisation problems (Lubich in From quantum to classical molecular dynamics: reduced methods and numerical analysis, 2008). We conclude with remarks on those applications and present some numerical examples.},
	language = {en},
	number = {4},
	urldate = {2019-12-18},
	journal = {Numerische Mathematik},
	author = {Holtz, Sebastian and Rohwedder, Thorsten and Schneider, Reinhold},
	month = apr,
	year = {2012},
	pages = {701--731},
}

@incollection{stoudenmire_supervised_2016,
	title = {Supervised {Learning} with {Tensor} {Networks}},
	urldate = {2020-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Stoudenmire, Edwin and Schwab, David J},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4799--4807},
}

@book{hackbusch_tensor_2012,
	address = {Berlin Heidelberg},
	series = {Springer {Series} in {Computational} {Mathematics}},
	title = {Tensor {Spaces} and {Numerical} {Tensor} {Calculus}},
	isbn = {978-3-642-28026-9},
	abstract = {Special numerical techniques are already needed to deal with nxn matrices for large n.Tensor data are of size nxnx...xn=nˆd, where nˆd exceeds the computer memory by far. They appear for problems of high spatial dimensions. Since standard methods fail, a particular tensor calculus is needed to treat such problems. The monograph describes the methods how tensors can be practically treated and how numerical operations can be performed. Applications are problems from quantum chemistry, approximation of multivariate functions, solution of pde, e.g., with stochastic coefficients, etc. ​},
	language = {en},
	urldate = {2020-01-30},
	publisher = {Springer-Verlag},
	author = {Hackbusch, Wolfgang},
	year = {2012},
	doi = {10.1007/978-3-642-28027-6},
}

@book{shalev-schwartz_shai_understanding_2014,
	address = {New York, NY, USA},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-05713-5},
	shorttitle = {Understanding {Machine} {Learning}},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
	language = {Englisch},
	publisher = {Cambridge University Press},
	author = {{Shalev-Schwartz, Shai} and {Ben-David, Shai}},
	month = jul,
	year = {2014},
}

@book{talagrand_upper_2014,
	address = {Berlin, Heidelberg},
	title = {Upper and {Lower} {Bounds} for {Stochastic} {Processes}: {Modern} {Methods} and {Classical} {Problems}},
	isbn = {978-3-642-54074-5},
	shorttitle = {Upper and {Lower} {Bounds} for {Stochastic} {Processes}},
	abstract = {The book develops modern methods and in particular the "generic chaining" to bound stochastic processes. This methods allows in particular to get optimal bounds for Gaussian and Bernoulli processes. Applications are given to stable processes, infinitely divisible processes, matching theorems, the convergence of random Fourier series, of orthogonal series, and to functional analysis. The complete solution of a number of classical problems is given in complete detail, and an ambitious program for future research is laid out.},
	language = {en},
	urldate = {2020-05-05},
	publisher = {Springer},
	author = {Talagrand, Michel},
	year = {2014},
	doi = {10.1007/978-3-642-54075-2},
}

@book{wainwright_high-dimensional_2019,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}},
	isbn = {978-1-108-49802-9},
	shorttitle = {High-{Dimensional} {Statistics}},
	abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
	urldate = {2020-11-23},
	publisher = {Cambridge University Press},
	author = {Wainwright, Martin J.},
	year = {2019},
	doi = {10.1017/9781108627771},
}

@article{oseledets_breaking_2009,
	title = {Breaking the {Curse} of {Dimensionality}, {Or} {How} to {Use} {SVD} in {Many} {Dimensions}},
	volume = {31},
	issn = {1064-8275},
	doi = {10.1137/090748330},
	abstract = {For d-dimensional tensors with possibly large {\textbackslash}d{\textbackslash}textgreater3{\textbackslash}, an hierarchical data structure, called the Tree-Tucker format, is presented as an alternative to the canonical decomposition. It has asymptotically the same (and often even smaller) number of representation parameters and viable stability properties. The approach involves a recursive construction described by a tree with the leafs corresponding to the Tucker decompositions of three-dimensional tensors, and is based on a sequence of SVDs for the recursively obtained unfolding matrices and on the auxiliary dimensions added to the initial “spatial” dimensions. It is shown how this format can be applied to the problem of multidimensional convolution. Convincing numerical examples are given.},
	number = {5},
	urldate = {2020-12-12},
	journal = {SIAM Journal on Scientific Computing},
	author = {Oseledets, I. V. and Tyrtyshnikov, E. E.},
	month = jan,
	year = {2009},
	pages = {3744--3759},
	annote = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445, 1095-7200},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	language = {en},
	number = {3},
	urldate = {2021-01-28},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	pages = {455--500},
}

@book{bellman_adaptive_1961,
	address = {New Jersey},
	title = {Adaptive {Control} {Processes}},
	isbn = {978-1-4008-7466-8},
	abstract = {The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Mr. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields. Originally published in 1961. The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.},
	language = {en},
	urldate = {2021-02-07},
	publisher = {Princeton University Press},
	author = {Bellman, Richard E.},
	year = {1961},
	annote = {Publication Title: Adaptive Control Processes},
}

@article{perez-garcia_matrix_2007,
	title = {Matrix product state representations},
	volume = {7},
	issn = {1533-7146},
	abstract = {This work gives a detailed investigation of matrix product state (MPS) representations for pure multipartite quantum states. We determine the freedom in representations with and without translation symmetry, derive respective canonical forms and provide efficient methods for obtaining them. Results on frustration free Hamiltonians and the generation of MPS are extended, and the use of the MPS-representation for classical simulations of quantum systems is discussed.},
	number = {5},
	journal = {Quantum Information \& Computation},
	author = {Perez-Garcia, D. and Verstraete, F. and Wolf, M. M. and Cirac, J. I.},
	month = jul,
	year = {2007},
	pages = {401--430},
}

@article{hitchcock_expression_1927,
	title = {The {Expression} of a {Tensor} or a {Polyadic} as a {Sum} of {Products}},
	volume = {6},
	copyright = {© 1927 by the Massachusetts Institute of Technology},
	issn = {1467-9590},
	doi = {https://doi.org/10.1002/sapm192761164},
	language = {en},
	number = {1-4},
	urldate = {2021-02-09},
	journal = {Journal of Mathematics and Physics},
	author = {Hitchcock, Frank L.},
	year = {1927},
	pages = {164--189},
}

@book{landsberg_tensors_2011,
	series = {Graduate {Studies} in {Mathematics}},
	title = {Tensors: {Geometry} and {Applications}},
	volume = {128},
	isbn = {978-0-8218-6907-9 978-0-8218-8481-2 978-0-8218-8483-6 978-1-4704-0923-4},
	shorttitle = {Tensors},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	urldate = {2021-02-10},
	publisher = {American Mathematical Society},
	author = {Landsberg, J.},
	month = dec,
	year = {2011},
}

@book{penrose_spinors_1987,
	address = {Cambridge},
	title = {Spinors and {Space}-{Time}: {Volume} 1, {Two}-{Spinor} {Calculus} and {Relativistic} {Fields}},
	isbn = {978-0-521-33707-6},
	shorttitle = {Spinors and {Space}-{Time}},
	abstract = {This volume introduces and systematically develops the calculus of 2-spinors. This is the first detailed exposition of this technique which leads not only to a deeper understanding of the structure of space-time, but also provides shortcuts to some very tedious calculations. Many results are given here for the first time.},
	language = {Englisch},
	publisher = {Cambridge University Press},
	author = {Penrose, Roger},
	month = feb,
	year = {1987},
}

@article{cichocki_tensor_2015,
	title = {Tensor {Decompositions} for {Signal} {Processing} {Applications}: {From} two-way to multiway component analysis},
	volume = {32},
	issn = {1558-0792},
	shorttitle = {Tensor {Decompositions} for {Signal} {Processing} {Applications}},
	doi = {10.1109/MSP.2013.2297439},
	abstract = {The widespread use of multisensor technology and the emergence of big data sets have highlighted the limitations of standard flat-view matrix models and the necessity to move toward more versatile data analysis tools. We show that higher-order tensors (i.e., multiway arrays) enable such a fundamental paradigm shift toward models that are essentially polynomial, the uniqueness of which, unlike the matrix methods, is guaranteed under very mild and natural conditions. Benefiting from the power of multilinear algebra as their mathematical backbone, data analysis techniques using tensor decompositions are shown to have great flexibility in the choice of constraints which match data properties and extract more general latent components in the data than matrix-based methods.},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Cichocki, Andrzej and Mandic, Danilo and De Lathauwer, Lieven and Zhou, Guoxu and Zhao, Qibin and Caiafa, Cesar and PHAN, HUY ANH},
	month = mar,
	year = {2015},
	keywords = {Big data, Data analysis, Data models, Matrix decomposition, Sensors, Tensile stress},
	pages = {145--163},
	annote = {Conference Name: IEEE Signal Processing Magazine},
}

@article{hackbusch_new_2009,
	title = {A {New} {Scheme} for the {Tensor} {Representation}},
	volume = {15},
	issn = {1531-5851},
	abstract = {The paper presents a new scheme for the representation of tensors which is well-suited for high-order tensors. The construction is based on a hierarchy of tensor product subspaces spanned by orthonormal bases. The underlying binary tree structure makes it possible to apply standard Linear Algebra tools for performing arithmetical operations and for the computation of data-sparse approximations. In particular, a truncation algorithm can be implemented which is based on the standard matrix singular value decomposition (SVD) method.},
	language = {en},
	number = {5},
	urldate = {2021-06-18},
	journal = {Journal of Fourier Analysis and Applications},
	author = {Hackbusch, W. and Kühn, S.},
	month = oct,
	year = {2009},
	pages = {706--722},
}

@article{falco_minimal_2012,
	title = {On {Minimal} {Subspaces} in {Tensor} {Representations}},
	volume = {12},
	doi = {10.1007/s10208-012-9136-6},
	abstract = {n this paper we introduce and develop the notion of minimal subspaces in the framework of algebraic and topological tensor product spaces. This mathematical structure arises in a natural way in the study of tensor representations. We use minimal subspaces to prove the existence of a best approximation, for any element in a Banach tensor space, by means of a tensor given in a typical representation format (Tucker, hierarchical, or tensor train). We show that this result holds in a tensor Banach space with a norm stronger than the injective norm and in an intersection of finitely many Banach tensor spaces satisfying some additional conditions. Examples using topological tensor products of standard Sobolev spaces are given},
	journal = {Foundations of Computational Mathematics},
	author = {Falco, Antonio and Hackbusch, Wolfgang},
	month = dec,
	year = {2012},
	pages = {765--803},
}

@article{orus_tensor_2019,
	title = {Tensor networks for complex quantum systems},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5820},
	doi = {10.1038/s42254-019-0086-7},
	abstract = {Originally developed in the context of condensed-matter physics and based on renormalization group ideas, tensor networks have been revived thanks to quantum information theory and the progress in understanding the role of entanglement in quantum many-body systems. Moreover, tensor network states have turned out to play a key role in other scientific disciplines. In this context, here I provide an overview of the basic concepts and key developments in the field. I briefly discuss the most important tensor network structures and algorithms, together with an outline of advances related to global and gauge symmetries, fermions, topological order, classification of phases, entanglement Hamiltonians, holografic duality, artificial intelligence, the 2D Hubbard model, 2D quantum antiferromagnets, conformal field theory, quantum chemistry, disordered systems and many-body localization.},
	language = {en},
	number = {9},
	urldate = {2021-07-05},
	journal = {Nature Reviews Physics},
	author = {Orús, Román},
	month = sep,
	year = {2019},
	keywords = {Condensed-matter physics, Quantum information, Theoretical physics},
	pages = {538--550},
	file = {Eingereichte Version:/Users/goessmann/Zotero/storage/NXCFTC4U/Orús - 2019 - Tensor networks for complex quantum systems.pdf:application/pdf},
}

@book{wainwright_graphical_2008,
	title = {Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}},
	isbn = {978-1-60198-184-4},
	abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances-including the key problems of computing marginals and modes of probability distributions-are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, Graphical Models, Exponential Families and Variational Inference develops general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. It describes how a wide variety of algorithms- among them sum-product, cluster variational methods, expectation-propagation, mean field methods, and max-product-can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
	language = {en},
	publisher = {Now Publishers Inc},
	author = {Wainwright, Martin J. and Jordan, Michael Irwin},
	year = {2008},
}

@article{glasser_expressive_2019,
	title = {Expressive power of tensor-network factorizations for probabilistic modeling},
	volume = {32},
	language = {en},
	urldate = {2021-07-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
	year = {2019},
}

@article{robeva_duality_2019,
	title = {Duality of graphical models and tensor networks},
	volume = {8},
	issn = {2049-8772},
	doi = {10.1093/imaiai/iay009},
	abstract = {In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph. We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.},
	number = {2},
	urldate = {2021-07-06},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Robeva, Elina and Seigal, Anna},
	month = jun,
	year = {2019},
	pages = {273--288},
}

@inproceedings{goessmann_tensor_2020,
	title = {Tensor network approaches for data-driven identiﬁcation of non-linear dynamical laws},
	abstract = {To date, scalable methods for data-driven identiﬁcation of non-linear governing equations do not exploit or offer insight into the fundamental underlying physical structure. In this work, we show that various physical constraints can be captured via tensor network based parameterizations for the governing equation, which naturally ensures scalability. In addition to providing analytic results motivating the use of such models for realistic physical systems, we demonstrate that efﬁcient rank-adaptive optimization algorithms can be used to learn optimal tensor network models without requiring a priori knowledge of the exact tensor ranks.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} - {First} {Workshop} on {Quantum} {Tensor} {Networks} in {Machine} {Learning}},
	author = {Goessmann, Alex and Roth, Ingo and Kutyniok, Gitta and Götte, Michael and Sweke, Ryan and Eisert, Jens},
	year = {2020},
	pages = {21},
}

@article{espig_variational_2012,
	title = {Variational calculus with sums of elementary tensors of fixed rank},
	volume = {122},
	issn = {0945-3245},
	doi = {10.1007/s00211-012-0464-x},
	abstract = {In this article we introduce a calculus of variations for sums of elementary tensors and apply it to functionals of practical interest. The survey provides all necessary ingredients for applying minimization methods in a general setting. The important cases of target functionals which are linear and quadratic with respect to the tensor product are discussed, and combinations of these functionals are presented in detail. As an example, we consider the solution of a linear system in structured tensor format. Moreover, we discuss the solution of an eigenvalue problem with sums of elementary tensors. This example can be viewed as a prototype of a constrained minimization problem. For the numerical treatment, we suggest a method which has the same order of complexity as the popular alternating least square algorithm and demonstrate the rate of convergence in numerical tests.},
	language = {en},
	number = {3},
	urldate = {2021-10-13},
	journal = {Numerische Mathematik},
	author = {Espig, Mike and Hackbusch, Wolfgang and Rohwedder, Thorsten and Schneider, Reinhold},
	month = nov,
	year = {2012},
	pages = {469--488},
}

@phdthesis{goessmann_uniform_2021,
	address = {Berlin},
	type = {{PhD} {Thesis}},
	title = {Uniform {Concentration} of {Tensor} and {Neural} {Networks}: {An} {Approach} towards {Recovery} {Guarantees}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://depositonce.tu-berlin.de/handle/11303/15990},
	abstract = {This thesis contributes to the uniform concentration approach towards guaranteeing the generalization of learned models. We show probabilistic bounds on various uniform concentration events and demonstrate their utility in recovery guarantees. The thesis is organized in three parts. In the first part, we develop a unified theoretical framework for the concentration of random variables and the uniform concentration of stochastic processes. We introduce functionals of stochastic processes and apply them in bounds on the supremum. Then we develop methods to transfer uniform concentration events into success guarantees for empirical risk minimization problems. The second part of this thesis investigates classes of structured random distributions. More precisely, we derive bounds on the uniform concentration of contracted random tensors, which are decomposable into tensor network formats. In particular, we show exact moment bounds on contracted Gaussian tensor networks, which are tensor networks consistent of independent standard Gaussian random cores. By applying comparison theorems for Gaussian variables, the upper moment bounds are extended to more generic Orlicz tensor networks, which are characterized by weaker assumptions made on the random cores. Furthermore, we derive bounds on the concentration of Haar tensor networks, which random cores follow the Haar distribution of Stiefel manifolds. For all examples we continue to provide probabilistic bounds on uniform concentration events, which imply recovery guarantees for tensor regression problems. We further apply our findings in deriving success guarantees for efficient algorithms solving tensor regression problems. In the third part, we transfer our findings to bounds on the uniform concentration of neural networks following two approaches. First, we derive concentration bounds for shallow ReLU networks with respect to standard Gaussian distributions, where we introduce parameter embeddings that capture the concentration structure. Second, we bound the Rademacher complexity of deep neural networks, which are activated by a contraction, by Rademacher complexities of linear functions. This enables the proof of recovery guarantees for neural networks, which are trained on structured data.},
	language = {en},
	urldate = {2022-01-13},
	school = {Technische Universität Berlin},
	author = {Goessmann, Alex},
	year = {2021},
	annote = {Accepted: 2021-12-30T15:00:58Z},
}

@article{richardson_markov_2006,
	title = {Markov logic networks},
	volume = {62},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-5833-1},
	doi = {10.1007/s10994-006-5833-1},
	abstract = {We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrst-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
	language = {en},
	number = {1-2},
	urldate = {2023-01-14},
	journal = {Machine Learning},
	author = {Richardson, Matthew and Domingos, Pedro},
	month = feb,
	year = {2006},
	pages = {107--136},
}

@inproceedings{galarraga_amie_2013,
	address = {Rio de Janeiro Brazil},
	title = {{AMIE}: association rule mining under incomplete evidence in ontological knowledge bases},
	isbn = {978-1-4503-2035-1},
	shorttitle = {{AMIE}},
	url = {https://dl.acm.org/doi/10.1145/2488388.2488425},
	doi = {10.1145/2488388.2488425},
	abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature ﬁeld, mining logical rules from KBs is diﬀerent in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today’s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for conﬁdence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Galárraga, Luis Antonio and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian},
	month = may,
	year = {2013},
	pages = {413--422},
}

@article{lehmann_class_2011,
	title = {Class expression learning for ontology engineering},
	volume = {9},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826811000023},
	doi = {10.1016/j.websem.2011.01.001},
	abstract = {While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Protégé and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.},
	number = {1},
	urldate = {2023-11-06},
	journal = {Journal of Web Semantics},
	author = {Lehmann, Jens and Auer, Sören and Bühmann, Lorenz and Tramp, Sebastian},
	month = mar,
	year = {2011},
	keywords = {Concept learning, Heuristics, Ontology editor plugins, Ontology engineering, OWL, Supervised machine learning},
	pages = {71--81},
}

@article{demir_drill-_2021,
	title = {{DRILL}- {Deep} {Reinforcement} {Learning} for {Refinement} {Operators} in {ALC}},
	volume = {abs/2106.15373},
	url = {https://ris.uni-paderborn.de/record/25217},
	language = {eng},
	urldate = {2023-11-06},
	journal = {CoRR},
	author = {Demir, Caglar and Ngonga Ngomo, Axel-Cyrille},
	year = {2021},
}

@incollection{kouagou_neural_2023,
	address = {Cham},
	title = {Neural {Class} {Expression} {Synthesis}},
	volume = {13870},
	isbn = {978-3-031-33454-2 978-3-031-33455-9},
	url = {https://link.springer.com/10.1007/978-3-031-33455-9_13},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Kouagou, N’Dah Jean and Heindorf, Stefan and Demir, Caglar and Ngonga Ngomo, Axel-Cyrille},
	editor = {Pesquita, Catia and Jimenez-Ruiz, Ernesto and McCusker, Jamie and Faria, Daniel and Dragoni, Mauro and Dimou, Anastasia and Troncy, Raphael and Hertling, Sven},
	year = {2023},
	doi = {10.1007/978-3-031-33455-9_13},
	pages = {209--226},
	annote = {Series Title: Lecture Notes in Computer Science},
}

@article{muggleton_inductive_1994,
	title = {Inductive logic programming: {Theory} and methods},
	volume = {19},
	shorttitle = {Inductive logic programming},
	url = {https://www.sciencedirect.com/science/article/pii/0743106694900353},
	urldate = {2023-11-06},
	journal = {The Journal of Logic Programming},
	author = {Muggleton, Stephen and De Raedt, Luc},
	year = {1994},
	pages = {629--679},
	annote = {Publisher: Elsevier},
}

@incollection{sakama_linear_2017,
	address = {Cham},
	title = {Linear {Algebraic} {Characterization} of {Logic} {Programs}},
	volume = {10412},
	isbn = {978-3-319-63557-6 978-3-319-63558-3},
	url = {http://link.springer.com/10.1007/978-3-319-63558-3_44},
	abstract = {This paper introduces a novel approach for computing logic programming semantics based on multilinear algebra. First, a propositional Herbrand base is represented in a vector space and if-then rules in a program are encoded in a matrix. Then we provide methods of computing the least model of a Horn logic program, minimal models of a disjunctive logic program, and stable models of a normal logic program by algebraic manipulation of higher-order tensors. The result of this paper exploits a new connection between linear algebraic computation and symbolic computation, which has potential to realize logical inference in huge scale of knowledge bases.},
	language = {en},
	urldate = {2023-11-06},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}},
	publisher = {Springer International Publishing},
	author = {Sakama, Chiaki and Inoue, Katsumi and Sato, Taisuke},
	editor = {Li, Gang and Ge, Yong and Zhang, Zili and Jin, Zhi and Blumenstein, Michael},
	year = {2017},
	doi = {10.1007/978-3-319-63558-3_44},
	pages = {520--533},
	annote = {Series Title: Lecture Notes in Computer Science},
}

@article{sato_linear_2017,
	title = {A linear algebraic approach to datalog evaluation},
	volume = {17},
	issn = {1471-0684, 1475-3081},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/linear-algebraic-approach-to-datalog-evaluation/CED3EEB903D9D8A16843CFCA5AC4D577},
	doi = {10.1017/S1471068417000023},
	abstract = {We propose a fundamentally new approach to Datalog evaluation. Given a linear Datalog program DB written using N constants and binary predicates, we first translate if-and-only-if completions of clauses in DB into a set E q (DB) of matrix equations with a non-linear operation, where relations in M DB, the least Herbrand model of DB, are encoded as adjacency matrices. We then translate E q (DB) into another, but purely linear matrix equations Ẽ q (DB). It is proved that the least solution of Ẽ q (DB) in the sense of matrix ordering is converted to the least solution of E q (DB) and the latter gives M DB as a set of adjacency matrices. Hence, computing the least solution of Ẽ q (DB) is equivalent to computing M DB specified by DB. For a class of tail recursive programs and for some other types of programs, our approach achieves O(N 3) time complexity irrespective of the number of variables in a clause since only matrix operations costing O(N 3) or less are used. We conducted two experiments that compute the least Herbrand models of linear Datalog programs. The first experiment computes transitive closure of artificial data and real network data taken from the Koblenz Network Collection. The second one compared the proposed approach with the state-of-the-art symbolic systems including two Prolog systems and two ASP systems, in terms of computation time for a transitive closure program and the same generation program. In the experiment, it is observed that our linear algebraic approach runs 101 {\textbackslash}textasciitilde 104 times faster than the symbolic systems when data is not sparse. Our approach is inspired by the emergence of big knowledge graphs and expected to contribute to the realization of rich and scalable logical inference for knowledge graphs.},
	language = {en},
	number = {3},
	urldate = {2023-11-06},
	journal = {Theory and Practice of Logic Programming},
	author = {Sato, Taisuke},
	month = may,
	year = {2017},
	keywords = {Datalog, least model, matrix, vector space},
	pages = {244--265},
	annote = {Publisher: Cambridge University Press},
}

@article{nickel_review_2016,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/7358050/},
	doi = {10.1109/JPROC.2015.2483592},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The ﬁrst is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google’s Knowledge Vault project as an example of such combination.},
	language = {en},
	number = {1},
	urldate = {2023-11-06},
	journal = {Proceedings of the IEEE},
	author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
	month = jan,
	year = {2016},
	pages = {11--33},
}

@article{trouillon_complex_nodate,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	language = {en},
	author = {Trouillon, Theo and Welbl, Johannes and Riedel, Sebastian},
}

@misc{trouillon_complex_2017,
	title = {Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}: {A} {Comparison}},
	shorttitle = {Complex and {Holographic} {Embeddings} of {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/1707.01475},
	abstract = {Embeddings of knowledge graphs have received significant attention due to their excellent performance for tasks like link prediction and entity resolution. In this short paper, we are providing a comparison of two state-of-the-art knowledge graph embeddings for which their equivalence has recently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio, 2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly review both models and discuss how their scoring functions are equivalent. We then analyze the discrepancy of results reported in the original articles, and show experimentally that they are likely due to the use of different loss functions. In further experiments, we evaluate the ability of both models to embed symmetric and antisymmetric patterns. Finally, we discuss advantages and disadvantages of both models and under which conditions one would be preferable to the other.},
	language = {en},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Trouillon, Théo and Nickel, Maximilian},
	month = jul,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv:1707.01475 [cs, stat]},
}

@article{trouillon_knowledge_2017,
	title = {Knowledge {Graph} {Completion} via {Complex} {Tensor} {Factorization}},
	language = {en},
	author = {Trouillon, Theo},
	year = {2017},
}

@inproceedings{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a uniﬁed learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq ˆ CityInCountrypb, cq ùñ N ationalitypa, cq. We ﬁnd that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art conﬁdence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	language = {en},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv:1412.6575 [cs]},
}

@misc{lam_graphcast_2023,
	title = {{GraphCast}: {Learning} skillful medium-range global weather forecasting},
	shorttitle = {{GraphCast}},
	url = {http://arxiv.org/abs/2212.12794},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but cannot directly use historical weather data to improve the underlying model. We introduce a machine learning-based method called “GraphCast”, which can be trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25° resolution globally, in under one minute. We show that GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.},
	language = {en},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = aug,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	annote = {arXiv:2212.12794 [physics]},
}

@article{badreddine_logic_2022,
	title = {Logic {Tensor} {Networks}},
	volume = {303},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221002009},
	doi = {10.1016/j.artint.2021.103649},
	abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.},
	urldate = {2023-11-20},
	journal = {Artificial Intelligence},
	author = {Badreddine, Samy and d'Avila Garcez, Artur and Serafini, Luciano and Spranger, Michael},
	month = feb,
	year = {2022},
	keywords = {Deep learning and reasoning, Many-valued logics, Neurosymbolic AI},
	pages = {103649},
}

@article{westphal_sml-bench_2019,
	title = {{SML}-{Bench} – {A} benchmarking framework for structured machine learning},
	volume = {10},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-180308},
	doi = {10.3233/SW-180308},
	abstract = {The availability of structured data has increased significantly over the past decade and several approaches to learn from structured data have been proposed. These logic-based, inductive learning methods are often conceptually similar, which would allow a comparison among them even if they stem from different research communities. However, so far no efforts were made to define an environment for running learning tasks on a variety of tools, covering multiple knowledge representation languages. With SML-Bench, we propose a benchmarking framework to run inductive learning tools from the ILP and semantic web communities on a selection of learning problems. In this paper, we present the foundations of SML-Bench, discuss the systematic selection of benchmarking datasets and learning problems, and showcase an actual benchmark run on the currently supported tools.},
	language = {en},
	number = {2},
	urldate = {2023-12-20},
	journal = {Semantic Web},
	author = {Westphal, Patrick and Bühmann, Lorenz and Bin, Simon and Jabeen, Hajira and Lehmann, Jens},
	editor = {Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia and Ngonga Ngomo, Axel-Cyrille and Fundulaki, Irini and Krithara, Anastasia},
	month = jan,
	year = {2019},
	pages = {231--245},
}

@article{marra_statistical_2024,
	title = {From statistical relational to neurosymbolic artificial intelligence: {A} survey},
	volume = {328},
	issn = {0004-3702},
	shorttitle = {From statistical relational to neurosymbolic artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
	doi = {10.1016/j.artint.2023.104062},
	abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.},
	urldate = {2024-02-20},
	journal = {Artificial Intelligence},
	author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
	month = mar,
	year = {2024},
	keywords = {Neurosymbolic AI, Learning and reasoning, Probabilistic logics, Statistical relational AI},
	pages = {104062},
}

@article{cohen_tensorlog_2020,
	title = {{TensorLog}: {A} {Probabilistic} {Database} {Implemented} {Using} {Deep}-{Learning} {Infrastructure}},
	volume = {67},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {{TensorLog}},
	url = {https://jair.org/index.php/jair/article/view/11944},
	doi = {10.1613/jair.1.11944},
	abstract = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
	language = {en},
	urldate = {2024-02-20},
	journal = {Journal of Artificial Intelligence Research},
	author = {Cohen, William and Yang, Fan and Mazaitis, Kathryn Rivard},
	month = feb,
	year = {2020},
	pages = {285--325},
}

@article{hochreiter_toward_2022,
	title = {Toward a broad {AI}},
	volume = {65},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3512715},
	doi = {10.1145/3512715},
	language = {en},
	number = {4},
	urldate = {2024-02-22},
	journal = {Communications of the ACM},
	author = {Hochreiter, Sepp},
	month = jan,
	year = {2022},
	pages = {56--57},
}

@article{pinkas_reasoning_1995,
	title = {Reasoning, nonmonotonicity and learning in connectionist networks that capture propositional knowledge},
	volume = {77},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029400032V},
	doi = {10.1016/0004-3702(94)00032-V},
	abstract = {The paper presents a connectionist framework that is capable of representing and learning propositional knowledge. An extended version of propositional calculus is developed and is demonstrated to be useful for nonmonotonic reasoning, dealing with conflicting beliefs and for coping with inconsistency generated by unreliable knowledge sources. Formulas of the extended calculus are proved to be equivalent in a very strong sense to symmetric networks (like Hopfield networks and Boltzmann machines), and efficient algorithms are given for translating back and forth between the two forms of knowledge representation. A fast learning procedure is presented that allows symmetric networks to learn representations of unknown logic formulas by looking at examples. A connectionist inference engine is then sketched whose knowledge is either compiled from a symbolic representation or learned inductively from training examples. Experiments with large scale randomly generated formulas suggest that the parallel local search that is executed by the networks is extremely fast on average. Finally, it is shown that the extended logic can be used as a high-level specification language for connectionist networks, into which several recent symbolic systems may be mapped. The paper demonstrates how a rigorous bridge can be constructed that ties together the (sometimes opposing) connectionist and symbolic approaches.},
	number = {2},
	urldate = {2024-02-23},
	journal = {Artificial Intelligence},
	author = {Pinkas, Gadi},
	month = sep,
	year = {1995},
	pages = {203--247},
}

@inproceedings{marra_neural_2021,
	title = {Neural markov logic networks},
	url = {https://proceedings.mlr.press/v161/marra21a.html},
	abstract = {We introduce neural Markov logic networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov logic networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Similarly to many neural symbolic methods, NMLNs can exploit embeddings of constants but, unlike them, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion, triple classification and on generation of molecular (graph) data.},
	language = {en},
	urldate = {2024-02-23},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Marra, Giuseppe and Kuželka, Ondřej},
	month = dec,
	year = {2021},
	pages = {908--917},
	annote = {ISSN: 2640-3498},
}

@article{sarker_neuro-symbolic_2022,
	title = {Neuro-symbolic artificial intelligence: {Current} trends},
	volume = {34},
	issn = {18758452, 09217126},
	shorttitle = {Neuro-symbolic artificial intelligence},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-210084},
	doi = {10.3233/AIC-210084},
	abstract = {Neuro-Symbolic Artificial Intelligence – the combination of symbolic methods with methods that are based on artificial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
	number = {3},
	urldate = {2024-04-17},
	journal = {AI Communications},
	author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
	month = mar,
	year = {2022},
	pages = {197--209},
}

@inproceedings{ankan_pgmpy_2015,
	address = {Austin, Texas},
	title = {pgmpy: {Probabilistic} {Graphical} {Models} using {Python}},
	shorttitle = {pgmpy},
	url = {https://conference.scipy.org/proceedings/scipy2015/ankur_ankan.html},
	doi = {10.25080/Majora-7b98e3ed-001},
	abstract = {Probabilistic Graphical Models (PGM) is a technique of compactly representing a joint distribution by exploiting dependencies between the random variables. It also allows us to do inference on joint distributions in a computationally cheaper way than the traditional methods. PGMs are widely used in the ﬁeld of speech recognition, information extraction, image segmentation, modelling gene regulatory networks.},
	language = {en},
	urldate = {2024-07-02},
	author = {Ankan, Ankur and Panda, Abinash},
	year = {2015},
	pages = {6--11},
}

@incollection{karalis_native_2023,
	title = {Native {Execution} of {GraphQL} {Queries} over {RDF} {Graphs} {Using} {Multi}-{Way} {Joins}},
	url = {https://ebooks.iospress.nl/volumearticle/64014},
	urldate = {2024-07-02},
	booktitle = {Knowledge {Graphs}: {Semantics}, {Machine} {Learning}, and {Languages}},
	publisher = {IOS Press},
	author = {Karalis, Nikolaos and Bigerl, Alexander and Ngonga Ngomo, Axel-Cyrille},
	year = {2023},
	pages = {77--93},
}

@inproceedings{karalis_efficient_2024,
	address = {Cham},
	title = {Efficient {Evaluation} of {Conjunctive} {Regular} {Path} {Queries} {Using} {Multi}-way {Joins}},
	isbn = {978-3-031-60626-7},
	doi = {10.1007/978-3-031-60626-7_12},
	abstract = {Recent analyses of real-world queries show that a prominent type of queries is that of conjunctive regular path queries. Despite the increasing popularity of this type of queries, only limited efforts have been invested in their efficient evaluation. Motivated by recent results on the efficiency of worst-case optimal multi-way join algorithms for the evaluation of conjunctive queries, we present a novel multi-way join algorithm for the efficient evaluation of conjunctive regular path queries. The hallmark of our algorithm is the evaluation of the regular path queries found in conjunctive regular path queries using multi-way joins. This enables the exploitation of regular path queries in the planning steps of the proposed algorithm, which is crucial for the algorithm’s efficiency, as shown by the results of our detailed evaluation using the Wikidata-based benchmark WDBench. The results of this evaluation also show that our approach achieves a value of query mixes per hour that is 4.3 higher than the state of the art and that it outperforms all of the competing graph storage solutions in almost 70\% of the benchmark’s queries.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Karalis, Nikolaos and Bigerl, Alexander and Heidrich, Liss and Sherif, Mohamed Ahmed and Ngonga Ngomo, Axel-Cyrille},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	year = {2024},
	pages = {218--235},
}

@inproceedings{kok_learning_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {Learning the structure of {Markov} logic networks},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102407},
	doi = {10.1145/1102351.1102407},
	abstract = {Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortest-first search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Kok, Stanley and Domingos, Pedro},
	month = aug,
	year = {2005},
	pages = {441--448},
}

@phdthesis{nyga_interpretation_2017,
	type = {{PhD} {Thesis}},
	title = {Interpretation of {Natural}-language {Robot} {Instructions}: {Probabilistic} {Knowledge} {Representation}, {Learning}, and {Reasoning}},
	copyright = {Bitte wählen Sie eine Lizenz aus: (Unsere Empfehlung: CC-BY)},
	shorttitle = {Interpretation of {Natural}-language {Robot} {Instructions}},
	url = {https://media.suub.uni-bremen.de/handle/elib/1215},
	abstract = {A robot that can be simply told in natural language what to do – this has been one of the ultimate long-standing goals in both Artificial Intelligence and Robotics research. In near-future applications, robotic assistants and companions will have to understand and perform commands such as set the table for dinner”, make pancakes for breakfast”, or cut the pizza into 8 pieces.” Although such instructions are only vaguely formulated, complex sequences of sophisticated and accurate manipulation activities need to be carried out in order to accomplish the respective tasks. The acquisition of knowledge about how to perform these activities from huge collections of natural-language instructions from the Internet has garnered a lot of attention within the last decade. However, natural language is typically massively unspecific, incomplete, ambiguous and vague and thus requires powerful means for interpretation. This work presents PRAC – Probabilistic Action Cores – an interpreter for natural-language instructions which is able to resolve vagueness and ambiguity in natural language and infer missing information pieces that are required to render an instruction executable by a robot. To this end, PRAC formulates the problem of instruction interpretation as a reasoning problem in first-order probabilistic knowledge bases. In particular, the system uses Markov logic networks as a carrier formalism for encoding uncertain knowledge. A novel framework for reasoning about unmodeled symbolic concepts is introduced, which incorporates ontological knowledge from taxonomies and exploits semantically similar relational structures in a domain of discourse. The resulting reasoning framework thus enables more compact representations of knowledge and exhibits strong generalization performance when being learnt from very sparse data. Furthermore, a novel approach for completing directives is presented, which applies semantic analogical reasoning to transfer knowledge collected from thousands of natural-language instruction sheets to new situations. In addition, a cohesive processing pipeline is described that transforms vague and incomplete task formulations into sequences of formally specified robot plans. The system is connected to a plan executive that is able to execute the computed plans in a simulator. Experiments conducted in a publicly accessible, browser-based web interface showcase that PRAC is capable of closing the loop from natural-language instructions to their execution by a robot.},
	language = {en},
	urldate = {2024-08-07},
	author = {Nyga, Daniel},
	month = may,
	year = {2017},
	annote = {Accepted: 2020-03-09T14:46:38Z Publisher: Universität Bremen},
}

@inproceedings{tsilionis_tensor-based_2024,
	address = {Jeju, South Korea},
	title = {A {Tensor}-{Based} {Formalization} of the {Event} {Calculus}},
	isbn = {978-1-956792-04-1},
	url = {https://www.ijcai.org/proceedings/2024/397},
	doi = {10.24963/ijcai.2024/397},
	abstract = {We present a formalization of the Event Calculus (EC) in tensor spaces. The motivation for a tensorbased predicate calculus comes from the area of composite event recognition (CER). As a CER engine, we adopt a logic programming implementation of EC with optimizations for continuous narrative assimilation on data streams. We show how to evaluate EC rules algebraically and solve a linear equation to compute the corresponding models. We demonstrate the scalability of our approach with the use of large datasets from a real-world application domain, and show it outperforms significantly symbolic EC, in terms of processing time.},
	language = {en},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the {Thirty}-{ThirdInternational} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Tsilionis, Efthimis and Artikis, Alexander and Paliouras, Georgios},
	month = aug,
	year = {2024},
	pages = {3584--3592},
}

@inproceedings{tran_fast_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Fast {Computation} of {Explanations} for {Inconsistency} in {Large}-{Scale} {Knowledge} {Graphs}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380014},
	doi = {10.1145/3366423.3380014},
	abstract = {Knowledge graphs (KGs) are essential resources for many applications including Web search and question answering. As KGs are often automatically constructed, they may contain incorrect facts. Detecting them is a crucial, yet extremely expensive task. Prominent solutions detect and explain inconsistency in KGs with respect to accompanying ontologies that describe the KG domain of interest. Compared to machine learning methods they are more reliable and human-interpretable but scale poorly on large KGs. In this paper, we present a novel approach to dramatically speed up the process of detecting and explaining inconsistency in large KGs by exploiting KG abstractions that capture prominent data patterns. Though much smaller, KG abstractions preserve inconsistency and their explanations. Our experiments with large KGs (e.g., DBpedia and Yago) demonstrate the feasibility of our approach and show that it significantly outperforms the popular baseline.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria and Kharlamov, Evgeny and Strötgen, Jannik},
	month = apr,
	year = {2020},
	pages = {2613--2619},
}

@misc{teyou_embedding_2024,
	title = {Embedding {Knowledge} {Graphs} in {Degenerate} {Clifford} {Algebras}},
	url = {http://arxiv.org/abs/2402.04870},
	abstract = {Clifford algebras are a natural extension of division algebras, including real numbers, complex numbers, quaternions, and octonions. Previous research in knowledge graph embeddings has focused exclusively on Clifford algebras of a specific type, which do not include nilpotent base vectors—elements that square to zero. In this work, we introduce a novel approach by incorporating nilpotent base vectors with a nilpotency index of two, leading to a more general form of Clifford algebras named degenerate Clifford algebras. This generalization to degenerate Clifford algebras does allow for covering dual numbers and as such include translations and rotations models under the same generalization paradigm for the first time. We develop two models to determine the parameters that define the algebra: one using a greedy search and another predicting the parameters based on neural network embeddings of the input knowledge graph. Our evaluation on seven benchmark datasets demonstrates that this incorporation of nilpotent vectors enhances the quality of embeddings. Additionally, our method outperforms state-ofthe-art approaches in terms of generalization, particularly regarding the mean reciprocal rank achieved on validation data. Finally, we show that even a simple greedy search can effectively discover optimal or near-optimal parameters for the algebra.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Teyou, Louis Mozart Kamdem and Demir, Caglar and Ngomo, Axel-Cyrille Ngonga},
	month = sep,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {arXiv:2402.04870 [cs]},
}

@book{getoor_introduction_2019,
	title = {Introduction to {Statistical} {Relational} {Learning}},
	isbn = {978-0-262-53868-8},
	abstract = {Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications.Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.},
	language = {Englisch},
	publisher = {MIT Press},
	author = {Getoor, Lise and Taskar, Ben},
	month = sep,
	year = {2019},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, Mass.},
	edition = {1. edition},
	title = {Probabilistic {Graphical} {Models}: {Principles} and {Techniques}},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic {Graphical} {Models}},
	abstract = {A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.},
	language = {English},
	publisher = {The MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	month = jul,
	year = {2009},
}

@book{hogan_knowledge_2021,
	address = {Cham},
	edition = {1st edition},
	title = {Knowledge {Graphs}},
	isbn = {978-3-031-00790-3},
	abstract = {This book provides a comprehensive and accessible introduction to knowledge graphs, which have recently garnered notable attention from both industry and academia. Knowledge graphs are founded on the principle of applying a graph-based abstraction to data, and are now broadly deployed in scenarios that require integrating and extracting value from multiple, diverse sources of data at large scale. The book defines knowledge graphs and provides a high-level overview of how they are used. It presents and contrasts popular graph models that are commonly used to represent data as graphs, and the languages by which they can be queried before describing how the resulting data graph can be enhanced with notions of schema, identity, and context. The book discusses how ontologies and rules can be used to encode knowledge as well as how inductive techniques—based on statistics, graph analytics, machine learning, etc.—can be used to encode and extract knowledge. It covers techniques for the creation, enrichment, assessment, and refinement of knowledge graphs and surveys recent open and enterprise knowledge graphs and the industries or applications within which they have been most widely adopted. The book closes by discussing the current limitations and future directions along which knowledge graphs are likely to evolve. This book is aimed at students, researchers, and practitioners who wish to learn more about knowledge graphs and how they facilitate extracting value from diverse data at large scale. To make the book accessible for newcomers, running examples and graphical notation are used throughout. Formal definitions and extensive references are also provided for those who opt to delve more deeply into specific topics.},
	language = {English},
	publisher = {Springer},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d’Amato, Claudia and Melo, Gerard de and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jose Emilio Labra and Navigli, Roberto and Neumaier, Sebastian},
	month = nov,
	year = {2021},
}

@article{shi_bayesian_2024,
	title = {Bayesian {Methods} in {Tensor} {Analysis}},
	volume = {17},
	issn = {19387989, 19387997},
	url = {http://arxiv.org/abs/2302.05978},
	doi = {10.4310/23-SII802},
	abstract = {Tensors, also known as multidimensional arrays, are useful data structures in machine learning and statistics. In recent years, Bayesian methods have emerged as a popular direction for analyzing tensor-valued data since they provide a convenient way to introduce sparsity into the model and conduct uncertainty quantification. In this article, we provide an overview of frequentist and Bayesian methods for solving tensor completion and regression problems, with a focus on Bayesian methods. We review common Bayesian tensor approaches including model formulation, prior assignment, posterior computation, and theoretical properties. We also discuss potential future directions in this field.},
	number = {2},
	urldate = {2025-01-06},
	journal = {Statistics and Its Interface},
	author = {Shi, Yiyao and Shen, Weining},
	year = {2024},
	keywords = {Statistics - Methodology},
	pages = {249--274},
	annote = {arXiv:2302.05978 [stat]},
}

@inproceedings{ganapathi_constrained_2008,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'08},
	title = {Constrained approximate maximum entropy learning of {Markov} random fields},
	isbn = {978-0-9749039-4-1},
	abstract = {Parameter estimation in Markov random fields (MRFs) is a difficult task, in which inference over the network is run in the inner loop of a gradient descent procedure. Replacing exact inference with approximate methods such as loopy belief propagation (LBP) can suffer from poor convergence. In this paper, we provide a different approach for combining MRF learning and Bethe approximation. We consider the dual of maximum likelihood Markov network learning — maximizing entropy with moment matching constraints — and then approximate both the objective and the constraints in the resulting optimization problem. Unlike previous work along these lines (Teh \&amp; Welling, 2003), our formulation allows parameter sharing between features in a general log-linear model, parameter regularization and conditional training. We show that piecewise training (Sutton \&amp; McCallum, 2005) is a very restricted special case of this formulation. We study two optimization strategies: one based on a single convex approximation and one that uses repeated convex approximations. We show results on several real-world networks that demonstrate that these algorithms can significantly outperform learning with loopy and piece-wise. Our results also provide a framework for analyzing the trade-offs of different relaxations of the entropy objective and of the constraints.},
	urldate = {2025-01-29},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Ganapathi, Varun and Vickrey, David and Duchi, John and Koller, Daphne},
	month = jul,
	year = {2008},
	pages = {196--203},
}

@incollection{casazza_introduction_2013,
	address = {Boston},
	title = {Introduction to {Finite} {Frame} {Theory}},
	isbn = {978-0-8176-8373-3},
	url = {https://doi.org/10.1007/978-0-8176-8373-3_1},
	abstract = {To date, frames have established themselves as a standard notion in applied mathematics, computer science, and engineering as a means to derive redundant, yet stable decompositions of a signal for analysis or transmission, while also promoting sparse expansions. The reconstruction procedure is then based on one of the associated dual frames, which—in the case of a Parseval frame—can be chosen to be the frame itself. In this chapter, we provide a comprehensive review of the basics of finite frame theory upon which the subsequent chapters are based. After recalling some background information on Hilbert space theory and operator theory, we introduce the notion of a frame along with some crucial properties and construction procedures. Then we discuss algorithmic aspects such as basic reconstruction algorithms and present brief introductions to diverse applications and extensions of frames. The subsequent chapters of this book will then extend key topics in many intriguing directions.},
	language = {en},
	urldate = {2025-02-04},
	booktitle = {Finite {Frames}: {Theory} and {Applications}},
	publisher = {Birkhäuser},
	author = {Casazza, Peter G. and Kutyniok, Gitta and Philipp, Friedrich},
	editor = {Casazza, Peter G. and Kutyniok, Gitta},
	year = {2013},
	doi = {10.1007/978-0-8176-8373-3_1},
	pages = {1--53},
}

@book{pearl_probabilistic_1988,
	address = {s.l.},
	title = {Probabilistic {Reasoning} in {Intelligent} {Systems}: {Networks} of {Plausible} {Inference}},
	isbn = {978-1-55860-479-7},
	shorttitle = {Probabilistic {Reasoning} in {Intelligent} {Systems}},
	abstract = {Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty-and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition-in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.},
	language = {Englisch},
	publisher = {Morgan Kaufmann},
	author = {Pearl, Judea},
	month = sep,
	year = {1988},
}

@book{pearl_causality_2009,
	address = {Cambridge New York, NY Port Melbourne New Delhi Singapore},
	edition = {2},
	title = {Causality: {Models}, {Reasoning} and {Inference}. {Ausgezeichnet}: {ACM} {Turing} {Award} for {Transforming} {Artificial} {Intelligence} 2011},
	isbn = {978-0-521-89560-6},
	shorttitle = {Causality},
	abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
	language = {Englisch},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	month = nov,
	year = {2009},
}

@book{russell_artificial_2021,
	address = {Boston},
	edition = {4},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}, {Global} {Edition}: {A} {Modern} {Approach}, {Global} {Edition}},
	isbn = {978-1-292-40113-3},
	shorttitle = {Artificial {Intelligence}},
	abstract = {Thelong-anticipated revision of ArtificialIntelligence: A Modern Approach explores the full breadth and depth of the field of artificialintelligence (AI). The 4th Edition brings readers up to date on the latest technologies,presents concepts in a more unified manner, and offers new or expanded coverageof machine learning, deep learning, transfer learning, multi agent systems,robotics, natural language processing, causality, probabilistic programming,privacy, fairness, and safe AI.},
	language = {Englisch},
	publisher = {Pearson},
	author = {Russell, Stuart and Norvig, Peter},
	month = may,
	year = {2021},
}

@book{mackay_information_2003,
	address = {Cambridge},
	edition = {Illustrated Edition},
	title = {Information {Theory}, {Inference} and {Learning} {Algorithms}},
	isbn = {978-0-521-64298-9},
	abstract = {Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.},
	language = {Englisch},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	month = sep,
	year = {2003},
}

@article{wolfram_statistical_1983,
	title = {Statistical mechanics of cellular automata},
	volume = {55},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.55.601},
	doi = {10.1103/RevModPhys.55.601},
	abstract = {Cellular automata are used as simple mathematical models to investigate self-organization in statistical mechanics. A detailed analysis is given of "elementary" cellular automata consisting of a sequence of sites with values 0 or 1 on a line, with each site evolving deterministically in discrete time steps according to definite rules involving the values of its nearest neighbors. With simple initial configurations, the cellular automata either tend to homogeneous states, or generate self-similar patterns with fractal dimensions ≃ 1.59 or ≃ 1.69. With "random" initial configurations, the irreversible character of the cellular automaton evolution leads to several self-organization phenomena. Statistical properties of the structures generated are found to lie in two universality classes, independent of the details of the initial state or the cellular automaton rules. More complicated cellular automata are briefly considered, and connections with dynamical systems theory and the formal theory of computation are discussed.},
	number = {3},
	urldate = {2025-02-11},
	journal = {Reviews of Modern Physics},
	author = {Wolfram, Stephen},
	month = jul,
	year = {1983},
	pages = {601--644},
	annote = {Publisher: American Physical Society},
}

@book{wolfram_new_2002,
	address = {Champaign (Ill.)},
	edition = {Illustrated Edition},
	title = {A {New} {Kind} of {Science}},
	isbn = {978-1-57955-008-0},
	abstract = {This book promises to revolutionize science as we know it' - Daily Telegraph 'Stephen's magnum opus may be the book of the decade if not the century' - Arthur C Clarke Long-awaited work from one of the world's most respected scientists presents a series of dramatic discoveries never before made public. Starting with a collection of computer experiments, Wolfram shows how their unexpected results force a whole new way of looking at the universe. A seminal work of enormous importance. Includes over 950 illustrations. BBC documentary in development.'},
	language = {Englisch},
	publisher = {Wolfram Media},
	author = {Wolfram, Stephen},
	month = may,
	year = {2002},
}

@book{barabasi_network_2016,
	address = {Cambridge},
	edition = {Illustrated edition},
	title = {Network {Science}},
	isbn = {978-1-107-07626-6},
	abstract = {Networks are everywhere, from the internet, to social networks, and the genetic networks that determine our biological existence. Illustrated throughout in full colour, this pioneering textbook, spanning a wide range of topics from physics to computer science, engineering, economics and the social sciences, introduces network science to an interdisciplinary audience. From the origins of the six degrees of separation to explaining why networks are robust to random failures, the author explores how viruses like Ebola and H1N1 spread, and why it is that our friends have more friends than we do. Using numerous real-world examples, this innovatively designed text includes clear delineation between undergraduate and graduate level material. The mathematical formulas and derivations are included within Advanced Topics sections, enabling use at a range of levels. Extensive online resources, including films and software for network analysis, make this a multifaceted companion for anyone with an interest in network science.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Barabási, Albert-László},
	month = jul,
	year = {2016},
}

@book{giovanni_russo_vito_latora_complex_2017,
	address = {Cambridge, United Kingdom ; New York, NY},
	title = {Complex {Networks}: {Principles}, {Methods} and {Applications}. {With} 58 exercises},
	isbn = {978-1-107-10318-4},
	shorttitle = {Complex {Networks}},
	abstract = {Networks constitute the backbone of complex systems, from the human brain to computer communications, transport infrastructures to online social systems and metabolic reactions to financial markets. Characterising their structure improves our understanding of the physical, biological, economic and social phenomena that shape our world. Rigorous and thorough, this textbook presents a detailed overview of the new theory and methods of network science. Covering algorithms for graph exploration, node ranking and network generation, among others, the book allows students to experiment with network models and real-world data sets, providing them with a deep understanding of the basics of network theory and its practical applications. Systems of growing complexity are examined in detail, challenging students to increase their level of skill. An engaging presentation of the important principles of network science makes this the perfect reference for researchers and undergraduate and graduate students in physics, mathematics, engineering, biology, neuroscience and the social sciences.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Giovanni Russo Vito Latora, Vincenzo Nicosia},
	month = sep,
	year = {2017},
}

@book{antoniou_semantic_2012,
	address = {Cambridge (Mass.)},
	edition = {third edition},
	title = {A {Semantic} {Web} {Primer}, third edition},
	isbn = {978-0-262-01828-9},
	abstract = {A new edition of the widely used guide to the key ideas, languages, and technologies of the Semantic WebThe development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its uses. A Semantic Web Primer provides an introduction and guide to this continuously evolving field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for independent study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own and includes exercises, project descriptions, and annotated references to relevant online materials.The third edition of this widely used text has been thoroughly updated, with significant new material that reflects a rapidly developing field. Treatment of the different languages (OWL2, rules) expands the coverage of RDF and OWL, defining the data model independently of XML and including coverage of N3/Turtle and RDFa. A chapter is devoted to OWL2, the new W3C standard. This edition also features additional coverage of the query language SPARQL, the rule language RIF and the possibility of interaction between rules and ontology languages and applications. The chapter on Semantic Web applications reflects the rapid developments of the past few years. A new chapter offers ideas for term projects. Additional material, including updates on the technological trends and research directions, can be found at http://www.semanticwebprimer.org.},
	language = {English},
	publisher = {The MIT Press},
	author = {Antoniou, Grigoris and Groth, Paul and Harmelen, Frank Van and Hoekstra, Rinke},
	month = aug,
	year = {2012},
}

@book{brachman_knowledge_2014,
	address = {Amsterdam Boston},
	title = {Knowledge {Representation} and {Reasoning}},
	isbn = {978-1-4933-0379-3},
	abstract = {Knowledge representation is at the very core of a radical idea for understanding intelligence. Instead of trying to understand or build brains from the bottom up, its goal is to understand and build intelligent behavior from the top down, putting the focus on what an agent needs to know in order to behave intelligently, how this knowledge can be represented symbolically, and how automated reasoning procedures can make this knowledge available as needed. This landmark text takes the central concepts of knowledge representation developed over the last 50 years and illustrates them in a lucid and compelling way. Each of the various styles of representation is presented in a simple and intuitive form, and the basics of reasoning with that representation are explained in detail. This approach gives readers a solid foundation for understanding the more advanced work found in the research literature. The presentation is clear enough to be accessible to a broad audience, including researchers and practitioners in database management, information retrieval, and object-oriented systems as well as artificial intelligence. This book provides the foundation in knowledge representation and reasoning that every AI practitioner needs.},
	language = {English},
	publisher = {Morgan Kaufmann},
	author = {Brachman, Ronald and Levesque, Dr Hector},
	month = dec,
	year = {2014},
}

@book{rockafellar_convex_1997,
	address = {Princeton},
	edition = {reprint edition},
	title = {Convex {Analysis}},
	isbn = {978-0-691-01586-6},
	abstract = {Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.},
	language = {English},
	publisher = {Princeton University Press},
	author = {Rockafellar, Ralph Tyrell},
	month = jan,
	year = {1997},
}

@book{ziegler_lectures_2013,
	address = {New York},
	edition = {1995th edition},
	title = {Lectures on {Polytopes}},
	isbn = {978-0-387-94365-7},
	abstract = {Based on a graduate course given at the Technische Universität, Berlin, these lectures present a wealth of material on the modern theory of convex polytopes. The clear and straightforward exposition features many illustrations, and provides complete proofs for most theorems. The material requires only linear algebra as a prerequisite, but takes the reader quickly from the basics to topics of recent research, including a number of unanswered questions. The lectures introduce the basic facts about polytopes, with an emphasis on the methods that yield the results, discuss important examples and elegant constructions, and show the excitement of current work in the field. They will provide interesting and enjoyable reading for researchers as well as students.},
	language = {English},
	publisher = {Springer},
	author = {Ziegler, Günter M.},
	month = oct,
	year = {2013},
}

@book{hiriart-urruty_convex_1993,
	address = {Berlin, Heidelberg},
	edition = {1993rd edition},
	title = {Convex {Analysis} and {Minimization} {Algorithms} {II}: {Advanced} {Theory} and {Bundle} {Methods}},
	isbn = {978-3-540-56852-0},
	shorttitle = {Convex {Analysis} and {Minimization} {Algorithms} {II}},
	abstract = {From the reviews: "The account is quite detailed and is written in a manner that will appeal to analysts and numerical practitioners alike...they contain everything from rigorous proofs to tables of numerical calculations.... one of the strong features of these books...that they are designed not for the expert, but for those who whish to learn the subject matter starting from little or no background...there are numerous examples, and counter-examples, to back up the theory...To my knowledge, no other authors have given such a clear geometric account of convex analysis." "This innovative text is well written, copiously illustrated, and accessible to a wide audience"},
	language = {English},
	publisher = {Springer},
	author = {Hiriart-Urruty, Jean-Baptiste and Lemarechal, Claude},
	month = oct,
	year = {1993},
}

@phdthesis{gillmann_01-polytopes_2007,
	type = {{PhD} {Thesis}},
	title = {0/1-{Polytopes}: {Typical} and {Extremal} {Properties}},
	shorttitle = {0/1-{Polytopes}},
	url = {https://depositonce.tu-berlin.de/items/urn:nbn:de:kobv:83-opus-14695},
	abstract = {0/1-Polytopen kann man in vielen Gebieten der diskreten Mathematik begegnen. Das bekannteste ist als "polyedrische Kombinatorik" bekannt. Auch wenn man die polyedrische Kombinatorik meist mit kombinatorischer Optimierung in Verbindung bringt, gibt es wichtige Zusammenhänge zu anderen Gebieten, zum Beispiel zum Zählen von kombinatorischen Strukturen. Trotz ihres breiten Anwendungsspektrums existiert keine umfassende Theorie über 0/1-Polytope. Es scheint so als seien 0/1-Polytope besonders komplizierte Objekte. Die Ergebnisse, welche in dieser Dissertation dargestellt werden, gruppieren sich um drei Hauptthemen: (1) obere Schranken für die minimale Anzahl von Seiten (Kanten und Facetten), (2) eine Vermutung von Mihail und Vazirani über die Kantenexpansion und (3) zufällige 0/1-Polytope. Zufälligen 0/1-Polytope können aus zwei verschiedenen Blickwinkeln betrachtet werden. Zum einen kann man Zufall benutzen, um die Komplexität allgemeiner 0/1-Polytope zu durchbrechen. Zum anderen kann man untersuchen, welche Eigenschaften zufällige 0/1-Polytope mit hoher Wahrscheinlichkeit besitzen. Dies sind dann typische Eigenschaften von 0/1-Polytopen und können nützlich sein, um Eigenschaften spezieller 0/1-Polytope einzuordnen. Nach einer kurzen Einführung in die Grundlagen der Polytoptheorie und einen Überblick über den aktuellen Stand der Forschung auf dem Gebiet der 0/1-Polytope, werden Grundlagen aus der Wahrscheinlichkeitstheorie kurz eingeführt. Die Ergebnisse zu zufälligen 0/1-Polytopen (3) umfassen: (a) Schwellenwert Resultate mit exponentieller Konvergenz zu der Wahrscheinlichkeit, dass {\textbackslash}k{\textbackslash} zufällige Ecken eines zufälligen 0/1-Polytopes eine {\textbackslash}(k-1){\textbackslash}-dimensionale Seite bilden; (b) Es gibt 0/1-Polytope mit exponentiell kleiner Eckenexpansion; (c) für jedes {\textbackslash}k{\textbackslash} gibt es eine Konstante {\textbackslash}c\_k{\textbackslash}, so dass ein zufälliges 0/1-Polytop in {\textbackslash}[0,1]ˆd{\textbackslash} mit höchstens {\textbackslash}2ˆ\{cd\}{\textbackslash} Ecken sehr wahrscheinlich {\textbackslash}k{\textbackslash}-nachbarschaftlich ist. Ergebnis (b) zeigt, dass es für die Vermutung von Mihail und Vazirani (2) nicht genügt die Eckenexpansion zu untersuchen. Allerdings scheint es schwierig oder unmöglich zu sein, dieses Negativresultat auf die Kantenexpansion auszuweiten. Ergebnisse (a) und (c) zeigen typische Eigenschaften von zufälligen 0/1-Polytopen. Hieraus lässt sich schließen, dass ein vollständiger Graph für 0/1-Polytope aus der kombinatorischen Optimierung nichts besonderes sind, da diese Polytope meistens nur sub-exponentiell viele Ecken haben. Bekannte Beispiele für 0/1-Polytope aus der kombinatorischen Optimierung sind Traveling Salesman Polytope” und Cut-Polytope”. Im letzten Kapitel stellen wir revlex-initiale 0/1-Polytope vor. Diese speziellen Knap{\textbackslash}textbackslash-sack-Poly{\textbackslash}textbackslash-tope haben sehr wenige Facetten und Kanten. Wir benutzen diese Polytope, um zu zeigen, dass es in jeder Dimension {\textbackslash}d{\textbackslash} und sinnvoller Eckenzahl {\textbackslash}n{\textbackslash} ein {\textbackslash}d{\textbackslash}-dimensionales 0/1-Polytope mit {\textbackslash}n{\textbackslash} Ecken gibt, welches höchstens {\textbackslash}3d{\textbackslash} Facetten und Eckengrad höchstens {\textbackslash}d+8{\textbackslash} hat. Dies sind die ersten Resultate zu oberen Schranken an die minimale Anzahl der Seiten von 0/1-Polytopen (1). Trotz des dünnen Graphen erfüllen diese 0/1-Polytope die Vermutung von Mihail und Vazirani (2). Abschließend betrachten wir noch kurz "erweiterte revlex-initiale 0/1-Polytope", welche eine weiterführende Richtung in der Untersuchung von Knapsack-Polytopen im Hinblick auf die Vermutung von Mihail und Vazirani sein können.},
	language = {English},
	urldate = {2025-03-04},
	author = {Gillmann, Rafael},
	month = feb,
	year = {2007},
}

@incollection{ziegler_lectures_2000,
	address = {Basel},
	title = {Lectures on 0/1-{Polytopes}},
	isbn = {978-3-0348-8438-9},
	url = {https://doi.org/10.1007/978-3-0348-8438-9_1},
	abstract = {These lectures on the combinatorics and geometry of 0/1-polytopes are meant as anintroductionandinvitation.Rather than heading for an extensive survey on 0/1-polytopes I present some interesting aspects of these objects; all of them are related to some quite recent work and progress.},
	language = {en},
	urldate = {2025-03-04},
	booktitle = {Polytopes — {Combinatorics} and {Computation}},
	publisher = {Birkhäuser},
	author = {Ziegler, Günter M.},
	editor = {Kalai, Gil and Ziegler, Günter M.},
	year = {2000},
	doi = {10.1007/978-3-0348-8438-9_1},
	pages = {1--41},
}

@misc{foundation_python_2025,
	title = {Python {Language} {Reference}, version 3.13.2},
	url = {https://docs.python.org/3/},
	urldate = {2025-03-06},
	author = {Foundation, Python Software},
	month = apr,
	year = {2025},
}

@book{vershynin_high-dimensional_2018,
	address = {New York, NY},
	edition = {1st edition},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	month = sep,
	year = {2018},
}

@book{baader_introduction_2017,
	address = {Cambridge},
	edition = {1},
	title = {An {Introduction} to {Description} {Logic}},
	isbn = {978-0-521-69542-8},
	abstract = {Description logics (DLs) have a long tradition in computer science and knowledge representation, being designed so that domain knowledge can be described and so that computers can reason about this knowledge. DLs have recently gained increased importance since they form the logical basis of widely used ontology languages, in particular the web ontology language OWL. Written by four renowned experts, this is the first textbook on description logics. It is suitable for self-study by graduates and as the basis for a university course. Starting from a basic DL, the book introduces the reader to their syntax, semantics, reasoning problems and model theory and discusses the computational complexity of these reasoning problems and algorithms to solve them. It then explores a variety of reasoning techniques, knowledge-based applications and tools and it describes the relationship between DLs and OWL.},
	language = {Englisch},
	publisher = {Cambridge University Press},
	author = {Baader, Franz and Horrocks, Ian and Lutz, Carsten and Sattler, Uli},
	month = apr,
	year = {2017},
}

@book{baader_description_2010,
	address = {Cambridge},
	edition = {2. Overcoloured. edition},
	title = {The {Description} {Logic} {Handbook}: {Theory}, {Implementation} and {Applications}},
	isbn = {978-0-521-15011-8},
	shorttitle = {The {Description} {Logic} {Handbook}},
	abstract = {Description logics are embodied in several knowledge-based systems and are used to develop various real-life applications. Now in paperback, The Description Logic Handbook provides a thorough account of the subject, covering all aspects of research in this field, namely: theory, implementation, and applications. Its appeal will be broad, ranging from more theoretically oriented readers, to those with more practically oriented interests who need a sound and modern understanding of knowledge representation systems based on description logics. As well as general revision throughout the book, this new edition presents a new chapter on ontology languages for the semantic web, an area of great importance for the future development of the web. In sum, the book will serve as a unique resource for the subject, and can also be used for self-study or as a reference for knowledge representation and artificial intelligence courses.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Baader, Franz},
	month = may,
	year = {2010},
}

@article{clifford_markov_1971,
	title = {Markov fields on finite graphs and lattices},
	url = {https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6},
	language = {English},
	urldate = {2025-03-11},
	journal = {Unpublished},
	author = {Clifford, P. and Hammersley, J. M.},
	year = {1971},
	annote = {Publisher: University of Oxford},
}

@incollection{rudolph_foundations_2011,
	address = {Berlin, Heidelberg},
	title = {Foundations of {Description} {Logics}},
	isbn = {978-3-642-23032-5},
	url = {https://doi.org/10.1007/978-3-642-23032-5_2},
	abstract = {This chapter accompanies the foundational lecture on Description Logics (DLs) at the 7th Reasoning Web Summer School in Galway, Ireland, 2011. It introduces basic notions and facts about this family of logics which has significantly gained in importance over the recent years as these logics constitute the formal basis for today’s most expressive ontology languages, the OWL (Web Ontology Language) family.},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Reasoning {Web}. {Semantic} {Technologies} for the {Web} of {Data}: 7th {International} {Summer} {School} 2011, {Galway}, {Ireland}, {August} 23-27, 2011, {Tutorial} {Lectures}},
	publisher = {Springer},
	author = {Rudolph, Sebastian},
	editor = {Polleres, Axel and d’Amato, Claudia and Arenas, Marcelo and Handschuh, Siegfried and Kroner, Paula and Ossowski, Sascha and Patel-Schneider, Peter},
	year = {2011},
	doi = {10.1007/978-3-642-23032-5_2},
	pages = {76--136},
}

@article{rauhut_tensor_2021,
	title = {Tensor theta norms and low rank recovery},
	volume = {88},
	issn = {1572-9265},
	url = {https://doi.org/10.1007/s11075-020-01029-x},
	doi = {10.1007/s11075-020-01029-x},
	abstract = {We study extensions of compressive sensing and low rank matrix recovery to the recovery of tensors of low rank from incomplete linear information. While the reconstruction of low rank matrices via nuclear norm minimization is rather well-understand by now, almost no theory is available so far for the extension to higher order tensors due to various theoretical and computational difficulties arising for tensor decompositions. In fact, nuclear norm minimization for matrix recovery is a tractable convex relaxation approach, but the extension of the nuclear norm to tensors is in general NP-hard to compute. In this article, we introduce convex relaxations of the tensor nuclear norm which are computable in polynomial time via semidefinite programming. Our approach is based on theta bodies, a concept from real computational algebraic geometry which is similar to the one of the better known Lasserre relaxations. We introduce polynomial ideals which are generated by the second-order minors corresponding to different matricizations of the tensor (where the tensor entries are treated as variables) such that the nuclear norm ball is the convex hull of the algebraic variety of the ideal. The theta body of order k for such an ideal generates a new norm which we call the θk-norm. We show that in the matrix case, these norms reduce to the standard nuclear norm. For tensors of order three or higher however, we indeed obtain new norms. The sequence of the corresponding unit-θk-norm balls converges asymptotically to the unit tensor nuclear norm ball. By providing the Gröbner basis for the ideals, we explicitly give semidefinite programs for the computation of the θk-norm and for the minimization of the θk-norm under an affine constraint. Finally, numerical experiments for order-three tensor recovery via θ1-norm minimization suggest that our approach successfully reconstructs tensors of low rank from incomplete linear (random) measurements.},
	language = {en},
	number = {1},
	urldate = {2025-03-19},
	journal = {Numerical Algorithms},
	author = {Rauhut, Holger and Stojanac, Željka},
	month = sep,
	year = {2021},
	keywords = {13P10, 15A60, 15A69, 52A41, 90C22, 90C25, 94A20, Compressive sensing, Convex relaxation, Gröbner bases, Low rank tensor recovery, Polynomial ideals, Semidefinite programming, Tensor nuclear norm, Theta bodies},
	pages = {25--66},
}

@article{mackworth_consistency_1977,
	title = {Consistency in networks of relations},
	volume = {8},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/0004370277900078},
	doi = {10.1016/0004-3702(77)90007-8},
	abstract = {Artificial intelligence tasks which can be formulated as constraint satisfaction problems, with which this paper is for the most part concerned, are usually by solved backtracking the examining the thrashing behavior that nearly always accompanies backtracking, identifying three of its causes and proposing remedies for them we are led to a class of algorithms whoch can profitably be used to eliminate local (node, arc and path) inconsistencies before any attempt is made to construct a complete solution. A more general paradigm for attacking these tasks is the altenation of constraint manipulation and case analysis producing an OR problem graph which may be searched in any of the usual ways. Many authors, particularly Montanari and Waltz, have contributed to the development of these ideas; a secondary aim of this paper is to trace that history. The primary aim is to provide an accessible, unified framework, within which to present the algorithms including a new path consistency algorithm, to discuss their relationships and the may applications, both realized and potential of network consistency algorithms.},
	number = {1},
	urldate = {2025-03-29},
	journal = {Artificial Intelligence},
	author = {Mackworth, Alan K.},
	month = feb,
	year = {1977},
	pages = {99--118},
}

@inproceedings{agerbeck_multi-agent_2008,
	title = {A {Multi}-{Agent} {Approach} to {Solving} {NP}-{Complete} {Problems}},
	url = {https://www.semanticscholar.org/paper/A-Multi-Agent-Approach-to-Solving-NP-Complete-Agerbeck-Hansen/3762bf7893da14839e06ae000b9e04d63dac8af4},
	abstract = {This master's project concerns the use of multi-agent design principles in making efficient solvers for NP-complete problems. The design of computer programs as multi-agent systems presents a new and very promising software engineering paradigm, where systems are described as individual problem-solving agents pursuing high-level goals. Recently, researchers have started to apply the multi-agent paradigm to the construction of efficient solvers for NP-complete problems. This has resulted in very effective tools for routing problems, graph partitioning and SAT-solving. The objective of the present project is to make further studies into the application of multi-agent principles to solving NP-complete problems. More specifically, the project has the following two goals. First, it should result in a general discussion of the use of multi-agent approaches to solving NP-complete problems. This should include a discussion of strengths and weaknesses compared to other approaches of solving the same problems. Second, it should result in a concrete software tool for solving n2 £ n2 Sudoku puzzles, which is known to be an NP-complete problem. The tool should be benchmarked against other solvers for Sudoku.},
	urldate = {2025-03-29},
	author = {Agerbeck, Christian and Hansen, Mikael},
	year = {2008},
}

@inproceedings{simonis_sudoku_2005,
	title = {Sudoku as a constraint problem},
	volume = {12},
	url = {https://ai.dmi.unibas.ch/_files/teaching/fs21/ai/material/ai26-simonis-cp2005ws.pdf},
	urldate = {2025-03-29},
	booktitle = {{CP} {Workshop} on modeling and reformulating {Constraint} {Satisfaction} {Problems}},
	publisher = {Citeseer Sitges, Spain},
	author = {Simonis, Helmut},
	year = {2005},
	pages = {13--27},
}

@misc{garcez_neural-symbolic_2019,
	title = {Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}},
	shorttitle = {Neural-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/1905.06088},
	abstract = {Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1905.06088},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {arXiv:1905.06088 [cs]},
}

@article{towell_knowledge-based_1994,
	title = {Knowledge-based artificial neural networks},
	volume = {70},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/0004370294901058},
	doi = {10.1016/0004-3702(94)90105-8},
	abstract = {Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific “domain theories”, represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.},
	number = {1},
	urldate = {2025-04-02},
	journal = {Artificial Intelligence},
	author = {Towell, Geoffrey G. and Shavlik, Jude W.},
	month = oct,
	year = {1994},
	keywords = {Computational biology, Connectionism, Explanation-based learning, Hybrid algorithms, Machine learning, Theory refinement},
	pages = {119--165},
}

@article{avila_garcez_connectionist_1999,
	title = {The {Connectionist} {Inductive} {Learning} and {Logic} {Programming} {System}},
	volume = {11},
	issn = {1573-7497},
	url = {https://doi.org/10.1023/A:1008328630915},
	doi = {10.1023/A:1008328630915},
	abstract = {This paper presents the Connectionist Inductive Learning and Logic Programming System (C-IL2P). C-IL2P is a new massively parallel computational model based on a feedforward Artificial Neural Network that integrates inductive learning from examples and background knowledge, with deductive learning from Logic Programming. Starting with the background knowledge represented by a propositional logic program, a translation algorithm is applied generating a neural network that can be trained with examples. The results obtained with this refined network can be explained by extracting a revised logic program from it. Moreover, the neural network computes the stable model of the logic program inserted in it as background knowledge, or learned with the examples, thus functioning as a parallel system for Logic Programming. We have successfully applied C-IL2P to two real-world problems of computational biology, specifically DNA sequence analyses. Comparisons with the results obtained by some of the main neural, symbolic, and hybrid inductive learning systems, using the same domain knowledge, show the effectiveness of C-IL2P.},
	language = {en},
	number = {1},
	urldate = {2025-04-02},
	journal = {Applied Intelligence},
	author = {Avila Garcez, Artur S. and Zaverucha, Gerson},
	month = jul,
	year = {1999},
	keywords = {Artificial Intelligence, artificial neural networks, computational biology, logic programming, machine learning, theory refinement},
	pages = {59--77},
}

@incollection{mccarthy_programs_1959,
	address = {London},
	title = {Programs with {Common} {Sense}},
	url = {http://www-formal.stanford.edu/jmc/mcc59.html},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the {Teddington} {Conference} on the {Mechanization} of {Thought} {Processes}},
	publisher = {Her Majesty's Stationary Office},
	author = {McCarthy, John},
	year = {1959},
	pages = {75--91},
}

@inproceedings{nickel_three-way_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {A three-way model for collective learning on multi-relational data},
	isbn = {978-1-4503-0619-5},
	abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
	month = jun,
	year = {2011},
	pages = {809--816},
}

@article{balazevic_tucker_2019,
	title = {{TuckER}: {Tensor} {Factorization} for {Knowledge} {Graph} {Completion}},
	shorttitle = {{TuckER}},
	url = {https://www.aclweb.org/anthology/D19-1522},
	doi = {10.18653/v1/D19-1522},
	abstract = {Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.},
	language = {en},
	urldate = {2025-04-02},
	journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	author = {Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
	year = {2019},
	pages = {5184--5193},
	annote = {Conference Name: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) Place: Hong Kong, China Publisher: Association for Computational Linguistics},
}

@incollection{bigerl_tentris_2020,
	address = {Cham},
	title = {Tentris – {A} {Tensor}-{Based} {Triple} {Store}},
	volume = {12506},
	isbn = {978-3-030-62418-7 978-3-030-62419-4},
	url = {https://link.springer.com/10.1007/978-3-030-62419-4_4},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {The {Semantic} {Web} – {ISWC} 2020},
	publisher = {Springer International Publishing},
	author = {Bigerl, Alexander and Conrads, Felix and Behning, Charlotte and Sherif, Mohamed Ahmed and Saleem, Muhammad and Ngonga Ngomo, Axel-Cyrille},
	editor = {Pan, Jeff Z. and Tamma, Valentina and d’Amato, Claudia and Janowicz, Krzysztof and Fu, Bo and Polleres, Axel and Seneviratne, Oshani and Kagal, Lalana},
	year = {2020},
	doi = {10.1007/978-3-030-62419-4_4},
	pages = {56--73},
	annote = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{serafini_learning_2016,
	address = {Berlin, Heidelberg},
	title = {Learning and {Reasoning} with {Logic} {Tensor} {Networks}},
	isbn = {978-3-319-49129-5},
	url = {https://doi.org/10.1007/978-3-319-49130-1_25},
	doi = {10.1007/978-3-319-49130-1_25},
	abstract = {The paper introduces real logic: a framework that seamlessly integrates logical deductive reasoning with efficient, data-driven relational learning. Real logic is based on full first order language. Terms are interpreted in n-dimensional feature vectors, while predicates are interpreted in fuzzy sets. In real logic it is possible to formally define the following two tasks: (i) learning from data in presence of logical constraints, and (ii) reasoning on formulas exploiting concrete data. We implement real logic in an deep learning architecture, called logic tensor networks, based on Google’s primitives. The paper concludes with experiments on a simple but representative example of knowledge completion.},
	urldate = {2025-04-02},
	booktitle = {{AI}*{IA} 2016 {Advances} in {Artificial} {Intelligence}: {XVth} {International} {Conference} of the {Italian} {Association} for {Artificial} {Intelligence}, {Genova}, {Italy}, {November} 29 – {December} 1, 2016, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Serafini, Luciano and d’Avila Garcez, Artur S.},
	month = nov,
	year = {2016},
	pages = {334--348},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	doi = {10.48550/arXiv.1912.01703},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	annote = {arXiv:1912.01703 [cs]},
}

@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2025-04-02},
	publisher = {arXiv},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	doi = {10.48550/arXiv.1603.04467},
	keywords = {Computer Science - Machine Learning, and Cluster Computing, Computer Science - Distributed, Parallel},
	annote = {arXiv:1603.04467 [cs]},
}

@inproceedings{nikolic_survey_2022,
	title = {A {Survey} of {Three} {Types} of {Processing} {Units}: {CPU}, {GPU} and {TPU}},
	shorttitle = {A {Survey} of {Three} {Types} of {Processing} {Units}},
	url = {https://ieeexplore.ieee.org/document/9828625},
	doi = {10.1109/ICEST55168.2022.9828625},
	abstract = {The CPU, GPU, and TPU are three different types of processing units. For the overall performance of the computer, the CPU is responsible. For delivering high-end graphics and video quality, the GPU is responsible. Along with the CPU, the GPU is a piece of additional hardware. TPU is used in the field of Artificial Intelligence, Machine Learning, and Deep Learning. Each of the three processing units has its own set of functions. This article may be of help to a reader with aim to understand the distinctions between the CPU, GPU, and TPU processing units.},
	urldate = {2025-04-02},
	booktitle = {2022 57th {International} {Scientific} {Conference} on {Information}, {Communication} and {Energy} {Systems} and {Technologies} ({ICEST})},
	author = {Nikolić, Goran S. and Dimitrijević, Bojan R. and Nikolić, Tatjana R. and Stojcev, Mile K.},
	month = jun,
	year = {2022},
	keywords = {Central Processing Unit, Computer performance, CPU, Deep learning, GPU, Graphics processing units, Hardware, hardware accelerator, Quality assessment, TPU, Video recording},
	pages = {1--6},
}

@inproceedings{jouppi_tpu_2023,
	address = {New York, NY, USA},
	series = {{ISCA} '23},
	title = {{TPU} v4: {An} {Optically} {Reconfigurable} {Supercomputer} for {Machine} {Learning} with {Hardware} {Support} for {Embeddings}},
	isbn = {979-8-4007-0095-8},
	shorttitle = {{TPU} v4},
	url = {https://dl.acm.org/doi/10.1145/3579371.3589350},
	doi = {10.1145/3579371.3589350},
	abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are \&lt;5\% of system cost and \&lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x–7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of {\textbackslash}textasciitilde60\% of peak FLOPS/second. For similar sized systems, it is {\textbackslash}textasciitilde4.3x–4.5x faster than the Graphcore IPU Bow and is 1.2x–1.7x faster and uses 1.3x–1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use {\textbackslash}textasciitilde2–6x less energy and produce {\textbackslash}textasciitilde20x less CO2e than contemporary DSAs in typical on-premise data centers.},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 50th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
	month = jun,
	year = {2023},
	pages = {1--14},
}

@book{degroot_probability_2016,
	title = {Probability and {Statistics}},
	isbn = {978-93-325-7387-1},
	abstract = {Brand New},
	language = {English},
	publisher = {PEARSON INDIA},
	author = {DeGroot, Morris H.},
	month = jan,
	year = {2016},
}

@book{murphy_probabilistic_2022,
	address = {Cambridge, Massachusetts London, England},
	title = {Probabilistic {Machine} {Learning}: {An} {Introduction}},
	isbn = {978-0-262-04682-4},
	shorttitle = {Probabilistic {Machine} {Learning}},
	abstract = {A detailed and up-to-date introduction to machine learning, presented through the unifying lens of probabilistic modeling and Bayesian decision theory.This book offers a detailed and up-to-date introduction to machine learning (including deep learning) through the unifying lens of probabilistic modeling and Bayesian decision theory. The book covers mathematical background (including linear algebra and optimization), basic supervised learning (including linear and logistic regression and deep neural networks), as well as more advanced topics (including transfer learning and unsupervised learning). End-of-chapter exercises allow students to apply what they have learned, and an appendix covers notation. Probabilistic Machine Learning grew out of the author’s 2012 book, Machine Learning: A Probabilistic Perspective. More than just a simple update, this is a completely new book that reflects the dramatic developments in the field since 2012, most notably deep learning. In addition, the new book is accompanied by online Python code, using libraries such as scikit-learn, JAX, PyTorch, and Tensorflow, which can be used to reproduce nearly all the figures; this code can be run inside a web browser using cloud-based notebooks, and provides a practical complement to the theoretical topics discussed in the book. This introductory text will be followed by a sequel that covers more advanced topics, taking the same probabilistic approach.},
	language = {English},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	month = mar,
	year = {2022},
}

@article{shannon_mathematical_1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {27},
	copyright = {© 1948 The Bell System Technical Journal},
	issn = {1538-7305},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	language = {en},
	number = {3},
	urldate = {2025-04-04},
	journal = {Bell System Technical Journal},
	author = {Shannon, C. E.},
	year = {1948},
	pages = {379--423},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	edition = {2nd edition},
	title = {Elements of {Information} {Theory}},
	isbn = {978-0-471-24195-9},
	abstract = {The latest edition of this classic is updated with new problem sets and material The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory. All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points. The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	month = sep,
	year = {2006},
}

@phdthesis{motzkin_beitrage_1936,
	address = {Jerusalem},
	type = {{PhD} {Thesis}},
	title = {Beiträge zur {Theorie} der linearen {Ungleichungen}},
	language = {ger},
	school = {Buchdruckerei Azriel},
	author = {Motzkin, Theodore S.},
	year = {1936},
	annote = {Book Title: Beiträge zur Theorie der linearen Ungleichungen},
}

@article{gray_quimb_2018,
	title = {quimb: {A} python package for quantum information and many-body calculations},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {quimb},
	url = {https://joss.theoj.org/papers/10.21105/joss.00819},
	doi = {10.21105/joss.00819},
	abstract = {Gray, (2018). quimb: A python package for quantum information and many-body calculations. Journal of Open Source Software, 3(29), 819, https://doi.org/10.21105/joss.00819},
	language = {en},
	number = {29},
	urldate = {2025-05-06},
	journal = {Journal of Open Source Software},
	author = {Gray, Johnnie},
	month = sep,
	year = {2018},
	pages = {819},
}

@misc{roberts_tensornetwork_2019,
	title = {{TensorNetwork}: {A} {Library} for {Physics} and {Machine} {Learning}},
	shorttitle = {{TensorNetwork}},
	url = {http://arxiv.org/abs/1905.01330},
	abstract = {TensorNetwork is an open source library for implementing tensor network algorithms. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers.},
	urldate = {2025-05-06},
	publisher = {arXiv},
	author = {Roberts, Chase and Milsted, Ashley and Ganahl, Martin and Zalcman, Adam and Fontaine, Bruce and Zou, Yijian and Hidary, Jack and Vidal, Guifre and Leichenauer, Stefan},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1905.01330},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Strongly Correlated Electrons, High Energy Physics - Theory, Physics - Computational Physics},
	annote = {arXiv:1905.01330 [physics]},
}

@misc{wang_anomaly_2020,
	title = {Anomaly {Detection} with {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2006.02516},
	abstract = {Originating from condensed matter physics, tensor networks are compact representations of high-dimensional tensors. In this paper, the prowess of tensor networks is demonstrated on the particular task of one-class anomaly detection. We exploit the memory and computational efficiency of tensor networks to learn a linear transformation over a space with dimension exponential in the number of original features. The linearity of our model enables us to ensure a tight fit around training instances by penalizing the model's global tendency to a predict normality via its Frobenius norm—a task that is infeasible for most deep learning models. Our method outperforms deep and classical algorithms on tabular datasets and produces competitive results on image datasets, despite not exploiting the locality of images.},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Wang, Jinhui and Roberts, Chase and Vidal, Guifre and Leichenauer, Stefan},
	month = jun,
	year = {2020},
	doi = {10.48550/arXiv.2006.02516},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantum Physics},
	annote = {arXiv:2006.02516 [cs]},
}

@misc{loconte_what_2025,
	title = {What is the {Relationship} between {Tensor} {Factorizations} and {Circuits} (and {How} {Can} {We} {Exploit} it)?},
	url = {http://arxiv.org/abs/2409.07953},
	abstract = {This paper establishes a rigorous connection between circuit representations and tensor factorizations, two seemingly distinct yet fundamentally related areas. By connecting these fields, we highlight a series of opportunities that can benefit both communities. Our work generalizes popular tensor factorizations within the circuit language, and unifies various circuit learning algorithms under a single, generalized hierarchical factorization framework. Specifically, we introduce a modular "Lego block" approach to build tensorized circuit architectures. This, in turn, allows us to systematically construct and explore various circuit and tensor factorization models while maintaining tractability. This connection not only clarifies similarities and differences in existing models, but also enables the development of a comprehensive pipeline for building and optimizing new circuit/tensor factorization architectures. We show the effectiveness of our framework through extensive empirical evaluations, and highlight new research opportunities for tensor factorizations in probabilistic modeling.},
	urldate = {2025-05-09},
	publisher = {arXiv},
	author = {Loconte, Lorenzo and Mari, Antonio and Gala, Gennaro and Peharz, Robert and Campos, Cassio de and Quaeghebeur, Erik and Vessio, Gennaro and Vergari, Antonio},
	month = feb,
	year = {2025},
	doi = {10.48550/arXiv.2409.07953},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv:2409.07953 [cs]},
}

@article{suess_mpnum_2017,
	title = {mpnum: {A} matrix product representation library for {Python}},
	volume = {2},
	issn = {2475-9066},
	shorttitle = {mpnum},
	url = {https://joss.theoj.org/papers/10.21105/joss.00465},
	doi = {10.21105/joss.00465},
	abstract = {Suess et al., (2017). mpnum: A matrix product representation library for Python. Journal of Open Source Software, 2(20), 465, https://doi.org/10.21105/joss.00465},
	language = {en},
	number = {20},
	urldate = {2025-06-23},
	journal = {Journal of Open Source Software},
	author = {Suess, Daniel and Holzäpfel, Milan},
	month = dec,
	year = {2017},
	pages = {465},
}

@misc{gels_pgelssscikit_tt_2025,
	title = {{PGelss}/scikit\_tt},
	copyright = {LGPL-3.0},
	url = {https://github.com/PGelss/scikit_tt},
	abstract = {Tensor Train Toolbox},
	urldate = {2025-06-23},
	author = {Gelß, Patrick},
	month = jun,
	year = {2025},
	keywords = {als, data-driven, dmrg, dynamic-mode-decomposition, dynamical-systems, extended-dynamic-mode-decomposition, mals, mandy, quantum-simulation, scikit, slim-decomposition, tensor, tensor-decomposition, tensor-network, tensor-train},
	annote = {original-date: 2018-11-23T13:10:49Z},
}

@misc{wolf_libxerusxerus_2024,
	title = {libxerus/xerus},
	copyright = {AGPL-3.0},
	url = {https://github.com/libxerus/xerus},
	abstract = {A general purpose library for numerical calculations with higher order tensors, Tensor-Train Decompositions / Matrix Product States and other Tensor Networks},
	urldate = {2025-06-23},
	publisher = {libxerus},
	author = {Wolf, Sebastian},
	month = feb,
	year = {2024},
	annote = {original-date: 2016-03-10T15:27:47Z},
}

@misc{puljak_tn4ml_2025,
	title = {tn4ml: {Tensor} {Network} {Training} and {Customization} for {Machine} {Learning}},
	shorttitle = {tn4ml},
	url = {http://arxiv.org/abs/2502.13090},
	abstract = {Tensor Networks have emerged as a prominent alternative to neural networks for addressing Machine Learning challenges in foundational sciences, paving the way for their applications to real-life problems. This paper introduces tn4ml, a novel library designed to seamlessly integrate Tensor Networks into optimization pipelines for Machine Learning tasks. Inspired by existing Machine Learning frameworks, the library offers a user-friendly structure with modules for data embedding, objective function definition, and model training using diverse optimization strategies. We demonstrate its versatility through two examples: supervised learning on tabular data and unsupervised learning on an image dataset. Additionally, we analyze how customizing the parts of the Machine Learning pipeline for Tensor Networks influences performance metrics.},
	urldate = {2025-06-23},
	publisher = {arXiv},
	author = {Puljak, Ema and Sanchez-Ramirez, Sergio and Masot-Llima, Sergi and Vallès-Muns, Jofre and Garcia-Saez, Artur and Pierini, Maurizio},
	month = feb,
	year = {2025},
	doi = {10.48550/arXiv.2502.13090},
	keywords = {Computer Science - Machine Learning, Quantum Physics, Computer Science - Mathematical Software},
	annote = {arXiv:2502.13090 [cs]},
}

@article{lu_tensor_2025,
	title = {Tensor networks and efficient descriptions of classical data},
	volume = {111},
	issn = {2469-9926, 2469-9934},
	url = {http://arxiv.org/abs/2103.06872},
	doi = {10.1103/PhysRevA.111.032409},
	abstract = {We investigate the potential of tensor network based machine learning methods to scale to large image and text data sets. For that, we study how the mutual information between a subregion and its complement scales with the subsystem size {\textbackslash}L{\textbackslash}, similarly to how it is done in quantum many-body physics. We find that for text, the mutual information scales as a power law {\textbackslash}Lˆ{\textbackslash}textbackslashnu{\textbackslash} with a close to volume law exponent, indicating that text cannot be efficiently described by 1D tensor networks. For images, the scaling is close to an area law, hinting at 2D tensor networks such as PEPS could have an adequate expressibility. For the numerical analysis, we introduce a mutual information estimator based on autoregressive networks, and we also use convolutional neural networks in a neural estimator method.},
	number = {3},
	urldate = {2025-07-07},
	journal = {Physical Review A},
	author = {Lu, Sirui and Kanász-Nagy, Márton and Kukuljan, Ivan and Cirac, J. Ignacio},
	month = mar,
	year = {2025},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantum Physics, Condensed Matter - Strongly Correlated Electrons},
	annote = {arXiv:2103.06872 [quant-ph]},
}

@article{dektor_tensor_2023,
	title = {Tensor rank reduction via coordinate flows},
	volume = {491},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999123004734},
	doi = {10.1016/j.jcp.2023.112378},
	abstract = {Recently, there has been a growing interest in efficient numerical algorithms based on tensor networks and low-rank techniques to approximate high-dimensional functions and solutions to high-dimensional PDEs. In this paper, we propose a new tensor rank reduction method based on coordinate transformations that can greatly increase the efficiency of high-dimensional tensor approximation algorithms. The idea is simple: given a multivariate function, determine a coordinate transformation so that the function in the new coordinate system has smaller tensor rank. We restrict our analysis to linear coordinate transformations, which gives rise to a new class of functions that we refer to as tensor ridge functions. Leveraging Riemannian gradient descent on matrix manifolds we develop an algorithm that determines a quasi-optimal linear coordinate transformation for tensor rank reduction. The results we present for rank reduction via linear coordinate transformations open the possibility for generalizations to larger classes of nonlinear transformations.},
	urldate = {2025-07-07},
	journal = {Journal of Computational Physics},
	author = {Dektor, Alec and Venturi, Daniele},
	month = oct,
	year = {2023},
	keywords = {Rank-adaptive tensor methods, Riemannian optimization, Tensor ridge functions, Tensor train},
	pages = {112378},
}

@misc{grelier_learning_2019,
	title = {Learning with tree-based tensor formats},
	url = {http://arxiv.org/abs/1811.04455},
	abstract = {This paper is concerned with the approximation of high-dimensional functions in a statistical learning setting, by empirical risk minimization over model classes of functions in tree-based tensor format. These are particular classes of rank-structured functions that can be seen as deep neural networks with a sparse architecture related to the tree and multilinear activation functions. For learning in a given model class, we exploit the fact that tree-based tensor formats are multilinear models and recast the problem of risk minimization over a nonlinear set into a succession of learning problems with linear models. Suitable changes of representation yield numerically stable learning problems and allow to exploit sparsity. For high-dimensional problems or when only a small data set is available, the selection of a good model class is a critical issue. For a given tree, the selection of the tuple of tree-based ranks that minimize the risk is a combinatorial problem. Here, we propose a rank adaptation strategy which provides in practice a good convergence of the risk as a function of the model class complexity. Finding a good tree is also a combinatorial problem, which can be related to the choice of a particular sparse architecture for deep neural networks. Here, we propose a stochastic algorithm for minimizing the complexity of the representation of a given function over a class of trees with a given arity, allowing changes in the topology of the tree. This tree optimization algorithm is then included in a learning scheme that successively adapts the tree and the corresponding tree-based ranks. Contrary to classical learning algorithms for nonlinear model classes, the proposed algorithms are numerically stable, reliable, and require only a low level expertise of the user.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Grelier, Erwan and Nouy, Anthony and Chevreuil, Mathilde},
	month = jan,
	year = {2019},
	doi = {10.48550/arXiv.1811.04455},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv:1811.04455 [stat]},
}

@article{ganahl_density_2023,
	title = {Density {Matrix} {Renormalization} {Group} with {Tensor} {Processing} {Units}},
	volume = {4},
	url = {https://link.aps.org/doi/10.1103/PRXQuantum.4.010317},
	doi = {10.1103/PRXQuantum.4.010317},
	abstract = {Google’s tensor processing units (TPUs) are integrated circuits specifically built to accelerate and scale up machine learning workloads. They can perform fast distributed matrix multiplications and therefore be repurposed for other computationally intensive tasks. In this work we demonstrate the use of TPUs for accelerating and scaling up the density matrix renormalization group (DMRG), a powerful numerical approach to compute the ground state of a local quantum many-body Hamiltonian. The cost of DMRG scales with system size 𝑁 as 𝑂⁡(𝑁⁢𝐷3), where the so-called bond dimension 𝐷 regulates how expressive the underlying matrix product state (MPS) variational ansatz is. We consider lattice models in two spatial dimensions, with square lattices of size 10 ×10 (free fermions) and 20 ×20 (transverse field Ising model), for which the required MPS bond dimension is known to scale at least as exp (√𝑁). Using half of a TPU v3 pod (namely 1024 TPU v3 cores), we reach an unprecedentedly large bond dimension 𝐷 =216 =65 536, for which optimizing a single MPS tensor takes about 2 min.},
	number = {1},
	urldate = {2025-07-07},
	journal = {PRX Quantum},
	author = {Ganahl, Martin and Beall, Jackson and Hauru, Markus and Lewis, Adam G.M. and Wojno, Tomasz and Yoo, Jae Hyeon and Zou, Yijian and Vidal, Guifre},
	month = feb,
	year = {2023},
	pages = {010317},
	annote = {Publisher: American Physical Society},
}

@misc{dubey_simulating_2025,
	title = {Simulating {Quantum} {Circuits} with {Tree} {Tensor} {Networks} using {Density}-{Matrix} {Renormalization} {Group} {Algorithm}},
	url = {http://arxiv.org/abs/2504.16718},
	abstract = {Quantum computing offers the potential for computational abilities that can go beyond classical machines. However, they are still limited by several challenges such as noise, decoherence, and gate errors. As a result, efficient classical simulation of quantum circuits is vital not only for validating and benchmarking quantum hardware but also for gaining deeper insights into the behavior of quantum algorithms. A promising framework for classical simulation is provided by tensor networks. Recently, the Density-Matrix Renormalization Group (DMRG) algorithm was developed for simulating quantum circuits using matrix product states (MPS). Although MPS is efficient for representing quantum states with one-dimensional correlation structures, the fixed linear geometry restricts the expressive power of the MPS. In this work, we extend the DMRG algorithm for simulating quantum circuits to tree tensor networks (TTNs). The framework employs a variational compression scheme that optimizes the TTN to approximate the evolved quantum state. To benchmark the method, we simulate random circuits and the quantum approximate optimization algorithm (QAOA) with various two-qubit gate connectivities. For the random circuits, we devise tree-like gate layouts that are suitable for TTN and show that TTN requires less memory than MPS for the simulations. For the QAOA circuits, a naive TTN construction that exploits graph structure significantly improves the simulation fidelities. Our findings show that the DMRG algorithm with TTNs provides a promising framework for simulating quantum circuits, particularly when gate connectivities exhibit clustering or a hierarchical structure.},
	urldate = {2025-07-11},
	publisher = {arXiv},
	author = {Dubey, Aditya and Zeybek, Zeki and Schmelcher, Peter},
	month = jul,
	year = {2025},
	doi = {10.48550/arXiv.2504.16718},
	keywords = {Quantum Physics},
	annote = {arXiv:2504.16718 [quant-ph]},
}

@book{vollmer_introduction_1999,
	address = {Berlin, Heidelberg},
	series = {Texts in {Theoretical} {Computer} {Science} {An} {EATCS} {Series}},
	title = {Introduction to {Circuit} {Complexity}: {A} {Uniform} {Approach}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-08398-3 978-3-662-03927-4},
	shorttitle = {Introduction to {Circuit} {Complexity}},
	url = {https://link.springer.com/10.1007/978-3-662-03927-4},
	language = {en},
	urldate = {2025-07-21},
	publisher = {Springer},
	author = {Vollmer, Heribert},
	editor = {Brauer, Wilfried and Rozenberg, Grzegorz and Salomaa, Arto},
	year = {1999},
	doi = {10.1007/978-3-662-03927-4},
	keywords = {algorithm analysis and problem complexity, algorithms, Algorithmus, Alphabet, Berechnungskomplexität, Berechnungsmodell, boolean circuits, complexity, complexity theory, computability, computational complexity, computational methods, Komplexitätsklassen, linear algebra, logic, Schaltkreis},
}

@misc{shrestha_natural_2023,
	title = {Natural {Gradient} {Methods}: {Perspectives}, {Efficient}-{Scalable} {Approximations}, and {Analysis}},
	shorttitle = {Natural {Gradient} {Methods}},
	url = {http://arxiv.org/abs/2303.05473},
	abstract = {Natural Gradient Descent, a second-degree optimization method motivated by the information geometry, makes use of the Fisher Information Matrix instead of the Hessian which is typically used. However, in many cases, the Fisher Information Matrix is equivalent to the Generalized Gauss-Newton Method, that both approximate the Hessian. It is an appealing method to be used as an alternative to stochastic gradient descent, potentially leading to faster convergence. However, being a second-order method makes it infeasible to be used directly in problems with a huge number of parameters and data. This is evident from the community of deep learning sticking with the stochastic gradient descent method since the beginning. In this paper, we look at the different perspectives on the natural gradient method, study the current developments on its efficient-scalable empirical approximations, and finally examine their performance with extensive experiments.},
	urldate = {2025-07-21},
	publisher = {arXiv},
	author = {Shrestha, Rajesh},
	month = mar,
	year = {2023},
	doi = {10.48550/arXiv.2303.05473},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv:2303.05473 [cs]},
}

@article{low_quantum_2014,
	title = {Quantum inference on {Bayesian} networks},
	volume = {89},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.89.062315},
	doi = {10.1103/PhysRevA.89.062315},
	abstract = {Performing exact inference on Bayesian networks is known to be \#𝑃-hard. Typically approximate inference techniques are used instead to sample from the distribution on query variables given the values 𝑒 of evidence variables. Classically, a single unbiased sample is obtained from a Bayesian network on 𝑛 variables with at most 𝑚 parents per node in time 𝑂⁡(𝑛⁢𝑚⁢𝑃⁢(𝑒)−1), depending critically on 𝑃⁡(𝑒), the probability that the evidence might occur in the first place. By implementing a quantum version of rejection sampling, we obtain a square-root speedup, taking 𝑂⁡(𝑛⁢2𝑚⁢𝑃⁢(𝑒)−12) time per sample. We exploit the Bayesian network's graph structure to efficiently construct a quantum state, a q-sample, representing the intended classical distribution, and also to efficiently apply amplitude amplification, the source of our speedup. Thus, our speedup is notable as it is unrelativized—we count primitive operations and require no blackbox oracle queries.},
	number = {6},
	urldate = {2025-07-23},
	journal = {Physical Review A},
	author = {Low, Guang Hao and Yoder, Theodore J. and Chuang, Isaac L.},
	month = jun,
	year = {2014},
	pages = {062315},
	annote = {Publisher: American Physical Society},
}

@article{wittek_quantum_2017,
	title = {Quantum {Enhanced} {Inference} in {Markov} {Logic} {Networks}},
	volume = {7},
	issn = {2045-2322},
	url = {http://arxiv.org/abs/1611.08104},
	doi = {10.1038/srep45672},
	abstract = {Markov logic networks (MLNs) reconcile two opposing schools in machine learning and artificial intelligence: causal networks, which account for uncertainty extremely well, and first-order logic, which allows for formal deduction. An MLN is essentially a first-order logic template to generate Markov networks. Inference in MLNs is probabilistic and it is often performed by approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling. An MLN has many regular, symmetric structures that can be exploited at both first-order level and in the generated Markov network. We analyze the graph structures that are produced by various lifting methods and investigate the extent to which quantum protocols can be used to speed up Gibbs sampling with state preparation and measurement schemes. We review different such approaches, discuss their advantages, theoretical limitations, and their appeal to implementations. We find that a straightforward application of a recent result yields exponential speedup compared to classical heuristics in approximate probabilistic inference, thereby demonstrating another example where advanced quantum resources can potentially prove useful in machine learning.},
	number = {1},
	urldate = {2025-07-23},
	journal = {Scientific Reports},
	author = {Wittek, Peter and Gogolin, Christian},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Quantum Physics},
	pages = {45672},
	annote = {arXiv:1611.08104 [stat]},
}

@article{boyd_proximal_nodate,
	title = {Proximal {Algorithm}},
	language = {en},
	author = {Boyd, Stephen},
}

@misc{zhao_d1_2025,
	title = {d1: {Scaling} {Reasoning} in {Diffusion} {Large} {Language} {Models} via {Reinforcement} {Learning}},
	shorttitle = {d1},
	url = {http://arxiv.org/abs/2504.12216},
	abstract = {Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/.},
	urldate = {2025-08-11},
	publisher = {arXiv},
	author = {Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya},
	month = jun,
	year = {2025},
	doi = {10.48550/arXiv.2504.12216},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {arXiv:2504.12216 [cs]},
}

@article{gray_hyper-optimized_2021,
	title = {Hyper-optimized tensor network contraction},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2002.01935},
	doi = {10.22331/q-2021-03-15-410},
	abstract = {Tensor networks represent the state-of-the-art in computational methods across many disciplines, including the classical simulation of quantum many-body systems and quantum circuits. Several applications of current interest give rise to tensor networks with irregular geometries. Finding the best possible contraction path for such networks is a central problem, with an exponential effect on computation time and memory footprint. In this work, we implement new randomized protocols that find very high quality contraction paths for arbitrary and large tensor networks. We test our methods on a variety of benchmarks, including the random quantum circuit instances recently implemented on Google quantum chips. We find that the paths obtained can be very close to optimal, and often many orders or magnitude better than the most established approaches. As different underlying geometries suit different methods, we also introduce a hyper-optimization approach, where both the method applied and its algorithmic parameters are tuned during the path finding. The increase in quality of contraction schemes found has significant practical implications for the simulation of quantum many-body systems and particularly for the benchmarking of new quantum chips. Concretely, we estimate a speed-up of over 10,000\{\vphantom{\}}{\textbackslash}textbackslashtimes{\textbackslash} compared to the original expectation for the classical simulation of the Sycamore `supremacy' circuits.},
	urldate = {2025-08-12},
	journal = {Quantum},
	author = {Gray, Johnnie and Kourtis, Stefanos},
	month = mar,
	year = {2021},
	keywords = {Quantum Physics, Physics - Computational Physics, Condensed Matter - Disordered Systems and Neural Networks},
	pages = {410},
	annote = {arXiv:2002.01935 [quant-ph]},
}

@misc{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks – multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with{\textbackslash}textbackslash,increased{\textbackslash}textbackslash,task{\textbackslash}textbackslash,complexity.},
	urldate = {2025-08-20},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = oct,
	year = {2023},
	doi = {10.48550/arXiv.2305.18654},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {arXiv:2305.18654 [cs]},
}

@article{michel_learning_2022,
	title = {Learning with tree tensor networks: complexity estimates and model selection},
	volume = {28},
	issn = {1350-7265},
	shorttitle = {Learning with tree tensor networks},
	url = {http://arxiv.org/abs/2007.01165},
	doi = {10.3150/21-BEJ1371},
	abstract = {Tree tensor networks, or tree-based tensor formats, are prominent model classes for the approximation of high-dimensional functions in computational and data science. They correspond to sum-product neural networks with a sparse connectivity associated with a dimension tree and widths given by a tuple of tensor ranks. The approximation power of these models has been proved to be (near to) optimal for classical smoothness classes. However, in an empirical risk minimization framework with a limited number of observations, the dimension tree and ranks should be selected carefully to balance estimation and approximation errors. We propose and analyze a complexity-based model selection method for tree tensor networks in an empirical risk minimization framework and we analyze its performance over a wide range of smoothness classes. Given a family of model classes associated with different trees, ranks, tensor product feature spaces and sparsity patterns for sparse tensor networks, a model is selected ({\textbackslash}textbackslash`a la Barron, Birg{\textbackslash}textbackslash'e, Massart) by minimizing a penalized empirical risk, with a penalty depending on the complexity of the model class and derived from estimates of the metric entropy of tree tensor networks. This choice of penalty yields a risk bound for the selected predictor. In a least-squares setting, after deriving fast rates of convergence of the risk, we show that our strategy is (near to) minimax adaptive to a wide range of smoothness classes including Sobolev or Besov spaces (with isotropic, anisotropic or mixed dominating smoothness) and analytic functions. We discuss the role of sparsity of the tensor network for obtaining optimal performance in several regimes. In practice, the amplitude of the penalty is calibrated with a slope heuristics method. Numerical experiments in a least-squares regression setting illustrate the performance of the strategy.},
	number = {2},
	urldate = {2025-08-26},
	journal = {Bernoulli},
	author = {Michel, Bertrand and Nouy, Anthony},
	month = may,
	year = {2022},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Statistics - Statistics Theory},
	annote = {arXiv:2007.01165 [math]},
}

@article{kourtis_fast_2019,
	title = {Fast counting with tensor networks},
	volume = {7},
	issn = {2542-4653},
	url = {https://scipost.org/10.21468/SciPostPhys.7.5.060},
	doi = {10.21468/SciPostPhys.7.5.060},
	abstract = {We introduce tensor network contraction algorithms for counting satisfying assignments of constraint satisfaction problems (\#CSPs). We represent each arbitrary \#CSP formula as a tensor network, whose full contraction yields the number of satisfying assignments of that formula, and use graph theoretical methods to determine favorable orders of contraction. We employ our heuristics for the solution of \#P-hard counting boolean satisfiability (\#SAT) problems, namely monotone \#1-in-3SAT and \#Cubic-Vertex-Cover, and find that they outperform state-of-the-art solvers by a significant margin.},
	number = {5},
	urldate = {2025-08-28},
	journal = {SciPost Physics},
	author = {Kourtis, Stefanos and Chamon, Claudio and Mucciolo, Eduardo and Ruckenstein, Andrei},
	month = nov,
	year = {2019},
	pages = {060},
}

@misc{garcia-saez_exact_2011,
	title = {An exact tensor network for the {3SAT} problem},
	url = {http://arxiv.org/abs/1105.3201},
	abstract = {We construct a tensor network that delivers an unnormalized quantum state whose coefficients are the solutions to a given instance of 3SAT, an NP-complete problem. The tensor network contraction that corresponds to the norm of the state counts the number of solutions to the instance. It follows that exact contractions of this tensor network are in the \#P-complete computational complexity class, thus believed to be a hard task. Furthermore, we show that for a 3SAT instance with n bits, it is enough to perform a polynomial number of contractions of the tensor network structure associated to the computation of local observables to obtain one of the explicit solutions to the problem, if any. Physical realization of a state described by a generic tensor network is equivalent to finding the satisfying assignment of a 3SAT instance and, consequently, this experimental task is expected to be hard.},
	urldate = {2025-08-28},
	publisher = {arXiv},
	author = {Garcia-Saez, A. and Latorre, J. I.},
	month = oct,
	year = {2011},
	doi = {10.48550/arXiv.1105.3201},
	keywords = {Quantum Physics, Computer Science - Computational Complexity, Condensed Matter - Statistical Mechanics},
	annote = {arXiv:1105.3201 [quant-ph]},
}

@misc{ali_explicit_2025,
	title = {Explicit {Solution} {Equation} for {Every} {Combinatorial} {Problem} via {Tensor} {Networks}: {MeLoCoToN}},
	shorttitle = {Explicit {Solution} {Equation} for {Every} {Combinatorial} {Problem} via {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2502.05981},
	abstract = {In this paper we show that every combinatorial problem has an exact explicit equation that returns its solution. We present a method to obtain an equation that solves exactly any combinatorial problem, both inversion, constraint satisfaction and optimization, by obtaining its equivalent tensor network. This formulation only requires a basic knowledge of classical logical operators, at a first year level of any computer science degree. These equations are not necessarily computable in a reasonable time, nor do they allow to surpass the state of the art in computational complexity, but they allow to have a new perspective for the mathematical analysis of these problems. These equations computation can be approximated by different methods such as Matrix Product State compression. We also present the equations for numerous combinatorial problems. This work proves that, if there is a physical system capable of contracting in polynomial time the tensor networks presented, every NP-Hard problem can be solved in polynomial time.},
	urldate = {2025-08-28},
	publisher = {arXiv},
	author = {Ali, Alejandro Mata},
	month = feb,
	year = {2025},
	doi = {10.48550/arXiv.2502.05981},
	keywords = {Computer Science - Emerging Technologies, Quantum Physics},
	annote = {arXiv:2502.05981 [cs]},
}

@misc{leskovec_kronecker_2009,
	title = {Kronecker {Graphs}: {An} {Approach} to {Modeling} {Networks}},
	shorttitle = {Kronecker {Graphs}},
	url = {http://arxiv.org/abs/0812.4905},
	abstract = {How can we model networks with a mathematically tractable model that allows for rigorous analysis of network properties? Networks exhibit a long list of surprising properties: heavy tails for the degree distribution; small diameters; and densification and shrinking diameters over time. Most present network models either fail to match several of the above properties, are complicated to analyze mathematically, or both. In this paper we propose a generative model for networks that is both mathematically tractable and can generate networks that have the above mentioned properties. Our main idea is to use the Kronecker product to generate graphs that we refer to as "Kronecker graphs". First, we prove that Kronecker graphs naturally obey common network properties. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super- exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on large real and synthetic networks show that KronFit finds accurate parameters that indeed very well mimic the properties of target networks. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null- models, anonymization, extrapolations, and graph summarization.},
	urldate = {2025-08-28},
	publisher = {arXiv},
	author = {Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
	month = aug,
	year = {2009},
	doi = {10.48550/arXiv.0812.4905},
	keywords = {Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Physics - Data Analysis, Physics - Physics and Society, Statistics and Probability},
	annote = {arXiv:0812.4905 [stat]},
}

@book{pinkus_n-widths_1985,
	address = {Berlin, Heidelberg},
	title = {n-{Widths} in {Approximation} {Theory}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-69896-5 978-3-642-69894-1},
	url = {http://link.springer.com/10.1007/978-3-642-69894-1},
	urldate = {2025-09-22},
	publisher = {Springer Berlin Heidelberg},
	author = {Pinkus, Allan},
	year = {1985},
	doi = {10.1007/978-3-642-69894-1},
}

@article{ali_approximation_2023,
	title = {Approximation {Theory} of {Tree} {Tensor} {Networks}: {Tensorized} {Univariate} {Functions} – {Part} {II}},
	volume = {58},
	issn = {0176-4276, 1432-0940},
	shorttitle = {Approximation {Theory} of {Tree} {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2007.00128},
	doi = {10.1007/s00365-023-09620-w},
	abstract = {We study the approximation by tensor networks (TNs) of functions from classical smoothness classes. The considered approximation tool combines a tensorization of functions in {\textbackslash}Lˆp([0,1)){\textbackslash}, which allows to identify a univariate function with a multivariate function (or tensor), and the use of tree tensor networks (the tensor train format) for exploiting low-rank structures of multivariate functions. The resulting tool can be interpreted as a feed-forward neural network, with first layers implementing the tensorization, interpreted as a particular featuring step, followed by a sum-product network with sparse architecture. In part I of this work, we presented several approximation classes associated with different measures of complexity of tensor networks and studied their properties. In this work (part II), we show how classical approximation tools, such as polynomials or splines (with fixed or free knots), can be encoded as a tensor network with controlled complexity. We use this to derive direct (Jackson) inequalities for the approximation spaces of tensor networks. This is then utilized to show that Besov spaces are continuously embedded into these approximation spaces. In other words, we show that arbitrary Besov functions can be approximated with optimal or near to optimal rate. We also show that an arbitrary function in the approximation class possesses no Besov smoothness, unless one limits the depth of the tensor network.},
	number = {2},
	urldate = {2025-09-22},
	journal = {Constructive Approximation},
	author = {Ali, Mazen and Nouy, Anthony},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Computer Science - Machine Learning, Mathematics - Functional Analysis},
	pages = {463--544},
	annote = {arXiv:2007.00118 [math]},
	annote = {arXiv:2007.00128 [math]},
}

@misc{wu_fast-dllm_2025,
	title = {Fast-{dLLM} v2: {Efficient} {Block}-{Diffusion} {LLM}},
	shorttitle = {Fast-{dLLM} v2},
	url = {http://arxiv.org/abs/2509.26328},
	abstract = {Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.},
	urldate = {2025-10-13},
	publisher = {arXiv},
	author = {Wu, Chengyue and Zhang, Hao and Xue, Shuchen and Diao, Shizhe and Fu, Yonggan and Liu, Zhijian and Molchanov, Pavlo and Luo, Ping and Han, Song and Xie, Enze},
	month = sep,
	year = {2025},
	doi = {10.48550/arXiv.2509.26328},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv:2509.26328 [cs]},
}

@misc{chen_coda_2025,
	title = {{CoDA}: {Coding} {LM} via {Diffusion} {Adaptation}},
	shorttitle = {{CoDA}},
	url = {http://arxiv.org/abs/2510.03270},
	abstract = {Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.},
	urldate = {2025-10-13},
	publisher = {arXiv},
	author = {Chen, Haolin and Wang, Shiyu and Qin, Can and Pang, Bo and Liu, Zuxin and Qiu, Jielin and Zhang, Jianguo and Zhou, Yingbo and Chen, Zeyuan and Xu, Ran and Heinecke, Shelby and Savarese, Silvio and Xiong, Caiming and Wang, Huan and Yao, Weiran},
	month = sep,
	year = {2025},
	doi = {10.48550/arXiv.2510.03270},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {arXiv:2510.03270 [cs]},
}

@misc{ergen_topological_2024,
	title = {Topological {Expressivity} of {ReLU} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2310.11130},
	doi = {10.48550/arXiv.2310.11130},
	abstract = {We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplification. This provides a mathematically rigorous explanation why deeper networks are better equipped to handle complex and topologically rich data sets.},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Ergen, Ekin and Grillo, Moritz},
	month = jun,
	year = {2024},
	note = {arXiv:2310.11130 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Discrete Mathematics, Mathematics - Algebraic Topology},
	annote = {Comment: 44 pages, to appear in COLT 2024},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/GEMUUU3Q/Ergen und Grillo - 2024 - Topological Expressivity of ReLU Neural Networks.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/N3RPAVP8/2310.html:text/html},
}

@misc{dahmen_expansive_2025,
	title = {Expansive {Natural} {Neural} {Gradient} {Flows} for {Energy} {Minimization}},
	url = {http://arxiv.org/abs/2507.13475},
	doi = {10.48550/arXiv.2507.13475},
	abstract = {This paper develops expansive gradient dynamics in deep neural network-induced mapping spaces. Specifically, we generate tools and concepts for minimizing a class of energy functionals in an abstract Hilbert space setting covering a wide scope of applications such as PDEs-based inverse problems and supervised learning. The approach hinges on a Hilbert space metric in the full diffeomorphism mapping space, which could be viewed as a generalized Wasserstein-2 metric. We then study a projection gradient descent method within deep neural network parameterized sets. More importantly, we develop an adaptation and expanding strategy to step-by-step enlarge the deep neural network structures. In particular, the expansion mechanism aims to enhance the alignment of the neural manifold induced natural gradient direction as well as possible with the ideal Hilbert space gradient descent direction leveraging the fact that we can evaluate projections of the Hilbert space gradient. We demonstrate the efficacy of the proposed strategy for several simple model problems for energies arising in the context of supervised learning, model reduction, or inverse problems. In particular, we highlight the importance of assembling the neural flow matrix based on the inner product for the ambient Hilbert space. The actual algorithms are the simplest specifications of a broader spectrum based on a correspondingly wider discussion, postponing a detailed analysis to forthcoming work.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Dahmen, Wolfgang and Li, Wuchen and Teng, Yuankai and Wang, Zhu},
	month = jul,
	year = {2025},
	note = {arXiv:2507.13475 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	annote = {Comment: 40 pages, 19 figures},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/Z5EN8YYJ/Dahmen et al. - 2025 - Expansive Natural Neural Gradient Flows for Energy Minimization.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/S3EXZPQR/2507.html:text/html},
}

@misc{georgiev_mathematical_2025,
	title = {Mathematical exploration and discovery at scale},
	url = {http://arxiv.org/abs/2511.02864},
	doi = {10.48550/arXiv.2511.02864},
	abstract = {AlphaEvolve is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems. To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights. These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.},
	urldate = {2025-11-08},
	publisher = {arXiv},
	author = {Georgiev, Bogdan and Gómez-Serrano, Javier and Tao, Terence and Wagner, Adam Zsolt},
	month = nov,
	year = {2025},
	note = {arXiv:2511.02864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Mathematics - Classical Analysis and ODEs, Mathematics - Combinatorics, Mathematics - Metric Geometry},
	annote = {Comment: 80 pages, 35 figures},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/66V2FAMY/Georgiev et al. - 2025 - Mathematical exploration and discovery at scale.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/DIL7L4N3/2511.html:text/html},
}

@book{ballard_tensor_2025,
	address = {Cambridge},
	title = {Tensor {Decompositions} for {Data} {Science}},
	isbn = {978-1-009-47167-1},
	url = {https://www.cambridge.org/core/books/tensor-decompositions-for-data-science/640814D308696CD61CB9112EA57B2911},
	abstract = {Tensors are essential in modern day computational and data sciences. This book explores the foundations of tensor decompositions, a data analysis methodology that is ubiquitous in machine learning, signal processing, chemometrics, neuroscience, quantum computing, financial analysis, social science, business market analysis, image processing, and much more. In this self-contained mathematical, algorithmic, and computational treatment of tensor decomposition, the book emphasizes examples using real-world downloadable open-source datasets to ground the abstract concepts. Methodologies for 3-way tensors (the simplest notation) are presented before generalizing to d-way tensors (the most general but complex notation), making the book accessible to advanced undergraduate and graduate students in mathematics, computer science, statistics, engineering, and physical and life sciences. Additionally, extensive background materials in linear algebra, optimization, probability, and statistics are included as appendices.},
	urldate = {2025-11-08},
	publisher = {Cambridge University Press},
	author = {Ballard, Grey and Kolda, Tamara G.},
	year = {2025},
	doi = {10.1017/9781009471664},
	file = {Snapshot:/Users/goessmann/Zotero/storage/8SIUPY4S/640814D308696CD61CB9112EA57B2911.html:text/html},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to the {Theory} of {Differential} {Inclusions}},
	url = {https://bookstore.ams.org/GSM/41},
	language = {en},
	urldate = {2025-11-10},
	file = {Snapshot:/Users/goessmann/Zotero/storage/PPADMMQ4/41.html:text/html},
}

@misc{shi_parallel_2021,
	title = {Parallel algorithms for computing the tensor-train decomposition},
	url = {http://arxiv.org/abs/2111.10448},
	doi = {10.48550/arXiv.2111.10448},
	abstract = {The tensor-train (TT) decomposition expresses a tensor in a data-sparse format used in molecular simulations, high-order correlation functions, and optimization. In this paper, we propose four parallelizable algorithms that compute the TT format from various tensor inputs: (1) Parallel-TTSVD for traditional format, (2) PSTT and its variants for streaming data, (3) Tucker2TT for Tucker format, and (4) TT-fADI for solutions of Sylvester tensor equations. We provide theoretical guarantees of accuracy, parallelization methods, scaling analysis, and numerical results. For example, for a \$d\$-dimension tensor in \${\textbackslash}mathbb\{R\}{\textasciicircum}\{n{\textbackslash}times{\textbackslash}dots{\textbackslash}times n\}\$, a two-sided sketching algorithm PSTT2 is shown to have a memory complexity of \${\textbackslash}mathcal\{O\}(n{\textasciicircum}\{{\textbackslash}lfloor d/2 {\textbackslash}rfloor\})\$, improving upon \${\textbackslash}mathcal\{O\}(n{\textasciicircum}\{d-1\})\$ from previous algorithms.},
	urldate = {2025-11-10},
	publisher = {arXiv},
	author = {Shi, Tianyi and Ruth, Maximilian and Townsend, Alex},
	month = nov,
	year = {2021},
	note = {arXiv:2111.10448 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
	annote = {Comment: 23 pages, 8 figures},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/M6VX6TLD/Shi et al. - 2021 - Parallel algorithms for computing the tensor-train decomposition.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/G24EISBW/2111.html:text/html},
}

@article{koch_dynamical_2007,
	title = {Dynamical {Low}‐{Rank} {Approximation}},
	volume = {29},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/10.1137/050639703},
	doi = {10.1137/050639703},
	abstract = {For the approximation of time-dependent data tensors and of solutions to tensor differential equations by tensors of low Tucker rank, we study a computational approach that can be viewed as a continuous-time updating procedure. This approach works with the increments rather than the full tensor and avoids the computation of decompositions of large matrices. In this method, the derivative is projected onto the tangent space of the manifold of tensors of Tucker rank \$(r\_1,{\textbackslash}dots,r\_N)\$ at the current approximation. This yields nonlinear differential equations for the factors in a Tucker decomposition, suitable for numerical integration. Approximation properties of this approach are analyzed.},
	number = {2},
	urldate = {2025-11-10},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Koch, Othmar and Lubich, Christian},
	month = jan,
	year = {2007},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {434--454},
}

@article{kieri_projection_2018,
	title = {Projection {Methods} for {Dynamical} {Low}-{Rank} {Approximation} of {High}-{Dimensional} {Problems}},
	volume = {19},
	doi = {10.1515/cmam-2018-0029},
	journal = {Comput. Meth. Appl. Math.},
	author = {Kieri, E and Vandereycken, B},
	year = {2018},
	file = {text/html Attachment:/Users/goessmann/Zotero/storage/VH9IB937/bibtexbrowser.html:text/html},
}

@article{haegeman_time-dependent_2011,
	title = {Time-dependent variational principle for quantum lattices},
	volume = {107},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1103.0936},
	doi = {10.1103/PhysRevLett.107.070601},
	abstract = {We develop a new algorithm based on the time-dependent variational principle applied to matrix product states to efficiently simulate the real- and imaginary time dynamics for infinite one-dimensional quantum lattice systems. This procedure: (1) is argued to be optimal; (2) does not rely on the Trotter decomposition and thus has no Trotter error; (3) explicitly preserves all symmetries and conservation laws; and (4) has low computational complexity. The algorithm is illustrated using both imaginary time and real-time examples.},
	number = {7},
	urldate = {2025-11-10},
	journal = {Physical Review Letters},
	author = {Haegeman, Jutho and Cirac, J. Ignacio and Osborne, Tobias J. and Pizorn, Iztok and Verschelde, Henri and Verstraete, Frank},
	month = aug,
	year = {2011},
	note = {arXiv:1103.0936 [cond-mat]},
	keywords = {Quantum Physics, Condensed Matter - Strongly Correlated Electrons, Condensed Matter - Statistical Mechanics},
	pages = {070601},
	annote = {Comment: main text (4+ pages, 2 figures, 1 table) + supplementary material (15 pages, 2 figures). Corrections and other small changes, added reference},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/5E9QQ9DQ/Haegeman et al. - 2011 - Time-dependent variational principle for quantum lattices.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/3LX99ZTJ/1103.html:text/html},
}

@article{paeckel_time-evolution_2019,
	title = {Time-evolution methods for matrix-product states},
	volume = {411},
	issn = {00034916},
	url = {http://arxiv.org/abs/1901.05824},
	doi = {10.1016/j.aop.2019.167998},
	abstract = {Matrix-product states have become the de facto standard for the representation of one-dimensional quantum many body states. During the last few years, numerous new methods have been introduced to evaluate the time evolution of a matrix-product state. Here, we will review and summarize the recent work on this topic as applied to finite quantum systems. We will explain and compare the different methods available to construct a time-evolved matrix-product state, namely the time-evolving block decimation, the MPO \$W{\textasciicircum}{\textbackslash}mathrm\{II\}\$ method, the global Krylov method, the local Krylov method and the one- and two-site time-dependent variational principle. We will also apply these methods to four different representative examples of current problem settings in condensed matter physics.},
	urldate = {2025-11-10},
	journal = {Annals of Physics},
	author = {Paeckel, Sebastian and Köhler, Thomas and Swoboda, Andreas and Manmana, Salvatore R. and Schollwöck, Ulrich and Hubig, Claudius},
	month = dec,
	year = {2019},
	note = {arXiv:1901.05824 [cond-mat]},
	keywords = {Quantum Physics, Condensed Matter - Strongly Correlated Electrons, Condensed Matter - Statistical Mechanics},
	pages = {167998},
	annote = {Comment: Content identical to final journal version, plus a table of contents and minus some formatting errors},
	annote = {Original TDVP paper (referenced by Max)

},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/72W3LEKC/Paeckel et al. - 2019 - Time-evolution methods for matrix-product states.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/YT5EJEEM/1901.html:text/html},
}

@misc{domingos_tensor_2025,
	title = {Tensor {Logic}: {The} {Language} of {AI}},
	shorttitle = {Tensor {Logic}},
	url = {http://arxiv.org/abs/2510.12269},
	doi = {10.48550/arXiv.2510.12269},
	abstract = {Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP and Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Domingos, Pedro},
	month = oct,
	year = {2025},
	note = {arXiv:2510.12269 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Programming Languages},
	annote = {Comment: 17 pages, 0 figures},
	file = {Snapshot:/Users/goessmann/Zotero/storage/559SDQ2K/2510.html:text/html},
}

@book{lehmann_theory_1998,
	address = {New York, NY Berlin Heidelberg},
	title = {Theory of {Point} {Estimation}},
	isbn = {978-0-387-98502-2},
	abstract = {Since the publication in 1983 of Theory of Point Estimation, much new work has made it desirable to bring out a second edition. The inclusion of the new material has increased the length of the book from 500 to 600 pages; of the approximately 1000 references about 25\% have appeared since 1983. The greatest change has been the addition to the sparse treatment of Bayesian inference in the first edition. This includes the addition of new sections on Equivariant, Hierarchical, and Empirical Bayes, and on their comparisons. Other major additions deal with new developments concerning the information in equality and simultaneous and shrinkage estimation. The Notes at the end of each chapter now provide not only bibliographic and historical material but also introductions to recent development in point estimation and other related topics which, for space reasons, it was not possible to include in the main text. The problem sections also have been greatly expanded. On the other hand, to save space most of the discussion in the first edition on robust estimation (in particu lar L, M, and R estimators) has been deleted. This topic is the subject of two excellent books by Hampel et al (1986) and Staudte and Sheather (1990). Other than subject matter changes, there have been some minor modifications in the presentation.},
	language = {Englisch},
	publisher = {Springer},
	author = {Lehmann, Erich L. and Casella, George},
	month = aug,
	year = {1998},
}

@article{gayen_generalized_2023,
	title = {Generalized {Fisher}-{Darmois}-{Koopman}-{Pitman} {Theorem} and {Rao}-{Blackwell} {Type} {Estimators} for {Power}-{Law} {Distributions}},
	volume = {69},
	issn = {0018-9448},
	url = {https://doi.org/10.1109/TIT.2023.3318184},
	doi = {10.1109/TIT.2023.3318184},
	abstract = {This paper generalizes the notion of sufficiency for estimation problems beyond maximum likelihood. In particular, we consider estimation problems based on Jones et al. and Basu et al. likelihood functions that are popular among distance-based robust inference methods. We first characterize the probability distributions that always have a fixed number of sufficient statistics (independent of sample size) with respect to these likelihood functions. These distributions are power-law extensions of the usual exponential family and contain Student distributions as a special case. We then extend the notion of minimal sufficient statistics and compute it for these power-law families. Finally, we establish a Rao-Blackwell-type theorem for finding the best estimators for a power-law family. This helps us establish Cram\&amp;\#x00E9;r-Rao-type lower bounds for power-law families.},
	number = {12},
	urldate = {2025-11-16},
	journal = {IEEE Trans. Inf. Theor.},
	author = {Gayen, Atin and Kumar, M. Ashok},
	month = dec,
	year = {2023},
	pages = {7565--7583},
	file = {Eingereichte Version:/Users/goessmann/Zotero/storage/3FND4GY5/Gayen und Kumar - 2023 - Generalized Fisher-Darmois-Koopman-Pitman Theorem and Rao-Blackwell Type Estimators for Power-Law Di.pdf:application/pdf},
}

@book{casella_statistical_2001,
	address = {Pacific Grove, Calif},
	title = {Statistical {Inference}},
	isbn = {978-0-534-24312-8},
	abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. This book can be used for readers who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.},
	language = {Englisch},
	publisher = {Cengage Learning},
	author = {Casella, George and Berger, Roger},
	month = jun,
	year = {2001},
}

@misc{chang_primer_2025,
	title = {A {Primer} on {Quantum} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2511.15969},
	doi = {10.48550/arXiv.2511.15969},
	abstract = {Quantum machine learning (QML) is a computational paradigm that seeks to apply quantum-mechanical resources to solve learning problems. As such, the goal of this framework is to leverage quantum processors to tackle optimization, supervised, unsupervised and reinforcement learning, and generative modeling-among other tasks-more efficiently than classical models. Here we offer a high level overview of QML, focusing on settings where the quantum device is the primary learning or data generating unit. We outline the field's tensions between practicality and guarantees, access models and speedups, and classical baselines and claimed quantum advantages-flagging where evidence is strong, where it is conditional or still lacking, and where open questions remain. By shedding light on these nuances and debates, we aim to provide a friendly map of the QML landscape so that the reader can judge when-and under what assumptions-quantum approaches may offer real benefits.},
	urldate = {2025-11-21},
	publisher = {arXiv},
	author = {Chang, Su Yeon and Cerezo, M.},
	month = nov,
	year = {2025},
	note = {arXiv:2511.15969 [quant-ph]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Quantum Physics},
	annote = {Comment: 29+16 pages, 5 figures, 15 boxes. Chapter for Comprehensive Quantum Physics. Comments welcomed!},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/MSCGYJPB/Chang und Cerezo - 2025 - A Primer on Quantum Machine Learning.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/HTRCRTZW/2511.html:text/html},
}

@article{fawzi_discovering_2022,
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	volume = {610},
	copyright = {2022 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05172-4},
	doi = {10.1038/s41586-022-05172-4},
	abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
	language = {en},
	number = {7930},
	urldate = {2025-11-24},
	journal = {Nature},
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computer science},
	pages = {47--53},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/DIFAK53E/Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithms with reinforcement learning.pdf:application/pdf},
}

@book{ziegler_convex_2003,
	address = {New York Berlin Heidelberg},
	title = {Convex {Polytopes}},
	isbn = {978-0-387-00424-2},
	abstract = {"The appearance of Grünbaum's book Convex Polytopes in 1967 was a moment of grace to geometers and combinatorialists. The special spirit of the book is very much alive even in those chapters where the book's immense influence made them quickly obsolete. Some other chapters promise beautiful unexplored land for future research. The appearance of the new edition is going to be another moment of grace. Kaibel, Klee and Ziegler were able to update the convex polytope saga in a clear, accurate, lively, and inspired way." (Gil Kalai, The Hebrew University of Jerusalem)"The original book of Grünbaum has provided the central reference for work in this active area of mathematics for the past 35 years...I first consulted this book as a graduate student in 1967; yet, even today, I am surprised again and again by what I find there. It is an amazingly complete reference for work on this subject up to that time and continues to be a major influence on research to this day." (Louis J. Billera, Cornell University)"The original edition of Convex Polytopes inspired a whole generation of grateful workers in polytope theory. Without it, it is doubtful whether many of the subsequent advances in the subject would have been made. The many seeds it sowed have since grown into healthy trees, with vigorous branches and luxuriant foliage. It is good to see it in print once again." (Peter McMullen, University College London)},
	language = {Englisch},
	publisher = {Springer},
	author = {Ziegler, Günter M. and Grünbaum, Branko},
	month = may,
	year = {2003},
}

@article{jaynes_information_1957,
	title = {Information {Theory} and {Statistical} {Mechanics}},
	volume = {106},
	url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	doi = {10.1103/PhysRev.106.620},
	abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
	number = {4},
	urldate = {2025-11-26},
	journal = {Physical Review},
	author = {Jaynes, E. T.},
	month = may,
	year = {1957},
	note = {Publisher: American Physical Society},
	pages = {620--630},
	file = {APS Snapshot:/Users/goessmann/Zotero/storage/3PZFGBT9/PhysRev.106.html:text/html},
}

@phdthesis{tang_tensor_2025,
	address = {Stanford University},
	title = {Tensor network methods for scientific computing},
	url = {https://purl.stanford.edu/np304kb7547},
	abstract = {Modern scientific computing heavily relies on developing new models with practical importance and scalability, and a key aspect of the advancement of new ideas lies in the extension of existing mod...},
	language = {en},
	urldate = {2025-11-26},
	author = {Tang, Xun},
	year = {2025},
	file = {Snapshot:/Users/goessmann/Zotero/storage/CDJXAZUU/np304kb7547.html:text/html},
}

@article{bach_hinge-loss_2017,
	title = {Hinge-loss {Markov} random fields and probabilistic soft logic},
	volume = {18},
	issn = {1532-4435},
	abstract = {A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hingeloss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Bach, Stephen H. and Broecheler, Matthias and Huang, Bert and Getoor, Lise},
	month = jan,
	year = {2017},
	pages = {3846--3912},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/D6GSW4SK/Bach et al. - 2017 - Hinge-loss Markov random fields and probabilistic soft logic.pdf:application/pdf},
}

@article{fishman_itensor_2022,
	title = {The {ITensor} {Software} {Library} for {Tensor} {Network} {Calculations}},
	issn = {2949-804X},
	url = {https://www.scipost.org/SciPostPhysCodeb.4},
	doi = {10.21468/SciPostPhysCodeb.4},
	abstract = {SciPost Journals Publication Detail SciPost Phys. Codebases 4 (2022) The ITensor Software Library for Tensor Network Calculations},
	language = {en},
	urldate = {2025-11-28},
	journal = {SciPost Physics Codebases},
	author = {Fishman, Matthew and White, Steven and Stoudenmire, Edwin Miles},
	month = aug,
	year = {2022},
	pages = {004},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/E82PDF9H/Fishman et al. - 2022 - The ITensor Software Library for Tensor Network Calculations.pdf:application/pdf},
}

@incollection{hagemann_sampling_2025,
	address = {Cham},
	title = {Sampling from {Boltzmann} {Densities} with {Physics} {Informed} {Low}-{Rank} {Formats}},
	isbn = {978-3-031-92366-1},
	abstract = {Our method enables the efficient generation of samples from an unnormalized Boltzmann density by solving the underlying continuity equation in the low-rank tensor train (TT) format. It is based on the annealing path commonly used in MCMC literature, which is given by the linear interpolation in the space of energies. Inspired by Sequential Monte Carlo, we alternate between deterministic time steps from the TT representation of the flow field and stochastic steps, which include Langevin and resampling steps. These adjust the relative weights of the different modes of the target distribution and anneal to the correct path distribution. We showcase the efficiency of our method on multiple numerical examples.},
	language = {en},
	booktitle = {Scale {Space} and {Variational} {Methods} in {Computer} {Vision}},
	publisher = {Springer Nature Switzerland},
	author = {Hagemann, Paul and Schütte, Janina and Sommer, David and Eigel, Martin and Steidl, Gabriele},
	editor = {Bubba, Tatiana A. and Gaburro, Romina and Gazzola, Silvia and Papafitsoros, Kostas and Pereyra, Marcelo and Schönlieb, Carola-Bibiane},
	year = {2025},
	pages = {374--386},
}

@article{pitman_sufficient_1936,
	title = {Sufficient statistics and intrinsic accuracy},
	volume = {32},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/abs/sufficient-statistics-and-intrinsic-accuracy/6A3E45FB1C423F3F684308F8910D6919},
	doi = {10.1017/S0305004100019307},
	abstract = {It is proved that if there exists a sufficient statistic for the estimation of an unknown parameter of a population, the frequency function of the population must be of a certain type.It is shown that some modification of previous theory of the intrinsic accuracy of statistics is necessary when the range of the population sampled is a function of the parameter to be estimated.Finally, the theory is extended to sufficient sets of statistics, i.e. sets of statistics which together contain all the information provided by a sample about an unknown parameter.},
	language = {en},
	number = {4},
	urldate = {2025-11-28},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Pitman, E. J. G.},
	month = dec,
	year = {1936},
	pages = {567--579},
}

@article{hollmann_accurate_2025,
	title = {Accurate predictions on small data with a tabular foundation model},
	volume = {637},
	copyright = {2025 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-08328-6},
	doi = {10.1038/s41586-024-08328-6},
	abstract = {Tabular data, spreadsheets organized in rows and columns, are ubiquitous across scientific fields, from biomedicine to particle physics to economics and climate science1,2. The fundamental prediction task of filling in missing values of a label column based on the rest of the columns is essential for various applications as diverse as biomedical risk models, drug discovery and materials science. Although deep learning has revolutionized learning from raw data and led to numerous high-profile success stories3–5, gradient-boosted decision trees6–9 have dominated tabular data for the past 20 years. Here we present the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model that outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time. In 2.8 s, TabPFN outperforms an ensemble of the strongest baselines tuned for 4 h in a classification setting. As a generative transformer-based foundation model, this model also allows fine-tuning, data generation, density estimation and learning reusable embeddings. TabPFN is a learning algorithm that is itself learned across millions of synthetic datasets, demonstrating the power of this approach for algorithm development. By improving modelling abilities across diverse fields, TabPFN has the potential to accelerate scientific discovery and enhance important decision-making in various domains.},
	language = {en},
	number = {8045},
	urldate = {2025-12-03},
	journal = {Nature},
	author = {Hollmann, Noah and Müller, Samuel and Purucker, Lennart and Krishnakumar, Arjun and Körfer, Max and Hoo, Shi Bin and Schirrmeister, Robin Tibor and Hutter, Frank},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Scientific data, Software, Statistics},
	pages = {319--326},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/L74NRH9Q/Hollmann et al. - 2025 - Accurate predictions on small data with a tabular foundation model.pdf:application/pdf},
}

@book{mezard_information_2009,
	address = {Oxford ; New York},
	title = {Information, {Physics}, and {Computation}},
	isbn = {978-0-19-857083-7},
	abstract = {This book presents a unified approach to a rich and rapidly evolving research domain at the interface between statistical physics, theoretical computer science/discrete mathematics, and coding/information theory. It is accessible to graduate students and researchers without a specific training in any of these fields. The selected topics include spin glasses, error correcting codes, satisfiability, and are central to each field. The approach focuses on large random instances and adopts a common probabilistic formulation in terms of graphical models. It presents message passing algorithms like belief propagation and survey propagation, and their use in decoding and constraint satisfaction solving. It also explains analysis techniques like density evolution and the cavity method, and uses them to study phase transitions.},
	language = {Englisch},
	publisher = {ACADEMIC},
	author = {Mézard, Marc},
	month = mar,
	year = {2009},
}

@misc{recio-armengol_train_2025,
	title = {Train on classical, deploy on quantum: scaling generative quantum machine learning to a thousand qubits},
	shorttitle = {Train on classical, deploy on quantum},
	url = {http://arxiv.org/abs/2503.02934},
	doi = {10.48550/arXiv.2503.02934},
	abstract = {We propose an approach to generative quantum machine learning that overcomes the fundamental scaling issues of variational quantum circuits. The core idea is to use a class of generative models based on instantaneous quantum polynomial circuits, which we show can be trained efficiently on classical hardware. Although training is classically efficient, sampling from these circuits is widely believed to be classically hard, and so computational advantages are possible when sampling from the trained model on quantum hardware. By combining our approach with a data-dependent parameter initialisation strategy, we do not encounter issues of barren plateaus and successfully circumvent the poor scaling of gradient estimation that plagues traditional approaches to quantum circuit optimisation. We investigate and evaluate our approach on a number of real and synthetic datasets, training models with up to one thousand qubits and hundreds of thousands of parameters. We find that the quantum models can successfully learn from high dimensional data, and perform surprisingly well compared to simple energy-based classical generative models trained with a similar amount of hyperparameter optimisation. Overall, our work demonstrates that a path to scalable quantum generative machine learning exists and can be investigated today at large scales.},
	urldate = {2025-12-04},
	publisher = {arXiv},
	author = {Recio-Armengol, Erik and Ahmed, Shahnawaz and Bowles, Joseph},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02934 [quant-ph]},
	keywords = {Quantum Physics},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/XPU5AH5B/Recio-Armengol et al. - 2025 - Train on classical, deploy on quantum scaling generative quantum machine learning to a thousand qub.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/B2NZ6Q82/2503.html:text/html},
}

@book{lauritzen_graphical_1996,
	address = {Oxford},
	title = {Graphical {Models}},
	isbn = {978-0-19-852219-5},
	abstract = {The application of graph theory to modelling systems began in several scientific areas, among them statistical physics (the study of large particle systems), genetics (studying inheritable properties of natural species), and interactions in contingency tables. The use of graphical models in statistics has increased considerably in these and other areas such as artificial intelligence, and the theory has been greatly developed and extended. This is the first comprehensive and authoritative account of the theory of graphical models. Written by a leading expert in the field, it contains the fundamentals graph required and a thorough study of Markov properties associated with various type of graphs, the statistical theory of log-linear and graphical models, and graphical tables with mixed discrete-continuous variables in developed detail. Special topics, such as the application of graphical models to probabilistic expert systems, are described briefly, and appendices give details of the multivariate normal distribution and of the theory of regular exponential families.},
	language = {Englisch},
	publisher = {Clarendon Press},
	author = {Lauritzen, Steffen L.},
	month = jul,
	year = {1996},
	annote = {Shows that triangulated is equal to junction tree existence


},
}

@article{lauritzen_local_1988,
	title = {Local {Computations} with {Probabilities} on {Graphical} {Structures} and {Their} {Application} to {Expert} {Systems}},
	volume = {50},
	issn = {0035-9246},
	url = {https://doi.org/10.1111/j.2517-6161.1988.tb01721.x},
	doi = {10.1111/j.2517-6161.1988.tb01721.x},
	abstract = {A causal network is used in a number of areas as a depiction of patterns of ‘influence’ among sets of variables. In expert systems it is common to perform ‘inference’ by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed ‘marrying’ and ‘filling-in‘. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
	number = {2},
	urldate = {2025-12-05},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Lauritzen, S. L. and Spiegelhalter, D. J.},
	month = jan,
	year = {1988},
	pages = {157--194},
	annote = {Reference for the Junction Tree Algorithm (by Wainwright)

},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/RE6SRXWA/Lauritzen und Spiegelhalter - 1988 - Local Computations with Probabilities on Graphical Structures and Their Application to Expert System.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/FFXFVQ6B/j.2517-6161.1988.tb01721.html:text/html},
}

@article{mulzer_minimum-weight_2008,
	title = {Minimum-weight triangulation is {NP}-hard},
	volume = {55},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1346330.1346336},
	doi = {10.1145/1346330.1346336},
	abstract = {A triangulation of a planar point set S is a maximal plane straight-line graph with vertex set S. In the minimum-weight triangulation (MWT) problem, we are looking for a triangulation of a given point set that minimizes the sum of the edge lengths. We prove that the decision version of this problem is NP-hard, using a reduction from PLANAR 1-IN-3-SAT. The correct working of the gadgets is established with computer assistance, using dynamic programming on polygonal faces, as well as the β-skeleton heuristic to certify that certain edges belong to the minimum-weight triangulation.},
	number = {2},
	urldate = {2025-12-05},
	journal = {J. ACM},
	author = {Mulzer, Wolfgang and Rote, Günter},
	month = may,
	year = {2008},
	pages = {11:1--11:29},
	file = {Eingereichte Version:/Users/goessmann/Zotero/storage/W45URH8I/Mulzer und Rote - 2008 - Minimum-weight triangulation is NP-hard.pdf:application/pdf},
}

@book{brown_fundamentals_1987,
	address = {Hayward, Calif},
	title = {Fundamentals of {Statistical} {Exponential} {Families}},
	isbn = {978-0-940600-10-2},
	language = {Englisch},
	publisher = {Institute of mathematical Statistics},
	author = {Brown, Lawrence D.},
	month = jun,
	year = {1987},
	annote = {Reference for the forward map one-to-one on the interior mean polytope

},
}

@book{grotschel_geometric_1993,
	address = {Berlin, Heidelberg},
	series = {Algorithms and {Combinatorics}},
	title = {Geometric {Algorithms} and {Combinatorial} {Optimization}},
	volume = {2},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-78242-8 978-3-642-78240-4},
	url = {http://link.springer.com/10.1007/978-3-642-78240-4},
	urldate = {2025-12-05},
	publisher = {Springer},
	author = {Grötschel, Martin and Lovász, László and Schrijver, Alexander},
	year = {1993},
	doi = {10.1007/978-3-642-78240-4},
	keywords = {algorithms, Basis Reduction in Lattices, Basisreduktion bei Gittern, combinatorial optimization, combinatorics, Convexity, Ellipsoid Method, Ellipsoidmethode, Kombinatorische Optimierung, Konvexität, Lattice, Linear Programming, Lineares Programmieren, operations resear},
	annote = {By Ziegler-0/1 referenced for facet complexity
},
}

@inproceedings{yoon_maximum_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '24},
	title = {Maximum entropy inverse reinforcement learning of diffusion models with energy-based models},
	volume = {37},
	isbn = {979-8-3313-1438-5},
	abstract = {We present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps. Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance.},
	urldate = {2025-12-05},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Yoon, Sangwoong and Hwang, Himchan and Kwon, Dohyun and Noh, Yung-Kyun and Park, Frank C.},
	month = dec,
	year = {2024},
	pages = {24601--24624},
}

@misc{miotto_new_2025,
	title = {A new pathway to generative artificial intelligence by minimizing the maximum entropy},
	url = {http://arxiv.org/abs/2502.13287},
	doi = {10.48550/arXiv.2502.13287},
	abstract = {Generative artificial intelligence revolutionized society. Current models are trained by minimizing the distance between the produced data and the training set. Consequently, development is plateauing as they are intrinsically data-hungry and challenging to direct during the generative process. To overcome these limitations, we introduce a paradigm shift through a framework where we do not fit the training set but find the most informative yet least noisy representation of the data simultaneously minimizing the entropy to reduce noise and maximizing it to remain unbiased via adversary training. The result is a general physics-driven model, which is data-efficient and flexible, permitting to control and influence the generative process. Benchmarking shows that our approach outperforms variational autoencoders. We demonstrate the methods effectiveness in generating images, even with limited training data, and its unprecedented capability to customize the generation process a posteriori without any fine-tuning or retraining},
	urldate = {2025-12-05},
	publisher = {arXiv},
	author = {Miotto, Mattia and Monacelli, Lorenzo},
	month = jun,
	year = {2025},
	note = {arXiv:2502.13287 [cs]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Computer Science - Information Theory},
	annote = {Comment: 10 pages, 7 figures},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/U7FCZGHI/Miotto und Monacelli - 2025 - A new pathway to generative artificial intelligence by minimizing the maximum entropy.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/Z69EVVUK/2502.html:text/html},
}

@article{rathnakumar_bayesian_2026,
	title = {Bayesian {Entropy} {Neural} {Networks} for physics-aware prediction},
	volume = {266},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832025007756},
	doi = {10.1016/j.ress.2025.111575},
	abstract = {This paper addresses the need for deep learning models to integrate well-defined constraints into their outputs, driven by their application in surrogate models, learning with limited data and partial information, and scenarios requiring flexible model behavior to incorporate non-data sample information. We introduce Bayesian Entropy Neural Networks (BENN), a framework grounded in Maximum Entropy (MaxEnt) principles, designed to impose constraints on Bayesian Neural Network (BNN) predictions. BENN is capable of constraining not only the predicted values but also their derivatives and variances, ensuring a more robust and reliable model output. To achieve simultaneous uncertainty quantification and constraint satisfaction, we employ the method of multipliers approach. This allows for the concurrent estimation of neural network parameters and the Lagrangian multipliers associated with the constraints. Our experiments, spanning diverse applications such as beam deflection modeling and microstructure generation, demonstrate the effectiveness of BENN. The results highlight significant improvements over traditional BNNs and showcase competitive performance relative to contemporary constrained deep learning methods.},
	urldate = {2025-12-05},
	journal = {Reliability Engineering \& System Safety},
	author = {Rathnakumar, Rahul and Lu, Xuandong and Huang, Jiayu and Yan, Hao and Liu, Yongming},
	month = feb,
	year = {2026},
	keywords = {Deep learning, Bayesian Neural Networks, Constrained learning, Expert knowledge, Uncertainty quantification},
	pages = {111575},
	file = {ScienceDirect Snapshot:/Users/goessmann/Zotero/storage/YAL32Z5M/S0951832025007756.html:text/html},
}

@article{mus_new-generation_2024,
	title = {New-generation maximum entropy method: a {Lagrangian}-based algorithm for dynamic reconstruction of interferometric data},
	volume = {528},
	issn = {0035-8711},
	shorttitle = {New-generation maximum entropy method},
	url = {https://doi.org/10.1093/mnras/stae234},
	doi = {10.1093/mnras/stae234},
	abstract = {Imaging interferometric data in radio astronomy requires the use of non-linear algorithms that rely on different assumptions on the source structure and may produce non-unique results. This is especially true for very long baseline interferometry (VLBI) observations, where the sampling of Fourier space is very sparse. A basic tenet in standard VLBI imaging techniques is to assume that the observed source structure does not evolve during the observation. However, the recent VLBI results of the supermassive black hole at our Galactic Centre (Sagittarius A*), recently reported by the Event Horizon Telescope Collaboration, require the development of dynamic imaging algorithms, since it exhibits variability at minute time-scales. In this paper, we introduce a new non-convex optimization problem that extends the standard maximum entropy method (MEM), for reconstructing intra-observation dynamical images from interferometric data that evolve in every integration time. We present a rigorous mathematical formalism to solve the problem via the primal–dual approach. We build a Newton strategy and we give its numerical complexity. We also give a strategy to iteratively improve the obtained solution and, finally, we define a novel figure of merit to evaluate the quality of the recovered solution. Then, we test the algorithm, called the new-generation MEM (ngMEM), in different synthetic data sets, with increasing difficulty. Finally, we compare it with another well-established dynamical imaging method. Within this comparison, we have identified a significant improvement of the ngMEM reconstructions. Moreover, the evaluation of the integration time evolution scheme and the time contribution showed that this method can play a crucial role in obtaining good dynamic reconstructions.},
	number = {4},
	urldate = {2025-12-05},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Mus, Alejandro and Martí-Vidal, Ivan},
	month = mar,
	year = {2024},
	pages = {5537--5557},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/EHV5VMUN/Mus und Martí-Vidal - 2024 - New-generation maximum entropy method a Lagrangian-based algorithm for dynamic reconstruction of in.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/X4RFVVQ9/stae234.html:text/html},
}

@article{song_survey_2025,
	title = {A {Survey} of {Maximum} {Entropy}-{Based} {Inverse} {Reinforcement} {Learning}: {Methods} and {Applications}},
	volume = {17},
	issn = {2073-8994},
	shorttitle = {A {Survey} of {Maximum} {Entropy}-{Based} {Inverse} {Reinforcement} {Learning}},
	url = {https://www.mdpi.com/2073-8994/17/10/1632},
	doi = {10.3390/sym17101632},
	abstract = {In recent years, inverse reinforcement learning algorithms have garnered substantial attention and demonstrated remarkable success across various control domains, including autonomous driving, intelligent gaming, robotic manipulation, and automated industrial systems. Nevertheless, existing methodologies face two persistent challenges: (1) finite or non-optimal expert demonstration and (2) ambiguity in which different reward functions lead to same expert strategies. To improve and enhance the expert demonstration data and to eliminate the ambiguity caused by the symmetry of rewards, there has been a growing interest in research on developing inverse reinforcement learning based on the maximum entropy method. The unique advantage of these algorithms lies in learning rewards from expert presentations by maximizing policy entropy, matching expert expectations, and then optimizing the policy. This paper first provides a comprehensive review of the historical development of maximum entropy-based inverse reinforcement learning (ME-IRL) methodologies. Subsequently, it systematically presents the benchmark experiments and recent application breakthroughs achieved through ME-IRL. The concluding section analyzes the persistent technical challenges, proposes promising solutions, and outlines the emerging research frontiers in this rapidly evolving field.},
	language = {en},
	number = {10},
	urldate = {2025-12-05},
	journal = {Symmetry},
	author = {Song, Li and Guo, Qinghui and Channa, Irfan Ali and Wang, Zeyu},
	month = oct,
	year = {2025},
	pages = {1632},
	annote = {[TLDR] A comprehensive review of the historical development of maximum entropy-based inverse reinforcement learning (ME-IRL) methodologies and systematically presents the benchmark experiments and recent application breakthroughs achieved through ME-IRL.},
}

@book{gallager_low-density_1963,
	title = {Low-{Density} {Parity}-{Check} {Codes}},
	isbn = {978-0-262-25621-6},
	url = {https://direct.mit.edu/books/monograph/3867/Low-Density-Parity-Check-Codes},
	abstract = {This is a complete presentation of all important theoretical and experimental work done on low-density codes. Low-density coding is one of the three techni},
	language = {en},
	urldate = {2025-12-05},
	publisher = {The MIT Press},
	author = {Gallager, Robert G.},
	month = sep,
	year = {1963},
	doi = {10.7551/mitpress/4347.001.0001},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/RIYQYDP8/Gallager - 1963 - Low-Density Parity-Check Codes.pdf:application/pdf},
}

@article{oseledets_tensor-train_2011,
	title = {Tensor-{Train} {Decomposition}},
	volume = {33},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/090752286},
	doi = {10.1137/090752286},
	abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.
MSC codes

15A23
15A69
65F99
Keywords

tensors
high-dimensional problems
SVD
TT-format},
	number = {5},
	urldate = {2025-12-08},
	journal = {SIAM Journal on Scientific Computing},
	author = {Oseledets, I. V.},
	month = jan,
	year = {2011},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {2295--2317},
}

@article{white_density-matrix_1993,
	title = {Density-matrix algorithms for quantum renormalization groups},
	volume = {48},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.48.10345},
	doi = {10.1103/PhysRevB.48.10345},
	abstract = {A formulation of numerical real-space renormalization groups for quantum many-body problems is presented and several algorithms utilizing this formulation are outlined. The methods are presented and demonstrated using S=1/2 and S=1 Heisenberg chains as test cases. The key idea of the formulation is that rather than keep the lowest-lying eigenstates of the Hamiltonian in forming a new effective Hamiltonian of a block of sites, one should keep the most significant eigenstates of the block density matrix, obtained from diagonalizing the Hamiltonian of a larger section of the lattice which includes the block. This approach is much more accurate than the standard approach; for example, energies for the S=1 Heisenberg chain can be obtained to an accuracy of at least 10−9. The method can be applied to almost any one-dimensional quantum lattice system, and can provide a wide variety of static properties.},
	number = {14},
	urldate = {2025-12-08},
	journal = {Physical Review B},
	author = {White, Steven R.},
	month = oct,
	year = {1993},
	note = {Publisher: American Physical Society},
	pages = {10345--10356},
	file = {APS Snapshot:/Users/goessmann/Zotero/storage/82WCDF27/PhysRevB.48.html:text/html},
}

@article{berezutskii_simulating_2025,
	title = {Simulating quantum circuits using the multi-scale entanglement renormalization ansatz},
	volume = {7},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.7.013063},
	doi = {10.1103/PhysRevResearch.7.013063},
	abstract = {Understanding the limiting capabilities of classical methods in simulating complex quantum systems is of paramount importance for quantum technologies. Although many advanced approaches have been proposed and recently used to challenge quantum advantage experiments, novel efficient methods for the approximate simulation of complex quantum systems are still in high demand. Here, we propose a scalable technique for approximate simulations of intermediate-size quantum circuits on the basis of the multi-scale entanglement renormalization ansatz (MERA) and Riemannian optimization. The MERA is a tensor network, whose geometry together with orthogonality constraints imposed on its tensors allow approximating many-body quantum states lying beyond the area-law scaling of the entanglement entropy. We benchmark the proposed technique for brick-wall quantum circuits of up to 243 qubits with various depths up to 20 layers. Our approach paves a way to exploring efficient simulation techniques for quantum many-body systems.},
	number = {1},
	urldate = {2025-12-08},
	journal = {Physical Review Research},
	author = {Berezutskii, A. V. and Luchnikov, I. A. and Fedorov, A. K.},
	month = jan,
	year = {2025},
	note = {Publisher: American Physical Society},
	pages = {013063},
	file = {APS Snapshot:/Users/goessmann/Zotero/storage/ND57MHGS/PhysRevResearch.7.html:text/html;Full Text PDF:/Users/goessmann/Zotero/storage/6Y58ICFQ/Berezutskii et al. - 2025 - Simulating quantum circuits using the multi-scale entanglement renormalization ansatz.pdf:application/pdf},
}

@article{eigel_adaptive_2017,
	title = {Adaptive stochastic {Galerkin} {FEM} with hierarchical tensor representations},
	volume = {136},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/s00211-016-0850-x},
	doi = {10.1007/s00211-016-0850-x},
	abstract = {The solution of PDE with stochastic data commonly leads to very high-dimensional algebraic problems, e.g. when multiplicative noise is present. The Stochastic Galerkin FEM considered in this paper then suffers from the curse of dimensionality. This is directly related to the number of random variables required for an adequate representation of the random fields included in the PDE. With the presented new approach, we circumvent this major complexity obstacle by combining two highly efficient model reduction strategies, namely a modern low-rank tensor representation in the tensor train format of the problem and a refinement algorithm on the basis of a posteriori error estimates to adaptively adjust the different employed discretizations. The adaptive adjustment includes the refinement of the FE mesh based on a residual estimator, the problem-adapted stochastic discretization in anisotropic Legendre Wiener chaos and the successive increase of the tensor rank. Computable a posteriori error estimators are derived for all error terms emanating from the discretizations and the iterative solution with a preconditioned ALS scheme of the problem. Strikingly, it is possible to exploit the tensor structure of the problem to evaluate all error terms very efficiently. A set of benchmark problems illustrates the performance of the adaptive algorithm with higher-order FE. Moreover, the influence of the tensor rank on the approximation quality is investigated.},
	language = {en},
	number = {3},
	urldate = {2025-12-08},
	journal = {Numerische Mathematik},
	author = {Eigel, Martin and Pfeffer, Max and Schneider, Reinhold},
	month = jul,
	year = {2017},
	keywords = {Tensor train, Tensor representation, Uncertainty quantification, 35R60, 47B80, 60H35, 65C20, 65J10, 65N12, 65N22, Adaptive methods, ALS, Low-rank, Operator equations, Partial differential equations with random coefficients, Reduced basis methods, Stochastic finite element methods},
	pages = {765--803},
}

@article{gourianov_tensor_2025,
	title = {Tensor networks enable the calculation of turbulence probability distributions},
	volume = {11},
	url = {https://www.science.org/doi/10.1126/sciadv.ads5990},
	doi = {10.1126/sciadv.ads5990},
	abstract = {Predicting the dynamics of turbulent fluids has been an elusive goal for centuries. Even with modern computers, anything beyond the simplest turbulent flows is too chaotic and multiscaled to be directly simulatable. An alternative is to treat turbulence probabilistically, viewing flow properties as random variables distributed according to joint probability density functions (PDFs). Such PDFs are neither chaotic nor multiscale, yet remain challenging to simulate due to their high dimensionality. Here, we overcome the dimensionality problem by encoding turbulence PDFs as highly compressed “tensor networks” (TNs). This enables single CPU core simulations that would otherwise be impractical even with supercomputers: for a 5 + 1 dimensional PDF of a chemically reactive turbulent flow, we achieve reductions in memory and computational costs by factors of 
O
(
10
6
)
 and 
O
(
10
3
)
, respectively, compared to standard finite-difference algorithms. A future path is opened toward something heretofore thought infeasible: directly simulating high-dimensional PDFs of both turbulent flows and other chaotic systems that can usefully be described probabilistically.},
	number = {5},
	urldate = {2025-12-08},
	journal = {Science Advances},
	author = {Gourianov, Nikita and Givi, Peyman and Jaksch, Dieter and Pope, Stephen B.},
	month = jan,
	year = {2025},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eads5990},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/NGGUHVVP/Gourianov et al. - 2025 - Tensor networks enable the calculation of turbulence probability distributions.pdf:application/pdf},
}

@article{affleck_rigorous_1987,
	title = {Rigorous results on valence-bond ground states in antiferromagnets},
	volume = {59},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.59.799},
	doi = {10.1103/PhysRevLett.59.799},
	abstract = {We present rigorous results on a phase in antiferromagnets in one dimension and more, which we call a valence-bond solid. The ground state is simply constructed out of valence bonds, is nondegenerate, and breaks no symmetries. There is an energy gap and an exponentially decaying correlation function. Physical applications are mentioned.},
	number = {7},
	urldate = {2025-12-08},
	journal = {Physical Review Letters},
	author = {Affleck, Ian and Kennedy, Tom and Lieb, Elliott H. and Tasaki, Hal},
	month = aug,
	year = {1987},
	note = {Publisher: American Physical Society},
	pages = {799--802},
	file = {APS Snapshot:/Users/goessmann/Zotero/storage/9KM2PQ74/PhysRevLett.59.html:text/html},
}

@article{lipton_mythos_2018,
	title = {The mythos of model interpretability},
	volume = {61},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3233231},
	doi = {10.1145/3233231},
	abstract = {In machine learning, the concept of interpretability is both important and slippery.},
	number = {10},
	urldate = {2025-12-08},
	journal = {Commun. ACM},
	author = {Lipton, Zachary C.},
	month = sep,
	year = {2018},
	pages = {36--43},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/AXARC736/Lipton - 2018 - The mythos of model interpretability.pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	urldate = {2025-12-08},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	keywords = {Accountability, Comprehensibility, Data Fusion, Deep Learning, Explainable Artificial Intelligence, Fairness, Interpretability, Machine Learning, Privacy, Responsible Artificial Intelligence, Transparency},
	pages = {82--115},
	file = {Akzeptierte Version:/Users/goessmann/Zotero/storage/JW5TVD77/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concepts, taxonomies, opportunities and challenges toward.pdf:application/pdf;ScienceDirect Snapshot:/Users/goessmann/Zotero/storage/V68I2GSH/S1566253519308103.html:text/html},
}

@inproceedings{colelough_neuro-symbolic_2024,
	address = {Jeju, South Korea},
	title = {Neuro-{Symbolic} {AI} in 2024: {A} {Systematic} {Review}},
	abstract = {Objective: This paper provides a systematic literature review of Neuro-Symbolic AI projects within the 2020-24 AI landscape, highlighting key developments, methodologies, and applications. It aims to identify where quality efforts are focused in 2024 and pinpoint existing research gaps in the field.
Methods: The review followed the PRISMA methodology, utilizing databases such as IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion criteria targeted peer-reviewed papers published between 2020 and 2024. Papers were screened for relevance to Neuro-Symbolic AI, with further inclusion based on the availability of associated codebases to ensure reproducibility.
Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria and were analyzed in detail. The majority of research efforts are concentrated in the areas of learning and inference (63\%), logic and reasoning (35\%), and knowledge representation (44\%). Explainability and trustworthiness are less represented (28\%), with Meta-Cognition being the least explored area (5\%). The review identifies significant interdisciplinary opportunities, particularly in integrating explainability and trustworthiness with other research areas. Discussion: The findings reveal a well-integrated body of work in learning and inference, logic and reasoning, and knowledge representation. However, there is a notable gap in research focused on explainability and trustworthiness, which is critical for the deployment of reliable AI systems. The sparse representation of Meta-Cognition highlights the need for further research to develop frameworks that enable AI systems to self-monitor, evaluate, and adjust their processes, enhancing autonomy and adaptability.
Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with concentrated efforts in learning and inference. Significant gaps remain in explainability, trustworthiness, and Meta-Cognition. Addressing these gaps through interdisciplinary research will be crucial for advancing the field towards more intelligent, reliable, and context-aware AI systems.},
	language = {en},
	author = {Colelough, Brandon C and Regli, William},
	year = {2024},
	file = {PDF:/Users/goessmann/Zotero/storage/K4G9UTAW/Colelough und Regli - Neuro-Symbolic AI in 2024 A Systematic Review.pdf:application/pdf},
}

@misc{goessmann_tensor-network_2025,
	title = {The {Tensor}-{Network} {Approach} to {Efficient} and {Explainable} {AI}},
	url = {https://github.com/EnexaProject/enexa-tensor-reasoning-documentation/},
	abstract = {Contribute to EnexaProject/enexa-tensor-reasoning-documentation development by creating an account on GitHub.},
	language = {en},
	urldate = {2025-12-08},
	journal = {GitHub},
	author = {Goessmann, Alex},
	year = {2025},
	file = {Snapshot:/Users/goessmann/Zotero/storage/HKQRM2LN/tnreason_report.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2025-12-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/6KIPVHAB/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{sander_large-scale_2025,
	title = {Large-scale stochastic simulation of open quantum systems},
	volume = {16},
	issn = {2041-1723},
	url = {http://arxiv.org/abs/2501.17913},
	doi = {10.1038/s41467-025-66846-x},
	abstract = {Understanding the precise interaction mechanisms between quantum systems and their environment is crucial for advancing stable quantum technologies, designing reliable experimental frameworks, and building accurate models of real-world phenomena. However, simulating open quantum systems, which feature complex non-unitary dynamics, poses significant computational challenges that require innovative methods to overcome. In this work, we introduce the tensor jump method (TJM), a scalable, embarrassingly parallel algorithm for stochastically simulating large-scale open quantum systems, specifically Markovian dynamics captured by Lindbladians. This method is built on three core principles where, in particular, we extend the Monte Carlo wave function (MCWF) method to matrix product states, use a dynamic time-dependent variational principle (TDVP) to significantly reduce errors during time evolution, and introduce what we call a sampling MPS to drastically reduce the dependence on the simulation's time step size. We demonstrate that this method scales more effectively than previous methods and ensures convergence to the Lindbladian solution independent of system size, which we show both rigorously and numerically. Finally, we provide evidence of its utility by simulating Lindbladian dynamics of XXX Heisenberg models up to a thousand spins using a consumer-grade CPU. This work represents a significant step forward in the simulation of large-scale open quantum systems, with the potential to enable discoveries across various domains of quantum physics, particularly those where the environment plays a fundamental role, and to both dequantize and facilitate the development of more stable quantum hardware.},
	number = {1},
	urldate = {2025-12-15},
	journal = {Nature Communications},
	author = {Sander, Aaron and Fröhlich, Maximilian and Eigel, Martin and Eisert, Jens and Gelß, Patrick and Hintermüller, Michael and Milbradt, Richard M. and Wille, Robert and Mendl, Christian B.},
	month = dec,
	year = {2025},
	note = {arXiv:2501.17913 [quant-ph]},
	keywords = {Condensed Matter - Other Condensed Matter, Quantum Physics},
	pages = {11074},
	annote = {Comment: 24 pages, 13 figures, 1 table (includes Methods and Appendix)},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/YUBVCNDP/Sander et al. - 2025 - Large-scale stochastic simulation of open quantum systems.pdf:application/pdf},
}

@misc{sander_quantum_2025,
	title = {Quantum circuit simulation with a local time-dependent variational principle},
	url = {http://arxiv.org/abs/2508.10096},
	doi = {10.48550/arXiv.2508.10096},
	abstract = {Classical simulations of quantum circuits are vital for assessing potential quantum advantage and benchmarking devices, yet they require sophisticated methods to avoid the exponential growth of resources. Tensor network approaches, in particular matrix product states (MPS) combined with the time-evolving block decimation (TEBD) algorithm, currently dominate large-scale circuit simulations. These methods scale efficiently when entanglement is limited but suffer rapid bond dimension growth with increasing entanglement and handle long-range gates via costly SWAP insertions. Motivated by the success of the time-dependent variational principle (TDVP) in many-body physics, we reinterpret quantum circuits as a series of discrete time evolutions, using gate generators to construct an MPS-based circuit simulation via a local TDVP formulation. This addresses TEBD's key limitations by (1) naturally accommodating long-range gates and (2) optimally representing states on the MPS manifold. By diffusing entanglement more globally, the method suppresses local bond growth and reduces memory and runtime costs. We benchmark the approach on five 49-qubit circuits: three Hamiltonian circuits (1D open and periodic Heisenberg, 2D 7x7 Ising) and two algorithmic ones (quantum approximate optimization, hardware-efficient ansatz). Across all cases, our method yields substantial resource reductions over standard tools, establishing a new state-of-the-art for circuit simulation and enabling advances across quantum computing, condensed matter, and beyond.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Sander, Aaron and Fröhlich, Maximilian and Ali, Mazen and Eigel, Martin and Eisert, Jens and Hintermüller, Michael and Mendl, Christian B. and Milbradt, Richard M. and Wille, Robert},
	month = aug,
	year = {2025},
	note = {arXiv:2508.10096 [quant-ph]},
	keywords = {Condensed Matter - Other Condensed Matter, Quantum Physics},
	annote = {Comment: 13 pages, 2 figures},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/KSGKAEQI/Sander et al. - 2025 - Quantum circuit simulation with a local time-dependent variational principle.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/8VSWN4RJ/2508.html:text/html},
}

@misc{ali_prime_2025,
	title = {Prime {Factorization} {Equation} from a {Tensor} {Network} {Perspective}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2508.00907},
	doi = {10.48550/ARXIV.2508.00907},
	abstract = {This paper presents an exact and explicit equation for prime factorization, along with an algorithm for its computation. The proposed method is based on the MeLoCoToN approach, which addresses combinatorial optimization problems through classical tensor networks. The presented tensor network performs the multiplication of every pair of possible input numbers and selects those whose product is the number to be factorized. Additionally, in order to make the algorithm more efficient, the number and dimension of the tensors and their contraction scheme are optimized. Finally, a series of tests on the algorithm are conducted, contracting the tensor network both exactly and approximately using tensor train compression, and evaluating its performance.},
	language = {en},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Ali, Alejandro Mata and Martín, Jorge Martínez and Subiñas, Sergio Muñiz and Hernando, Miguel Franco and Sedano, Javier and García-Vico, Ángel Miguel},
	year = {2025},
	note = {Version Number: 1},
	keywords = {65F99, 81P68, 94A60, Computational Physics (physics.comp-ph), Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Mathematics, FOS: Physical sciences, Optimization and Control (math.OC), Quantum Physics (quant-ph)},
	annote = {Other
Submission to SciPost, 18 pages, 12 figures},
	file = {PDF:/Users/goessmann/Zotero/storage/J7YUKF46/Ali et al. - 2025 - Prime Factorization Equation from a Tensor Network Perspective.pdf:application/pdf},
}

@article{costa_quantum_2016,
	title = {Quantum causal modelling},
	volume = {18},
	issn = {1367-2630},
	url = {https://doi.org/10.1088/1367-2630/18/6/063032},
	doi = {10.1088/1367-2630/18/6/063032},
	abstract = {Causal modelling provides a powerful set of tools for identifying causal structure from observed correlations. It is well known that such techniques fail for quantum systems, unless one introduces ‘spooky’ hidden mechanisms. Whether one can produce a genuinely quantum framework in order to discover causal structure remains an open question. Here we introduce a new framework for quantum causal modelling that allows for the discovery of causal structure. We define quantum analogues for core features of classical causal modelling techniques, including the causal Markov condition and faithfulness. Based on the process matrix formalism, this framework naturally extends to generalised structures with indefinite causal order.},
	language = {en},
	number = {6},
	urldate = {2025-12-29},
	journal = {New Journal of Physics},
	author = {Costa, Fabio and Shrapnel, Sally},
	month = jun,
	year = {2016},
	note = {Publisher: IOP Publishing},
	pages = {063032},
	file = {IOP Full Text PDF:/Users/goessmann/Zotero/storage/RCCKVU8G/Costa und Shrapnel - 2016 - Quantum causal modelling.pdf:application/pdf},
}

@article{ried_quantum_2015,
	title = {A quantum advantage for inferring causal structure},
	volume = {11},
	issn = {1745-2473},
	url = {https://ui.adsabs.harvard.edu/abs/2015NatPh..11..414R/abstract},
	doi = {10.1038/nphys3266},
	abstract = {The problem of inferring causal relations from observed correlations is relevant to a wide variety of scientific disciplines. Yet given the correlations between just two classical variables, it is impossible to determine whether they arose from a causal influence of one on the other or a common cause influencing both. Only a randomized trial can settle the issue. Here we consider the problem of causal inference for quantum variables. We show that the analogue of a randomized trial, causal tomography, yields a complete solution. We also show that, in contrast to the classical case, one can sometimes infer the causal structure from observations alone. We implement a quantum-optical experiment wherein we control the causal relation between two optical modes, and two measurement schemes—with and without randomization—that extract this relation from the observed correlations. Our results show that entanglement and quantum coherence provide an advantage for causal inference.},
	language = {en},
	number = {5},
	urldate = {2025-12-29},
	journal = {Nature Physics},
	author = {Ried, Katja and Agnew, Megan and Vermeyden, Lydia and Janzing, Dominik and Spekkens, Robert W. and Resch, Kevin J.},
	month = may,
	year = {2015},
	pages = {414--420},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/8QNP8WCF/Ried et al. - 2015 - A quantum advantage for inferring causal structure.pdf:application/pdf},
}

@article{brukner_quantum_2014,
	title = {Quantum causality},
	volume = {10},
	copyright = {2014 Springer Nature Limited},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/nphys2930},
	doi = {10.1038/nphys2930},
	abstract = {Traditionally, quantum theory assumes the existence of a fixed background causal structure. But if the laws of quantum mechanics are applied to the causal relations, then one could imagine situations in which the causal order of events is not always fixed, but is subject to quantum uncertainty. Such indefinite causal structures could make new quantum information processing tasks possible and provide methodological tools in quantum theories of gravity. Here, I review recent theoretical progress in this emerging area.},
	language = {en},
	number = {4},
	urldate = {2025-12-29},
	journal = {Nature Physics},
	author = {Brukner, Časlav},
	month = apr,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	keywords = {Quantum information},
	pages = {259--263},
}

@article{ozols_quantum_2013,
	title = {Quantum rejection sampling},
	volume = {5},
	issn = {1942-3454},
	url = {https://doi.org/10.1145/2493252.2493256},
	doi = {10.1145/2493252.2493256},
	abstract = {Rejection sampling is a well-known method to sample from a target distribution, given the ability to sample from a given distribution. The method has been first formalized by von Neumann [1951] and has many applications in classical computing. We define a quantum analogue of rejection sampling: given a black box producing a coherent superposition of (possibly unknown) quantum states with some amplitudes, the problem is to prepare a coherent superposition of the same states, albeit with different target amplitudes. The main result of this article is a tight characterization of the query complexity of this quantum state generation problem. We exhibit an algorithm, which we call quantum rejection sampling, and analyze its cost using semidefinite programming. Our proof of a matching lower bound is based on the automorphism principle that allows to symmetrize any algorithm over the automorphism group of the problem. Our main technical innovation is an extension of the automorphism principle to continuous groups that arise for quantum state generation problems where the oracle encodes unknown quantum states, instead of just classical data. Furthermore, we illustrate how quantum rejection sampling may be used as a primitive in designing quantum algorithms, by providing three different applications. We first show that it was implicitly used in the quantum algorithm for linear systems of equations by Harrow et al. [2009]. Second we show that it can be used to speed up the main step in the quantum Metropolis sampling algorithm by Temme et al. [2011]. Finally, we derive a new quantum algorithm for the hidden shift problem of an arbitrary Boolean function and relate its query complexity to “water-filling” of the Fourier spectrum.},
	number = {3},
	urldate = {2025-12-29},
	journal = {ACM Trans. Comput. Theory},
	author = {Ozols, Maris and Roetteler, Martin and Roland, Jérémie},
	month = aug,
	year = {2013},
	pages = {11:1--11:33},
	file = {Eingereichte Version:/Users/goessmann/Zotero/storage/RAHKVTZA/Ozols et al. - 2013 - Quantum rejection sampling.pdf:application/pdf},
}

@book{schuld_machine_2021,
	address = {Cham},
	title = {Machine {Learning} with {Quantum} {Computers}},
	isbn = {978-3-030-83097-7},
	abstract = {This book offers an introduction into quantum machine learning research, covering approaches that range from "near-term" to fault-tolerant quantum machine learning algorithms, and from theoretical to practical techniques that help us understand how quantum computers can learn from data. Among the topics discussed are parameterized quantum circuits, hybrid optimization, data encoding, quantum feature maps and kernel methods, quantum learning theory, as well as quantum neural networks. The book aims at an audience of computer scientists and physicists at the graduate level onwards. The second edition extends the material beyond supervised learning and puts a special focus on the developments in near-term quantum machine learning seen over the past few years.},
	language = {Englisch},
	publisher = {Springer},
	author = {Schuld, Maria and Petruccione, Francesco},
	month = oct,
	year = {2021},
}

@misc{cuiper_houches_2025,
	title = {Les {Houches} {Lectures} {Notes} on {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2512.24390},
	doi = {10.48550/arXiv.2512.24390},
	abstract = {Tensor networks provide a powerful new framework for classifying and simulating correlated and topological phases of quantum matter. Their central premise is that strongly correlated matter can only be understood by studying the underlying entanglement structure and its associated (generalised) symmetries. In essence, tensor networks provide a compressed, holographic description of the complicated vacuum fluctuations in strongly correlated systems, and as such they break down the infamous many-body exponential wall. These lecture notes provide a concise overview of the most important conceptual, computational and mathematical aspects of this theory.},
	urldate = {2026-01-02},
	publisher = {arXiv},
	author = {Cuiper, Bram Vancraeynest-De and Wiesiolek, Weronika and Verstraete, Frank},
	month = dec,
	year = {2025},
	note = {arXiv:2512.24390 [cond-mat]},
	keywords = {Quantum Physics, Condensed Matter - Strongly Correlated Electrons, High Energy Physics - Theory, Mathematical Physics},
	annote = {Comment: Comments welcome},
	file = {Full Text PDF:/Users/goessmann/Zotero/storage/F982PJY7/Cuiper et al. - 2025 - Les Houches Lectures Notes on Tensor Networks.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/3IQMUB46/2512.html:text/html},
}

@article{araujo_low-rank_2023,
	title = {Low-rank quantum state preparation},
	volume = {43},
	url = {https://ieeexplore.ieee.org/abstract/document/10190145/},
	number = {1},
	urldate = {2026-01-05},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Araujo, Israel F. and Blank, Carsten and Araújo, Ismael CS and da Silva, Adenilton J.},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {161--170},
	file = {Available Version (via Google Scholar):/Users/goessmann/Zotero/storage/WN2FKDXU/Araujo et al. - 2023 - Low-rank quantum state preparation.pdf:application/pdf},
}

@article{herbert_no_2021,
	title = {No quantum speedup with {Grover}-{Rudolph} state preparation for quantum {Monte} {Carlo} integration},
	volume = {103},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.103.063302},
	doi = {10.1103/PhysRevE.103.063302},
	abstract = {We prove that there is no quantum speedup when using quantum Monte Carlo integration to estimate the mean (and other moments) of analytically defined log-concave probability distributions prepared as quantum states using the Grover-Rudolph method.},
	number = {6},
	urldate = {2026-01-05},
	journal = {Physical Review E},
	author = {Herbert, Steven},
	month = jun,
	year = {2021},
	note = {Publisher: American Physical Society},
	pages = {063302},
	file = {APS Snapshot:/Users/goessmann/Zotero/storage/PXWPI48V/PhysRevE.103.html:text/html;Eingereichte Version:/Users/goessmann/Zotero/storage/PCDEB7HX/Herbert - 2021 - No quantum speedup with Grover-Rudolph state preparation for quantum Monte Carlo integration.pdf:application/pdf},
}

@misc{grover_creating_2002,
	title = {Creating superpositions that correspond to efficiently integrable probability distributions},
	url = {http://arxiv.org/abs/quant-ph/0208112},
	doi = {10.48550/arXiv.quant-ph/0208112},
	abstract = {We give a simple and efficient process for generating a quantum superposition of states which form a discrete approximation of any efficiently integrable (such as log concave) probability density functions.},
	urldate = {2026-01-05},
	publisher = {arXiv},
	author = {Grover, Lov and Rudolph, Terry},
	month = aug,
	year = {2002},
	note = {arXiv:quant-ph/0208112},
	keywords = {Quantum Physics},
	annote = {Comment: 2 pages},
	file = {Preprint PDF:/Users/goessmann/Zotero/storage/H7KFR3ZQ/Grover und Rudolph - 2002 - Creating superpositions that correspond to efficiently integrable probability distributions.pdf:application/pdf;Snapshot:/Users/goessmann/Zotero/storage/QN6WK5RQ/0208112.html:text/html},
}

@article{rudolph_synergistic_2023,
	title = {Synergistic pretraining of parametrized quantum circuits via tensor networks},
	volume = {14},
	url = {https://www.nature.com/articles/s41467-023-43908-6},
	number = {1},
	urldate = {2026-01-05},
	journal = {Nature Communications},
	author = {Rudolph, Manuel S. and Miller, Jacob and Motlagh, Danial and Chen, Jing and Acharya, Atithi and Perdomo-Ortiz, Alejandro},
	year = {2023},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {8367},
	file = {Available Version (via Google Scholar):/Users/goessmann/Zotero/storage/2XWZ8B6U/s41467-023-43908-6.html:text/html},
}

@article{rudolph_decomposition_2023,
	title = {Decomposition of matrix product states into shallow quantum circuits},
	volume = {9},
	url = {https://iopscience.iop.org/article/10.1088/2058-9565/ad04e6/meta},
	number = {1},
	urldate = {2026-01-05},
	journal = {Quantum Science and Technology},
	author = {Rudolph, Manuel S. and Chen, Jing and Miller, Jacob and Acharya, Atithi and Perdomo-Ortiz, Alejandro},
	year = {2023},
	note = {Publisher: IOP Publishing},
	pages = {015012},
	file = {Available Version (via Google Scholar):/Users/goessmann/Zotero/storage/WT6JDSGX/Rudolph et al. - 2023 - Decomposition of matrix product states into shallow quantum circuits.pdf:application/pdf},
}
