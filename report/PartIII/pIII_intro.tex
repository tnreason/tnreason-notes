\chapter{Introduction to \parref{par:three}}

\begin{highlight}
    In view of all that..., the many obstacles we appear to have surmounted, what casts the pall over our victory celebration?
    It is the curse of dimensionality, a malediction that has plagued the scientists from earliest days. - \text{Richard Bellman \cite{bellman_adaptive_1961}}
\end{highlight}

In \parref{par:three} we provide a more general discussion of tensor calculus, which has been applied in \parref{par:one} and \parref{par:two}.
Since the contraction operation is central in these calculus schemes, we refer to the set of techniques as contraction calculus.

\sect{Encoding Schemes for Functions}

We investigate in more detail the following encoding schemes for functions:
\begin{itemize}
    \item \textbf{\coordinateEncodings{}} use the real coordinates multiplying each one-hot encoding to store information.
    They have implicitly been used in factored representation of systems in \parref{par:one}, where probability distributions and logical formulas have been treated as tensors.
    More precise, these tensors are the coordinate encodings of probability distributions and logical formulas, which are maps of the state set into the interval $[0,1]$ and into the set $\ozset$.
    We investigate these encodings in more detail in \charef{cha:coordinateCalculus}.
    \item \textbf{\basisEncodings{}} are sums of a collection of one-hot encodings.
    They differ from coordinate encodings that they do not allow weights by general real numbers in these sums, and restrict to booleans.
    Basis encodings therefore map the set of subsets to an enumerated set bijectively onto the set of boolean tensors.
    We introduce them in \charef{cha:basisCalculus} in most generality as an encoding scheme for subsets of enumerated set, which we then extend towards relations and functions.
    The main advantage of basis encodings is the efficient representation of function compositions by tensor networks of basis encodings.
    This scheme is the main representation paradigm to derive efficient tensor network decompositions in artificial intelligence.
    \item \textbf{Selection encodings} are specific coordinate encodings for tensor-valued functions on the state set of a factored representation.
    Here selection variables are introduced to enumerate the coordinates of the target tensor space and treated as additional variables of a factored system representation.
    We have exploited them in \parref{par:two} for efficient representation of function sets, where the functions rely on a common structure.
\end{itemize}


\sect{Schemes for Tensor Calculus}

We applied in \parref{par:one} and \parref{par:two} two schemes of tensor calculus, which we now investigate in \parref{par:three} in more detail.
\begin{itemize}
    \item \textbf{\CoordinateCalculus{}:} We study in \charef{cha:coordinateCalculus} schemes to exploit \coordinateEncodings{} in calculus.
    Retrieval of single coordinates is done by contractions of the tensors with one-hot encodings, with no open variables (see \theref{the:coordinateCalculus}).
    \item \textbf{\BasisCalculus{}:} In \charef{cha:basisCalculus} we investigate the properties of \basisEncodings{} to perform calculus.
    The evaluation of a function at a state is performed by contractions with one-hot encodings, with the computed variable left open (see \theref{the:basisCalculus}).
    As the main advantage of \basisEncodings{} over \coordinateEncodings{}, we can represent composed function by contractions of \basisEncodings{} to each component (see \theref{the:compositionByContraction}).
\end{itemize}

\sect{Classification of Tensors}

We frequently worked in \parref{par:one} and \parref{par:two} with tensors, which have non-negative coordinates and occasionally are boolean (see \defref{def:booleanTensor}) or directed (see \defref{def:directedTensor}).
While boolean tensors have appeared as semantical representation of formulas, directed tensors have appeared mostly as conditional distributions.
The set of tensors, which are both boolean and directed receive further interest, since they are exactly the basis encodings of functions.
We sketch this coarse classification scheme in \figref{fig:dbTensorSketch}.
\begin{figure}[t]
    \begin{center}
        \input{PartIII/tikz_pics/basis_calculus/directed_binary_sketch.tex}
    \end{center}
    \caption{Sketch of the tensors with non-negative coordinates.
    We investigate in this chapter tensors, which are directed and boolean.}\label{fig:dbTensorSketch}
\end{figure}

\sect{Efficient Representation and Reasoning}

As a main topic of this work, tensor networks are motivated to provide efficient representation and reasoning schemes in artificial intelligence.
More precisely, we investigate in \parref{par:three} three aspects of efficiency:
\begin{itemize}
    \item \textbf{Efficient storage schemes} can be derived for tensors with specific properties, whereas generic tensor storage schemes suffer from the curse of dimensions.
    In \charef{cha:sparseRepresentation} we investigate the efficient representation of tensors based on $\cpformat$ decompositions.
    When restricting the allowed leg tensors in different ways, we show that relational databases can be utilized as sparse storage of tensors.
    \item \textbf{Efficient executions of contractions} will be derived based on message-passing schemes in \charef{cha:messagePassing}.
    They are in most generality derived from expectation propagation schemes in variational inference.
    \item \textbf{Approximation schemes} to tensors are investigated in \charef{cha:approximation}.
    Approximating formats are oriented on the efficient representation of the approximating tensor.
\end{itemize}

