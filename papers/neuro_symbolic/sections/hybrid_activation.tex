\section{\HybridLogicNetworks{}}\label{sec:hln}

Let us now exploit the common formulation of logical formulas and probabilistic models in \CompActNets{} to define hybrid models that combine both aspects.
We call \CompActNets{} \HybridLogicNetworks{} in the special case of Boolean statistics $\sstat$ and elementary activations.

\subsection{Parametrization}

We first introduce \HybridLogicNetworks{}, which can be regarded as a unification of logical and probabilistic models.

\begin{definition}[\HybridLogicNetwork{} (HLN)]
    \label{def:hybridLogicNetwork}
    Given a Boolean statistic $\hlnstat$, we call any element of $\elrealizabledistsof{\hlnstat}$ a \HybridLogicNetwork{}.
    The extended canonical parameter set for $\hlnstat$ is the set
    \begin{align*}
        \hybridparamset\coloneqq
        \{\hardparam\wcols \hardlegset\subset[\seldim]\ncond \headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]\} \times \parspace \, .
    \end{align*}
    For each \HybridLogicNetwork{} $\hlnwith$, we can associate a tuple $\hybridparam$ consisting of a subset $\hardlegset\subset[\seldim]$, a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$, and $\canparamwithin$ such that
    \begin{align*}
        \hlnwith
        = \normalizationof{\hlnstatccwith,\paracttensorwith}{\shortcatvariables}
    \end{align*}
    where the activation core is
    \begin{align*}
        \paracttensorwith = \contractionof{\softacttensorwith,\hardacttensorwith}{\headvariables} \, .
    \end{align*}
\end{definition}

%% Parametrization explanation
We notice that the parametrization by $\hybridparamset$ is one-to-one for any non-vanishing elementary activation tensor up to a scalar factor.
Given an arbitrary elementary activation tensor $\bigotimes_{\selindexin}\acttensorlegwith$, we can always find a corresponding tuple in $\hybridparamset$ by choosing\footnote{Here $\nonzeroof{\cdot}$ is the indicator of non-zero entries acting coordinatewise and $\onesat{\headvariableof{\selindex}}$ is the vector $[1,1]^T$.}
\begin{align*}
    \hardlegset = \{\selindex \wcols \nonzeroof{\acttensorlegwith}\neq\onesat{\headvariableof{\selindex}}\} \, ,
\end{align*}
further for all $\selindex\in\hardlegset$
\begin{align*}
    \headindexof{\selindex}
    = \begin{cases}
          0 & \ifspace \nonzeroof{\acttensorlegwith} = \onehotmapofat{0}{\headvariableof{\selindex}} \\
          1 & \ifspace \nonzeroof{\acttensorlegwith} = \onehotmapofat{1}{\headvariableof{\selindex}} \,
    \end{cases}
\end{align*}
and a parameter vector $\canparamwithin$ defined for all $\selindexin$ as
\begin{align*}
    \canparamat{\indexedselvariable} =
    \begin{cases}
        0 & \ifspace \selindex\in\hardlegset \\
        \lnof{\frac{\acttensorlegat{\headvariableof{\selindex}=1}}{\acttensorlegat{\headvariableof{\selindex}=0}}}
        & \ifspace \selindex\notin\hardlegset \, .
    \end{cases}
\end{align*}
Then we have by construction that there is $\lambda>0$ with
\begin{align*}
    \bigotimes_{\selindexin}\acttensorlegwith
    = \lambda \cdot \paracttensorwith \, .
\end{align*}

Let us demonstrate the utility of \HybridLogicNetworks{} with an example from accounting.

\input{../examples/hybrid_activation/hln_accounting_rep}

\subsection{Parameter estimation in \HybridLogicNetworks{}}\label{sec:paramEst}

%% Likelihood maximization
Let us now briefly discuss how \HybridLogicNetworks{} can be trained on data based on likelihood maximization.
Given a dataset $\dataset$ consisting of $\datanum$ independent and identically distributed samples from an unknown distribution, we want to find a \HybridLogicNetwork{} $\hlnwith$ with a statistic $\sstat=(\formulaof{0},\dots,\formulaof{\seldim-1})$ that minimizes the negative log likelihood
\begin{align*}
    \lossof{\hybridparam} \coloneqq -\frac{1}{\datanum} \sum_{\datindexin} \lnof{\probofat{\hlnparameters}{\shortcatvariables=\shortcatindices^{\datindex}}}
    \, .
\end{align*}
% We notice that this is $\infty$ if and only if there is a data point $\datindexin$ with
% \begin{align*}
%     \hlnformulaat{\shortcatvariables=\shortcatindices^{\datindex}}=0 \, ,
% \end{align*}    
% \janina{where the tuple $(A,y_A)$ denotes the hard logic part of the network, $t=(f_1,\dots,f_L)$, and
% \begin{align*}
%     \hlnformulawith =
%     \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
%     \land
%     \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
% \end{align*} (Moved from theorem in next subsection to here.)}
% If this is not the case, w
We can rewrite the loss using the empirical mean vector $\datameanwith\in\parspace$, which is defined for $\selindexin$ as
\begin{align*}
    \datameanat{\indexedselvariable}
    = \frac{1}{\datanum} \sum_{\datindexin} \formulaofat{\selindex}{\shortcatvariables=\shortcatindices^{\datindex}} \, ,
\end{align*}
by
\begin{align*}
    \lossof{\hybridparam} =
    \contraction{\datameanwith,\canparamwith} - \lnof{\contraction{\paracttensorwith,\bencsstatwith}} \, .
\end{align*}
Since $\hardparam$ influences only the second term, the best hard parameters can be found by
\begin{align*}
    \hardlegset = \{\selindex \wcols \datameanat{\indexedselvariable}\in\ozset\} \andspace
    \headindexof{\selindex} = \datameanat{\indexedselvariable} \quad \text{for} \quad \selindex\in\variableset \, .
\end{align*}
We further optimize the coordinates $\selindex\in[\seldim]\setexcept{\hardlegset}$ of $\canparamwithin$ alternately by the coordinate descent steps
\begin{align*}
    \difofwrt{\lossof{\hybridparam}}{\canparamat{\indexedselvariable}} = 0
    \Leftrightarrow
    \canparamat{\indexedselvariable}
    = \lnof{
        \frac{\meanparamat{\indexedselvariable}}{(1-\meanparamat{\indexedselvariable})}
        \cdot \frac{\hypercoreat{\headvariableof{\selindex}=0}}{\hypercoreat{\headvariableof{\selindex}=1}}
    } \, .
\end{align*}
where
\begin{align*}
    \hypercoreat{\headvariableof{\selindex}}
    = \contractionof{\{\bencodingof{\formulaof{\secselindex}} \wcols \secselindex\in[\seldim]\}
        \cup\{\softactsymbolof{\secselindex,\canparam} \wcols \secselindex\in[\seldim]\ncond\secselindex\neq\selindex\}
        \cup\{\basemeasure\}}{\headvariableof{\selindex}} \, .
\end{align*}
Based on an interpretation of the coordinate descent steps as matching steps for the mean parameters or moments to $\formulaof{\selindex}$, we call this method \emph{alternating moment matching} for \HybridLogicNetworks{} and provide pseudocode for it it in \algoref{alg:AMM_HLN}.
%% Forward inference in inner loop
We notice that, during the coordinate descent steps, computing the marginal probability of the variable $\headvariableof{\selindex}$ with respect to the current network parameters is required.
This is the computational bottleneck of the algorithm and can be approached by various approximate inference methods, e.g., variational inference (see for example the CAMEL method \cite{ganapathi_constrained_2008}).

\begin{algorithm}[hbt!]
    \caption{Alternating Moment Matching for \HybridLogicNetworks{}}\label{alg:AMM_HLN}
    \begin{algorithmic}
        \Require Mean parameter $\datameanwith$
        \Ensure Parameters $\hybridparam$ for the approximating HLN $\expdist$ %, such that $\expdist$ is the (approximative) moment projection of $\empdistribution$ onto $\hlnsetof{\formulaset}$
        \iosepline
        \State Set
        \begin{align*}
            \hardlegset = \Big\{ \selindex \wcols \selindexin \ncond \meanparamat{\indexedselvariable}\in\{0,1\} \Big\}
        \end{align*}
        and a tuple $\headindexof{\hardlegset}$ with $\headindexof{\selindex}=\meanparamat{\indexedselvariable}$ for $\selindex\in\hardlegset$.
        \State Set $\canparamwith=\zerosat{\selvariable}$
        \While{Convergence criterion is not met}
            \ForAll{$\selindex\in[\seldim]\setexcept{\hardlegset}$}
                \State Compute
                \begin{align*}
                    \hypercoreat{\headvariableof{\selindex}}
                    = \contractionof{\{\bencodingof{\formulaof{\secselindex}} \wcols \secselindex\in[\seldim]\}
                        \cup\{\softactsymbolof{\secselindex,\canparam} \wcols \secselindex\in[\seldim]\ncond\secselindex\neq\selindex\}
                        \cup\{\basemeasure\}}{\headvariableof{\selindex}}
                \end{align*}
                \State Set
                \begin{align*}
                    \canparamat{\indexedselvariable} = \lnof{
                        \frac{\meanparamat{\indexedselvariable}}{(1-\meanparamat{\indexedselvariable})}
                        \cdot \frac{\hypercoreat{\headvariableof{\selindex}=0}}{\hypercoreat{\headvariableof{\selindex}=1}}
                    }
                \end{align*}
            \EndFor
        \EndWhile
        \State \Return $(\hardlegset,\headindexof{\hardlegset},\canparamwith)$
    \end{algorithmic}
\end{algorithm}

It can be shown that the algorithm converges if and only if there is a \HybridLogicNetwork{} matching the empirical moments of the data.
For more details we refer to~\cite[Chapter~9]{goessmann_tensor-network_2025}.

\input{../examples/hybrid_activation/hln_accounting_amm}

\subsection{Entailment by \HybridLogicNetworks{}} % Now

Let us now demonstrate a further use of our unified treatment of probabilistic and logical models by investigating a generalized concept of entailment.
Entailment can be generalized to probabilistic models by deciding whether a propositional formula is always satisfied given a probabilistic model.

\begin{theorem}\label{the:hybridEntailment}
    Let $\probofat{\hlnparameters}{\shortcatvariables}$ be a \HybridLogicNetwork{} and $\secexformulaat{\shortcatvariables}$ a propositional formula.
    Then $\probof{\hlnparameters}$ probabilistically entails $\secexformula$, that is,
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\secexformulaat{\shortcatvariables}} = 1 \, ,
    \end{align*}
    if and only if
    \begin{align*}
        \hlnformula \models \secexformula \, ,
    \end{align*}
    where
    \begin{align*}
        \hlnformulawith =
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
    \end{align*} 
\end{theorem}
\begin{proof}
    We have
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\secexformulaat{\shortcatvariables}} = 1
    \end{align*}
    if and only if 
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\secexformulaat{\shortcatvariables}-\onesat{\shortcatvariables}} = 0
    \end{align*}
    which is equal to 
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\lnot\secexformulaat{\shortcatvariables}} = 0 \, .
    \end{align*}
    Since $\probofat{\hlnparameters}{\shortcatvariables}$ is non-negative this is equivalent to
    \begin{align*}
        \contraction{\nonzeroof{\probofat{\hlnparameters}{\shortcatvariables}},\lnot\secexformulaat{\shortcatvariables}} = 0 \, .
    \end{align*}
    We use that
    \begin{align*}
        \nonzeroof{\probofat{\hlnparameters}{\shortcatvariables}} 
        = \hlnformulawith 
    \end{align*}
    and get that this is further equivalent to
    \begin{align*}
        \contraction{\hlnformulawith ,\lnot\secexformulaat{\shortcatvariables}} = 0 \, ,
    \end{align*}
    which is by \defref{def:logicalEntailment} $\hlnformula \models \secexformula$.
\end{proof}

\input{../examples/hybrid_activation/hln_accounting_entailment}