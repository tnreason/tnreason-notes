\section{Foundations}\label{sec:notation}

In this section, we introduce the hypergraph-based tensor network formalism and define the most general tensor network architecture of \CompActNets{} based on this formalism.

\subsection{Tensors}
\label{sec:tensors}

Tensors are multiway arrays and a generalization of vectors and matrices to higher orders.
We first provide a formal definition as real maps from index sets enumerating the coordinates of vectors, matrices, and higher-order tensors.
To ease the notation, we abbreviate sets as $[\catorder]=\{0,\ldots,\catorder-1\}$, tuples of state indices by $\catindexof{[\catorder]}=(\catindexof{0},\ldots,\catindexof{\catorder-1})$ and tuples of variables by $\catvariableof{[\catorder]}=(\catvariableof{0},\ldots,\catvariableof{\catorder-1})$.

\begin{definition}[Tensor]
    \label{def:tensor}
    For $\atomenumeratorin$, let $\catdimof{\atomenumerator}\in\nn$ and let $\catvariableof{\atomenumerator}$ be categorical variables taking values in $[\catdimof{\catenumerator}]$.
    A tensor $\hypercoreat{\catvariables}$ of order $\catorder$ and leg dimensions $\catdimof{0},\dots,\catdimof{\atomorder-1}$ is defined through its coordinates
    \begin{align*}
        \hypercoreat{\indexedshortcatvariables} =
        \hypercoreat{\indexedcatvariableof{0},\ldots,\indexedcatvariableof{\catorder-1}} \in \rr
    \end{align*}
    for index tuples
    \begin{align*}
        \shortcatindices=(\catindexof{0},\ldots,\catindexof{\catorder-1}) \in \facstates \, .
    \end{align*}
    Tensors $\hypercoreat{\shortcatvariables}$ are elements of the tensor space
    \begin{align*}
        \bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,,
    \end{align*}
    which is a linear space, enriched with the operations of coordinate-wise summation and scalar multiplication.
    We call a tensor $\hypercoreat{\shortcatvariables}$ boolean, when all coordinates are in $\{0,1\}$, and positive, when all coordinates are greater than $0$.
\end{definition}

We introduced tensors here in a non-canonical way based on categorical variables assigned to their axes.
While this may look like syntactic sugar at this point, it allows us to define contractions without further specification of axes, based on comparisons of shared variables.
%\subsection{Continuous Variables} % Needed for families of distributions
We occasionally also allow for variables $\catvariable$ taking values in infinite sets such as $\rr$, in which case we denote the set of values to a variable by $\valof{\catvariable}$.
%Contractions are then performed by generic integrations with respect to e.g. the Lebesgue measure -> But not needed!

%This notation of tensors opposed to its notation through ordered indices as common in tensor calculus, facilitates writing down contractions along individual legs and other operations.
%Occasionally, when the categorical variables of a tensor are clear from the context, we omit the notation of the variables. %further abbreviate $\hypercoreat{\catvariables}$ by $\hypercore$.
%\begin{example}[Trivial Tensor]
%    \label{exa:trivialTensor}
%    \alex{Maybe directly do the Dirac tensor (special case of $d=1$ is the trivial tensor) - explains the black dots.}
%    The trivial tensor
%    \begin{align*}
%        \onesat{\shortcatvariables} \in \facspace
%    \end{align*}
%    is defined by all coordinates being $1$, that is for all $\catindices\in\facstates$
%    \begin{align*}
%        \onesat{\indexedshortcatvariables} = 1 \, .
%    \end{align*}
%\end{example}

\begin{example}[Delta tensor]\label{exa:diracDelta}
    Given a tuple of variables $\shortcatvariables=(\catvariables)$ with identical dimension $\catdim$, where $\catorder\geq 1$, the delta tensor is the element
    \begin{align*}
        \dirdeltawith \in \bigotimes_{\catenumeratorin} \rr^{\catdim}
    \end{align*}
    with coordinates
    \begin{align*}
        \dirdeltaofat{[\catorder],\catdim}{\indexedshortcatvariables} =
        \begin{cases}
            1 \quad & \ifspace \catindexof{0} = \ldots = \catindexof{\catorder-1} \\
            0 & \text{else}
        \end{cases} \, .
    \end{align*}
    We depict this tensor by black dots, which sometimes appears as auxiliary elements in tensor network diagrams (see e.g. \figref{fig:contraction}).
    For $\catorder=1$, the delta tensor is the trivial vector, whose coordinates are constantly $1$, which we denote by $\onesat{\catvariable}$.
\end{example}

\subsection{Tensor networks and contractions}

% Diagrammatic representation in factor graphs
We use a standard visualization of tensors (dating back to \cite{penrose_spinors_1987}) by blocks with lines depicting the axes of the tensor.
Additionally, we assign to each axis of the tensor the corresponding variable $\catvariableof{\atomenumerator}$:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/hypercore}
\end{center}
We now associate categorical variables with nodes of a hypergraph and tensors with hyperedges, which are arbitrary subsets of nodes.
Based on this association we continue with the definition of tensor networks.

\begin{definition}[Tensor network]
    \label{def:tensorNetwork}
    Let $\graph=(\nodes,\edges)$ be a hypergraph, let $\catvariableof{\node}$ for $\node\in\nodes$ be categorical variables with dimensions $\catdimof{\node} \in \nn$, and let
    \begin{align*}
        \hypercoreofat{\edge}{\catvariableof{\edge}} \in \bigotimes_{\node\in\edge}\rr^{\catdimof{\node}}
    \end{align*}
    be tensors for $\edge\in\edges$, where we denote by $\catvariableof{\edge}$ the set of categorical variables $\catvariableof{\node}$ with $\node\in\edge$.
    Then, we call the set
    \begin{align*}
        \tnetofat{\graph}{\catvariableof{\nodes}} = \{\hypercoreofat{\edge}{\catvariableof{\edge}}  \wcols \edge\in\edges\}
    \end{align*}
    the tensor network of the decorated hypergraph $\graph$.
    The set of tensor networks on $\graph$ such that all tensors have non-negative coordinates is denoted by $\tnsetof{\graph}$.
\end{definition}

As examples we now present the $\cpformat$ and the $\ttformat$ formats in our hypergraph notation.

\input{../examples/notation/cp_format.tex}

\input{../examples/notation/tt_format.tex}

\subsection{Generic contractions}

Let us now exploit our graphical approach to tensor networks in the definition of contractions.

\begin{definition}
    \label{def:contraction}
    Let $\tnetof{\graph}$ be a tensor network on a decorated hypergraph $\graph=(\nodes,\edges)$.
    For any subset $\secnodes\subset\nodes$ we define the contraction of $\tnetof{\graph}$ with open variables $\catvariableof{\secnodes}$ to be the tensor (for an example see \figref{fig:contraction})
    \begin{align*}
        \contractionof{\tnetof{\graph}}{\secnodevariables} \in \bigotimes_{\node\in\secnodes} \rr^{\catdimof{\node}}
    \end{align*}
    with coordinates at indices $\catindexof{\secnodes}\in\bigtimes_{\node\in\secnodes}[\catdimof{\node}]$ by
    \begin{align*}
        \contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}} =
        \sum_{\catindexof{\setwithout{\nodes}{\secnodes}} \in\,\nodestatesof{\setwithout{\nodes}{\secnodes}}}
        \left( \prod_{\edge\in\edges}\hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right) \, .
    \end{align*}
\end{definition}

% Open variables
When an open variable $\catvariable$ does not appear in any tensor in a contraction, we define the contraction as a tensor product with the trivial tensor $\onesat{\catvariable}$ (see \exaref{exa:diracDelta}).
To ease notation, we often omit the set notation by brackets $\{\cdot\}$.

\begin{figure}
    \begin{center}
        \input{../tikz_pics/notation_basic_concepts/contraction.tex}
    \end{center}
    \caption{
        Graphical depiction of a tensor network contraction with the open variables $\catvariableof{1},\catvariableof{3}$.
        Open variables are depicted by those without a dot at the end of the line.
    }\label{fig:contraction}
\end{figure}

\input{../examples/notation/tensor_product.tex}

\subsection{Normalizations}

Based on generic contractions, we now introduce the normalization of tensors, which introduces certain contraints on tensors to be depicted by directed hyperedges.
%which is later needed in the probabilistic paradigm, e.g. to represent probability distributions in the tensor network framework.

%% Directionality by Normalization
\begin{definition}
    \label{def:normalization}
    The normalization of a tensor $\hypercorewithnodes$ on incoming nodes $\innodes\subset\nodes$ and outgoing nodes $\outnodes\subset\setwithout{\nodes}{\innodes}$ is the tensor $\normalizationofwrt{\hypercorewithnodes}{\catvariableof{\outnodes}}{\catvariableof{\innodes}}$ defined for $\catindexof{\innodes}$ as
    \begin{align*}
        \normalizationofwrt{\hypercorewithnodes}{\catvariableof{\outnodes}}{\indexedcatvariableof{\innodes}}
        = \begin{cases}
              \frac{\contractionof{\hypercore}{\catvariableof{\outnodes},\indexedcatvariableof{\innodes}}}{\contractionof{\hypercore}{\indexedcatvariableof{\innodes}}} & \ifspace \contractionof{\hypercore}{\indexedcatvariableof{\innodes}} \neq 0 \\
              \frac{1}{\prod_{\node\in\outnodes}\catdimof{\node}}\onesat{\catvariableof{\outnodes}} & \text{else}
        \end{cases} \, .
    \end{align*}
    We say that $\hypercorewithnodes$ is normalized with incoming nodes $\innodes\subset\nodes$, if
    \begin{align*}
        \hypercorewithnodes
        = \normalizationofwrt{\hypercorewithnodes}{\catvariableof{\setwithout{\nodes}{\innodes}}}{\catvariableof{\innodes}} \, .
    \end{align*}
\end{definition}

%% Diagrammatic notation
In our graphical tensor notation, we depict normalized tensors by directed hyperedges (a), which are decorated by directed tensors (b), for example when $\catvariableof{\innodes}=(\catvariableof{2},\catvariableof{3})$ and $\catvariableof{\setwithout{\nodes}{\innodes}}=(\catvariableof{0},\catvariableof{1})$:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/directed_core.tex}
\end{center}

\subsection{Function encoding and \ComputationActivationNetworks{}}

Towards presenting the function encoding schemes, we define one-hot encodings mapping the states of variables to basis tensors.

\begin{definition}[One-hot encoding]
    \label{def:onehotenc}
    To any variable $\catvariable$ taking values in $[\catdim]$, the one-hot encoding of any state $\catindex\in[\catdim]$ is the vector with coordinates
    \begin{align*}
        \onehotmapofat{\catindex}{\catvariable=\tilde{\catindex}}
        \coloneqq \begin{cases}
                      1 & \ifspace \catindex=\tilde{\catindex} \\
                      0 & \text{else} \, .
        \end{cases}
    \end{align*}
    To any tuple $\shortcatvariables$ of variables taking values in $\facstates$, the one-hot encoding of a state tuple $\shortcatindices$ is the tensor product
    \begin{align*}
        \onehotmapofat{\shortcatindices}{\shortcatvariables}
        \coloneqq \bigotimes_{\catenumeratorin} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, .
    \end{align*}
\end{definition}

We now use one-hot encodings to encode functions between state sets.

\begin{definition}[Basis encoding of maps between state sets]
    \label{def:functionRepresentation}
    Let there be two sets of variables $\shortcatvariables$ and $\headvariables$, and let there be a map
    \begin{align*}
        \statesetfunction\defcols\facstates \rightarrow  \bigtimes_{\selindexin}[\headdimof{\selindex}]
    \end{align*}
    between their state sets.
    Then, the basis encoding of $\statesetfunction$ is a tensor
    \begin{align*}
        \bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables}
        \in \left(\bigotimes_{\selindexin}\rr^{\headdimof{\selindex}}\right) \otimes \left(\facspace\right)
    \end{align*}
    defined by
    \begin{align*}
        \bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables}
        = \sum_{\shortcatindices\in\facstates}
        \onehotmapofat{\statesetfunctionev{\shortcatindices}}{\headvariables} \otimes  \onehotmapofat{\shortcatindices}{\shortcatvariables} \, .
    \end{align*}
\end{definition}
Basis encodings are normalized tensors and are thus depicted as decorations of directed edges in hypergraphs:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/bencoding}
\end{center}

% Image enumeration
We further generalize basis encodings to arbitrary functions between finite sets by the use of bijective image enumeration maps.
Given an arbitrary set $\arbset$, we say a map
\begin{align*}
    \indexinterpretation \defcols
    \bigtimes_{\catenumeratorin}[\catdimof{\catenumerator}] \rightarrow \arbset
\end{align*}
is an enumeration map of $\arbset$ by $\catorder$ variables $\catvariableof{\catenumerator}$, taking values in $\catdimof{\catenumerator}$.
Given a function $\exfunction:\inset\rightarrow\outset$ between arbitrary sets and enumerating maps $\indexinterpretationof{\insymbol}$ and $\indexinterpretationof{\outsymbol}$ for both sets, we define the basis encoding of $\exfunction$ as
\begin{align*}
    \bencodingofat{\exfunction}{\headvariables,\shortcatvariables}
    = \sum_{\arbelement\in\inset} \onehotmapofat{\invindexinterpretationofat{\outsymbol}{\exfunction(\arbelement)}}{\headvariables}
    \otimes \onehotmapofat{\invindexinterpretationofat{\insymbol}{\arbelement}}{\shortcatvariables} \, ,
\end{align*}
where $\shortcatvariables,\headvariables$ are variables taking values in $[\cardof{\inset}]$ and $[\cardof{\outset}]$.
In \exaref{exa:madicRepresentation} we present index enumeration maps for summations in $\catdim$-adic integer representations.
%
Based on these concepts, we define the most general tensor network architecture to be applied in the rest of this work.

\begin{definition}[\ComputationActivationNetwork{} (\CompActNets{})]
    \label{def:compActNets}
    Let there be a function $\sstat : \facstates \allowbreak \rightarrow \parspace$ with basis encoding $\bencodingofat{\sstat}{\headvariables,\shortcatvariables}$, where $\headvariables$ is a tuple of variables to an enumeration map of the image of $\sstat$.
    Let there further be a hypergraph $\graph=(\nodes,\edges)$ with nodes $\nodes$ containing $[\seldim]$.
    We define the by $\sstat$ computable and by $\graph$ activated family of distributions by
    \begin{align*}
        \realizabledistsof{\sstat,\graph}
        = \left\{ \normalizationof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\contractionof{\acttensor}{\headvariables} %\} \cup \{\hypercoreofat{\edge}{\sstatcatof{\edge}} \wcols \edgein \}
        }{\shortcatvariables}
              \wcols \acttensorat{\headvariableof{\nodes}} \in \tnsetof{\graph} \right\} \, .
    \end{align*}
    We refer to any member $\probat{\shortcatvariables}\in\realizabledistsof{\sstat,\graph}$ as a \emph{\ComputationActivationNetwork{}} (or shorter as a \emph{\CompActNet{}}).
    We call $\bencsstatwith$ (and any decomposition of it) the \emph{computation network} and $\acttensorat{\headvariableof{\nodes}}$ the \emph{activation network}.
\end{definition}

%% Elementary and Max hypergraphs
The elementary activated networks are representable by an elementary activation tensor with respect to the graph
\begin{align*}
    \elgraph = \big(\nodes,\{\{\node\}\wcols\nodein\}\big)
\end{align*}
and we denote such networks by $\realizabledistsof{\sstat,\elgraph}$.
Any \CompActNet{} is representable with respect to the maximal hypergraph
\begin{align*}
    \maxgraph  = \big(\nodes,\{\nodes\}\big) \, .
\end{align*}
We therefore have for any graph that $\realizabledistsof{\sstat,\graph}\subset\realizabledistsof{\sstat,\maxgraph}$.

