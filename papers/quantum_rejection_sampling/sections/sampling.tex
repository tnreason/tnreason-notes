\section{Sampling from \ComputationActivationNetworks{}}\label{sec:sampling}

We investigate, how the above circuit encoding schemes can be applied in the preparation of states, which computational basis measurements are samples from specific distributions.

In particular, we build graph-controlled circuits being compositions of \computationCircuits{} and \activationCircuits{}, for which specific conditional distributions coincide with \ComputationActivationNetworks{}.

\subsection{Preparing ancilla augmented distributions for \ComputationActivationNetworks{}}

The computation network consists already of directed cores and therefore is already a Bayesian network.
The activation network however needs ancilla augmentation.
Therefore we have:
\begin{itemize}
    \item Pendant for Coordinate Encoding in \tnreason{}: Amplitude Encoding, storing the function value in the amplitude of an ancilla qubit.
    This is realized by an \textbf{\ActivationCircuit{}} acting on an ancilla qubit in the ground state.
    \item Pendant for Basis Encoding in \tnreason{}: \textbf{\ComputationCircuit{}}, with composition by contraction property.
    Applied on the ground state, the \computationCircuit{} generates the basis encoding quantum state, which is parallel to the basis encoding.
\end{itemize}
Both are defined using controlled single qubit gates (see Sections 4.2-3 in \cite{nielsen_quantum_2010}) with ancilla qubits being the target qubits. % where the incoming qubit variable is $\cavariable{\insymbol}$ and the outgoing $\cavariable{\outsymbol}$.

\begin{figure}[t]
    \begin{center}
        \input{tikz_pics/sampling/can_elementary_augmentation}
    \end{center}
    \caption{Ancilla augmentation of an elementary \ComputationActivationNetwork{} a) by replacing each leg vector of the elementary activation tensor by an directed ancilla augmentation.}
    \label{fig:canElementaryAugmentation}
\end{figure}

%% Elementary CAN Augmentation
Elementary \ComputationActivationNetworks{} can be augmented by ancilla augmentation of each leg vector in the activation tensor network (see \figref{fig:canElementaryAugmentation}).
We understand ancilla variables as variables in a Bayesian Network having single parents, corresponding with the ancilla augmentation of an elementary tensor.
For an example, see \figref{fig:toyAccounting}.

%% Hidden variables in activation graph
To a generic activation hypergraph $\graph$, we would do ancilla augmentation for the activation tensor network, where each hidden variable is treated as a distributed qubit.
To treat it as a distributed qubit, each hidden activation qubit is prepared in uniform state (i.e. a Hadamard gate acting on a ground state) and will be omitted in the measurement scheme (either not measured, or its measurement ignored).
This can be understood from a tensor network perspective, where marginalization means contracting, which is exactly how hidden activation variables are used.
For an example in the $\ttformat$ format, see \figref{fig:canTTAugmentation}.

\begin{figure}[t]
    \begin{center}
        \input{tikz_pics/sampling/can_TT_augmentation}
    \end{center}
    \caption{Ancilla augmentation of an $\ttformat$ \ComputationActivationNetwork{} a) by a Bayesian Network b).
    The hidden variables in the $\ttformat$ activation tensor are treated as distributed variables.
    Marginalization over the hidden variables and conditioning on the activation variables in state $1$, we reproduce the \ComputationActivationNetwork{}.}
    \label{fig:canTTAugmentation}
\end{figure}

\subsection{Amplitude Amplification}

The above scheme prepares the ancilla augmented \ComputationActivationNetwork{}
\begin{align*}
    \secprobat{\avariables,\headvariables,\shortcatvariables}
    = \acceptanceprob \cdot \onehotmapofat{1_{[\seldim]}}{\avariables}
    \otimes \contractionof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\probwith}{\headvariables,\shortcatvariables}
    + (1-\acceptanceprob) \probofat{\perp}{\shortcatvariables,\headvariables,\avariables}
\end{align*}
and the prepared state is the combination of q-samples
\begin{align*}
    \qstateofat{0}{\avariables,\headvariables,\shortcatvariables}
    = \sqrt{\acceptanceprob} \cdot \onehotmapofat{1_{[\seldim]}}{\avariables}
    \otimes \contractionof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\sqrt{\probwith}}{\headvariables,\shortcatvariables}
    + \sqrt{(1-\acceptanceprob)} \sqrt{\probofat{\perp}{\shortcatvariables,\headvariables,\avariables}} \, .
\end{align*}

In the subspace $\subspaceof{}$ spanned by the axes
\begin{itemize}
    \item $\qstateof{t}=\onehotmapofat{1_{[\seldim]}}{\avariables}\otimes \contractionof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\sqrt{\probwith}}{\headvariables,\shortcatvariables}$
    \item $\qstateof{\perp}=\sqrt{\probof{\perp}}$
\end{itemize}
which can be thought of the polar angle coordinate of the state
\begin{align*}
    \qstateofat{0}{\avariables,\headvariables,\shortcatvariables}
    = \sinof{\frac{\rotanglesymbol}{2}} \qstateofat{0}{\avariables,\headvariables,\shortcatvariables}
    + \cosof{\frac{\rotanglesymbol}{2}} \qstateofat{1}{\avariables,\headvariables,\shortcatvariables}
\end{align*}
where we define an angle $\rotanglesymbol$ by
\begin{align*}
    \sinof{\frac{\rotanglesymbol}{2}} \coloneqq \sqrt{\acceptanceprob}
\end{align*}

The amplitude amplification scheme now iterates between
\begin{itemize}
    \item Reflection along $\qstateof{t}$, by
    \begin{align*}
        \reflectionofat{\goodstatesymbol}{\cavariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\outsymbol}}
        = \identityat{\cavariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\outsymbol}}
        - 2 \onehotmapofat{\onetuple{[\seldim]}}{\cavariableof{[\seldim]}{\insymbol}}
        \otimes \onehotmapofat{\onetuple{[\seldim]}}{\cavariableof{[\seldim]}{\outsymbol}}
    \end{align*}
    \item Reflection along $\qstate$, by
    \begin{align*}
        \reflectionofat{\qstate}{\cavariableof{[\seldim]}{\insymbol},\cheadvariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\outsymbol},\cheadvariableof{[\seldim]}{\outsymbol},\cavariableof{[\seldim]}{\outsymbol}}
        =& \identityat{\cavariableof{[\seldim]}{\insymbol},\cheadvariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\outsymbol},\cheadvariableof{[\seldim]}{\outsymbol},\cavariableof{[\seldim]}{\outsymbol}} \\
        &- 2 \qstateofat{0}{\cavariableof{[\seldim]}{\insymbol},\cheadvariableof{[\seldim]}{\insymbol},\ccatvariableof{[\catorder]}{\insymbol}} \otimes
        \qstateofat{0}{\cavariableof{[\seldim]}{\outsymbol},\cheadvariableof{[\seldim]}{\outsymbol},\ccatvariableof{[\catorder]}{\outsymbol}}
        %U (\identityat{\cavariableof{[\seldim]}{\insymbol},\cavariableof{[\seldim]}{\outsymbol}} - 2 \onehotmapofat{1_{[\seldim]}}{\cavariableof{[\seldim]}{\insymbol}} \otimes \onehotmapofat{1_{[\seldim]}}{\cavariableof{[\seldim]}{\outsymbol}}) U^T
    \end{align*}
    This reflection can be done by concatenating the operator preparing the ancilla-augmentation of the \ComputationActivationNetwork{} with a initial state reflection.
\end{itemize}


Now the application of $\repnum$ grover operators $S_0S_{\qstate}$ on $\qstateof{0}$ prepares a state
\begin{align*}
    \qstateofat{\repnum}{\avariables,\headvariables,\shortcatvariables}
    = \sinof{\left(\frac{1}{2}+\repnum\right)\rotanglesymbol} \qstateofat{\goodstatesymbol}{\avariables,\headvariables,\shortcatvariables}
    + \sinof{\left(\frac{1}{2}+\repnum\right)\rotanglesymbol} \qstateofat{\badstatesymbol}{\avariables,\headvariables,\shortcatvariables}
\end{align*}
The state is closest to $\qstateof{\goodstatesymbol}$ when
\begin{align*}
    \left(\frac{1}{2}+\repnum\right)
    \rotanglesymbol \approx \frac{\pi}{2} \, .
\end{align*}

Estimating for small $\acceptanceprob$ (tight for small $\acceptanceprob$)
\begin{align*}
    \sqrt{\acceptanceprob} = \sinof{\frac{\rotanglesymbol}{2}}\leq \frac{\rotanglesymbol}{2}
\end{align*}
we get a bound
\begin{align*}
    \repnumof{\mathrm{opt}} \leq \frac{\pi}{4\sqrt{\acceptanceprob}} \, .
\end{align*}
This shows that we have a square root improvement on classical rejection sampling, which would require $\frac{1}{\acceptanceprob}$ repetitions to produce a sample.
Note however that we compare quantum circuit length with the number of classical repetitions in sampling.

Literature:
\begin{itemize}
    \item \cite{grover_fast_1996} Grover algorithm (search in unstructured database)
    \item \cite{brassard_exact_1997,brassard_quantum_2002} generalization to amplitude amplification
    \item \cite{ozols_quantum_2013} introduced quantum rejection sampling (using amplitude amplification)
    \item \cite{low_quantum_2014} used quantum rejection sampling for Bayesian network sampling (which is NP-hard when conditioned on evidence, see e.g. \cite{koller_probabilistic_2009})
    \item No exponential speedups expected by Quantum Computing in AI \cite{bennett_strengths_1997}
\end{itemize}

Note, that the variable qubits are uniformly distributed when only the computation circuit is applied.
When sampling the probability distribution, we need the ancilla qubits to be in state $1$ in order for the sample to be valid.
Any other states will have to be rejected.

Classically, this can be simulated in the same way:
Just draw the variables from uniform, calculate the value qubit by a logical circuit inference and accept with probability by the computed value.

For this procedure to be more effective (and in particular not having an efficient classical pendant), we need amplitude amplification on the value qubit.
This can provide a square root speedup in the complexity compared with classical rejection sampling.

\textbf{Open Question:} Is there a way to avoid amplitude amplification and use a more direct circuit implementation of the activation network?
- Cannot be the case, when the encoding is determined by the activation tensor alone: Needs to use the computated statistic as well.
Negative result in \cite[Section~6.6]{nielsen_quantum_2010}: When using multiple applications of \computationCircuit{} in combination with further unitaries, need at least as many applications as with amplitude amplification.

\subsection{Sampling from \ComputationActivationNetworks{} as Quantum Circuits}

\red{So far: Sample from \HybridLogicNetworks{}, would need qudits for more general \ComputationActivationNetworks{}.
Can do non-elementary \ComputationActivationNetworks{}, when activating whole activation core.}

\tnreason{} provides tensor network representations of knowledge bases and exponential families following a Computation Activation architecture.
Here are some ideas to utilize quantum circuits for sampling from \ComputationActivationNetworks{}.
We can produce Q-samples for ancilla augmented \ComputationActivationNetworks{}  using \computationCircuits{} and \activationCircuits{}:
\begin{itemize}
    \item For each (sub-) statistic, prepare a qubit by \ComputationCircuits{}
    \item Based on the computed qubits, prepare ancilla qubits by \ActivationCircuits{} to the activation cores.
\end{itemize}

\begin{figure}
    \begin{center}
        \input{tikz_pics/sampling/ca_circuit}
    \end{center}
    \caption{
        Quantum Circuit to reproduce a \ComputationActivationNetwork{} (with elementary activation) by rejection sampling.
        We measure the distributed qubits $\shortcatvariables$ and the ancilla qubits $\avariableof{[\seldim]}$ and reject all samples, where an ancilla qubit is measured as $0$.
    }\label{fig:caCircuit}
\end{figure}

\input{examples/sampling/toy_accounting_circuit}

\subsection{Application}

%% Usage as forward inferer
When sampling from probability distributions, we can use these samples to estimate probabilistic queries.
Building on such particle-based inference schemes, we can perform various inference schemes for \ComputationActivationNetworks{}, such as backward inference and message passing schemes.
