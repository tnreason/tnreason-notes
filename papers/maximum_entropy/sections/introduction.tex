\section{Introduction}

\subsection{Neuro-Symbolic AI and tensor networks}

\textbf{Hybrid Reasoning:}
Combining logical and probabilistic AI is a common aim of statistical relational AI, neuro-symbolic AI and explainable AI.
Logical and probabilistic models can be treated in the tensor network formalism, which thus serves as a unifying representation language.

MaxEnt in recent AI research: % by Gemini
\begin{itemize}
    \item Reinforcement learning \cite{yoon_maximum_2024,song_survey_2025}
    \item AGI \cite{miotto_new_2025}
    \item For interferometric data \cite{mus_new-generation_2024}
    \item Bayesian Entropy NN \cite{rathnakumar_bayesian_2026}
\end{itemize}

\subsection{Entropy}

\textbf{Entropy as Information Quantifier:}
Based on Shannons source code theorem, one can interpret entropy as an quantifier of the information content in a random variable.
This has fundamental equivalences in physics and machine learning.

\textbf{Maximum Entropy in Physics:}
E.g. Maxwell-Boltzmann distributions.

\textbf{Maximum Entropy in Learning:}
Consider a learning problem where we want to estimate a model based on observed data.
The maximum entropy problem principle approaches this problem by designing statistics of the data, which means shall be reproduced in the model, and choosing the model reproducing the means of the statistic with least structure.
The entropy of a distribution quantifies the degree of structureless in a distribution and is therefore maximized to solve the learning task.

\textbf{The maximum support problem:}
Factorization using the exponential represent always distributions with the maximal support (i.e. that of the base measure).
Thus they do not discuss situations appearing in hybrid reasoning.
We here investigate the more generic case, where distributions have different support.

\subsection{Tensor Notation}

Introduce here tensors, variables, contractions.

\subsection{The Maximum Entropy Problem}

Given a non-negative and non-vanishing base measure $\basemeasurewith$, a probability distribution is a non-negative tensor $\probwith$ such that
\begin{align*}
    \contraction{\probwith,\basemeasurewith} = 1 \, .
\end{align*}
We denote the set of such distributions by
\begin{align*}
    \bmrealprobof{\basemeasure} = \left\{ \probwith \wcols \probwith \geq \zerosat{\shortcatvariables} \ncond \contraction{\probwith,\basemeasurewith} = 1 \right\} \, .
\end{align*}

The mean parameter of a distribution $\probwith$ to a statistic $\sstat:\facstates\rightarrow\selstates$ is the vector $\meanparamwith\in\rr^\seldim$ with the coordinates
\begin{align*}
    \meanparamat{\indexedselvariable} = \expectationof{\enumformula} = \contraction{\enumformulaat{\shortcatvariables},\probwith,\basemeasurewith} \, .
\end{align*}

%We express the computation of the mean parameter in the contraction of the selection encoding $\sencsstatwith$ of $\sstat$
%\begin{align*}
%    \meanparamwith = \contractionof{\probwith,\sencsstatwith}{\selvariable} \, .
%\end{align*}

The entropy of a distribution is
\begin{align*}
    \sentropyofwrt{\probwith}{\basemeasure} = \contraction{\probwith,\lnof{\probwith},\basemeasurewith} \, .
\end{align*}

The maximum entropy problem given a mean parameter $\genmeanwith$ is stated by
\begin{equation}
    \tag{$\mathrm{P}_{\sstat,\meanparam,\basemeasure}$}\label{prob:maxEntropy}
    \max_{\probwith\in\bmrealprobof{\basemeasure}} \sentropyofwrt{\probwith}{\basemeasure}
    \stspace
    \forall_{\selindexin} \wcols
    \contractionof{\probwith,\sstatcoordinateofat{\selindex}{\shortcatvariables},\basemeasurewith}{\selvariable} = \genmeanat{\indexedselvariable}
\end{equation}

%% REDUNDANT FROM LATER PROOFS
%    A quick argument shows, that maximum entropy distributions always have $\sstat$ as a sufficient statistics.
%
%    \begin{theorem}
%        \label{the:maxEntWithSufficientStatistic}
%        Any maximum entropy distribution with respect to a moment constraint on $\sstat$ and a base measure $\basemeasure$ has the sufficient statistic $\sstat$.
%    \end{theorem}
%    \begin{proof}
%        Let $\probwith$ be a feasible distribution for the maximum entropy problem, which does not have a sufficient statistic $\sstat$.
%        Then we find $\shortcatindices,\tildeshortcatindices\in\facstates$ with $\shortcatindices\neq\tildeshortcatindices$, $\sstatat{\shortcatindices}=\sstatat{\tildeshortcatindices}$, $\basemeasureat{\indexedshortcatvariables}\neq0$, $\basemeasureat{\shortcatvariables=\tildeshortcatindices}$ and $\probat{\indexedshortcatvariables}\neq\probat{\shortcatvariables=\tildeshortcatindices}$.
%        We then define a distribution $\secprobat{\shortcatvariables}$ coinciding with $\probwith$ except for the coordinates $\shortcatindices,\tildeshortcatindices$, where we set
%        \begin{align*}
%            \secprobat{\indexedshortcatvariables} = \secprobat{\shortcatvariables=\tildeshortcatindices} = \frac{\probat{\indexedshortcatvariables}+\secprobat{\shortcatvariables=\tildeshortcatindices}}{2}
%        \end{align*}
%        We notice that $\secprobat{\shortcatvariables}$ is also a feasible distribution with an larger entropy than $\probwith$.
%        Therefore, a distribution which does not have the sufficient statistic $\sstat$ cannot be a maximum entropy distribution.
%    \end{proof}
%
%    This shows that any maximum entropy distribution is in $\realizabledistsof{\sstat,\maxgraph}$, where $\maxgraph$ is the maximal hypergraph $\maxgraph=([\seldim],\{[\seldim]\})$.
%    We search for sparse representations of the corresponding activation tensors and investigate in which cases the maximum entropy distribution is also in $\realizabledistsof{\sstat,\graph}$ for sparser hypergraphs $\graph$.

%% OUTLOOK

\subsection{Contributions}

We in this paper provide tensor network representations
\begin{itemize}
%    \item Representation of any distribution with a sufficient statistics: Generic activation tensors.
    \item Representation of distributions with maximum entropy, in case of positive realizability: Elementary activation tensors
    \item Representation of generic maximum entropy distributions: CP activation tensors.
\end{itemize}

Now, we want to characterize the CP rank of the activation tensors
\begin{itemize}
    \item Depends on the face of the mean polytope, which contains the mean parameter
    \item We have thus a well-defined "CP rank" of faces
    \item Largest faces and vertices have always CP rank of 1, intermediate faces can have larger CP rank
\end{itemize}

For boolean statistics we further provide insights for boolean statistics (see Chapter~8.5):
\begin{itemize}
    \item Example of independent statistics (see Exa.~8.28): Always elementary activation tensors (hypercubes)
    \item Example of partition statistics (see Exa.~8.30):
    \item Generic criterion for elementary activation: "Cube-like" polytopes (see Def.~8.29)
\end{itemize}


\subsection{Outlook}

To prepare for the presentation of our main results we introduce
\begin{itemize}
    \item \ComputationActivationNetworks{}: A tensor network architecture, which will be used to represent maximum entropy distributions
    \item Mean polytopes: Polytopes, which contain all realizable mean parameter vectors.
\end{itemize}
We will then show, that dependent on the position of the mean parameter in the mean polytope, we can characterize the corresponding maximum entropy distribution by a \ComputationActivationNetwork{}.
