\section{Sampling from \ComputationActivationNetworks{}}

We investigate, how the above circuit encoding schemes can be applied in the preparation of states, which computational basis measurements are samples from specific distributions.

In particular, we build graph-controlled circuits being compositions of \computationCircuits{} and \activationCircuits{}, for which specific conditional distributions coincide with \ComputationActivationNetworks{}.

\subsection{Preparing ancilla augmented distributions for \ComputationActivationNetworks{}}

The computation network consists already of directed cores and therefore is already a Bayesian network.
The activation network however needs ancilla augmentation.
Therefore we have:
\begin{itemize}
    \item Pendant for Coordinate Encoding in \tnreason{}: Amplitude Encoding, storing the function value in the amplitude of an ancilla qubit.
    This is realized by an \textbf{\ActivationCircuit{}} acting on an ancilla qubit in the ground state.
    \item Pendant for Basis Encoding in \tnreason{}: \textbf{\ComputationCircuit{}}, with composition by contraction property.
    Applied on the ground state, the \computationCircuit{} generates the basis encoding quantum state, which is parallel to the basis encoding.
\end{itemize}
Both are defined using controlled single qubit gates (see Sections 4.2-3 in \cite{nielsen_quantum_2010}) with ancilla qubits being the target qubits. % where the incoming qubit variable is $\avariableof{\insymbol}$ and the outgoing $\avariableof{\outsymbol}$.


For elementary \ComputationActivationNetworks{}, we can prepare ancilla variables having single parents, corresponding with the ancilla augmentation of an elementary tensor.
For an example, see \figref{fig:toyAccounting}.

To a generic activation hypergraph $\graph$, we would do ancilla augmentation for the activation tensor network, where each hidden variable is prepared by an additional qubit in uniform state (i.e. a Hadamard gate acting on a ground state).

\subsection{Amplitude Amplification}

Literature:
\begin{itemize}
    \item \cite{grover_fast_1996} Grover algorithm (search in unstructured database)
    \item \cite{ozols_quantum_2013} introduced quantum rejection sampling (using amplitude amplification)
    \item \cite{low_quantum_2014} used quantum rejection sampling for Bayesian network sampling (which is NP-hard when conditioned on evidence, see e.g. \cite{koller_probabilistic_2009})
\end{itemize}

Note, that the variable qubits are uniformly distributed when only the computation circuit is applied.
When sampling the probability distribution, we need the ancilla qubits to be in state $1$ in order for the sample to be valid.
Any other states will have to be rejected.

Classically, this can be simulated in the same way:
Just draw the variables from uniform, calculate the value qubit by a logical circuit inference and accept with probability by the computed value.

For this procedure to be more effective (and in particular not having an efficient classical pendant), we need amplitude amplification on the value qubit.
This can provide a square root speedup in the complexity compared with classical rejection sampling.

\textbf{Open Question:} Is there a way to avoid amplitude amplification and use a more direct circuit implementation of the activation network?
- Cannot be the case, when the encoding is determined by the activation tensor alone: Needs to use the computated statistic as well.

\subsection{Sampling from \ComputationActivationNetworks{} as Quantum Circuits}

\red{So far: Sample from \HybridLogicNetworks{}, would need qudits for more general \ComputationActivationNetworks{}.
Can do non-elementary \ComputationActivationNetworks{}, when activating whole activation core.}

\tnreason{} provides tensor network representations of knowledge bases and exponential families following a Computation Activation architecture.
Here are some ideas to utilize quantum circuits for sampling from \ComputationActivationNetworks{}.
We can produce Q-samples for ancilla augmented \ComputationActivationNetworks{}  using \computationCircuits{} and \activationCircuits{}:
\begin{itemize}
    \item For each (sub-) statistic, prepare a qubit by \ComputationCircuits{}
    \item Based on the computed qubits, prepare ancilla qubits by \ActivationCircuits{} to the activation cores.
\end{itemize}

\begin{figure}
    \begin{center}
        \input{./tikz_pics/ca_circuit.tex}
    \end{center}
    \caption{
        Quantum Circuit to reproduce a \ComputationActivationNetwork{} (with elementary activation) by rejection sampling.
        We measure the distributed qubits $\shortcatvariables$ and the ancilla qubits $\avariableof{[\seldim]}$ and reject all samples, where an ancilla qubit is measured as $0$.
    }\label{fig:caCircuit}
\end{figure}

\input{examples/sampling/toy_accounting_circuit}

\subsection{Acceptance Probability by $\infty$ Renyi Divergence}

In more generality we draw samples from a generic proposal distribution $\mathbb{Q}$, which support needs to include the support of the target distribution $\probtensor$.
We draw a sample $\shortcatindices$ from $\mathbb{Q}$ and accept it with the probability
\begin{align*}
    \frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    \cdot \left(\max_{\shortcatindicesin}\frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    \right)^{-1}
\end{align*}

The acceptance probability is the $\infty$ Renyi Divergence between the proposal distribution (typically uniform) and the target distribution.

For $0<\alpha<\infty$ and $\alpha\neq 1$ the Renyi Divergence is defined as
\begin{align*}
    D_{\alpha}\left[\probtensor||\mathbb{Q}\right]
    = \frac{1}{1-\alpha} \lnof{\sum_{\shortcatindicesin}
        \frac{\probat{\indexedshortcatvariables}^{\alpha}}{\mathbb{Q}\left[\indexedshortcatvariables\right]^{\alpha-1}}
    } \, .
\end{align*}
The $\infty$ Renyi Divergence is the limit $\alpha\rightarrow\infty$
\begin{align*}
    D_{\infty}\left[\probtensor||\mathbb{Q}\right]
    = \lnof{\max_{\shortcatindicesin}
        \frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    } \, .
\end{align*}

Then we have the acceptance probability
\begin{align*}
    \secprobat{\avariable=1}
    &= \mathbb{E}_{\shortcatindices\sim\mathbb{Q}}\left[\frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]} \cdot \left(\max_{\shortcatindicesin}
                                                                                                                                                              \frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    \right)^{-1}
    \right] \\
    &= \mathbb{E}_{\shortcatindices\sim\mathbb{Q}}\left[\frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]} \right] \cdot \left(\max_{\shortcatindicesin}
                                                                                                                                                                      \frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    \right)^{-1} \\
    & = \left(\max_{\shortcatindicesin}
            \frac{\probat{\indexedshortcatvariables}}{\mathbb{Q}\left[\indexedshortcatvariables\right]}
    \right)^{-1}  \\
    & = \expof{-D_{\infty}\left[\probtensor||\mathbb{Q}\right]} \, .
\end{align*}

\textbf{Extension:}
We could increase the acceptance probability, when we sample from a propsal distribution $\mathbb{Q}$ with smaller $\infty$ Renyi divergence to $\probtensor$.
When sampling with Quantum Circuits, this could be implemented by a state preparation for the distributed variables, before the \ComputationActivationCircuit{} is applied.
It would be interesting to train variational quantum circuits for this task.
However, when we want to apply the same scheme as above, one needs to encode $\mathbb{Q}$ into the ancilla preparing rotations, so $\mathbb{Q}$ would need to be an elementary \ComputationActivationNetwork{} as well.

\subsection{Application}

%% Usage as forward inferer
When sampling from probability distributions, we can use these samples to estimate probabilistic queries.
Building on such particle-based inference schemes, we can perform various inference schemes for \ComputationActivationNetworks{}, such as backward inference and message passing schemes.
