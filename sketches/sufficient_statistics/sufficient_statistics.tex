\documentclass[aps,onecolumn,nofootinbib,pra]{article}

\usepackage{../../spec_files/arxiv}
\input{../../spec_files/standard_preamble.tex}

\input{../../macros/organization_macros.tex}
\input{../../macros/general_macros.tex}
\input{../../macros/tc_macros.tex}
\input{../../macros/tikz_macros.tex}

\begin{document}
    \title{Characterization of \ComputationActivationNetworks{} by sufficient statistics}

    \maketitle
    \date{\today}
    \tableofcontents


    \section{Foundations}

    \subsection{Information Theory (see Section 2.10 in \cite{cover_elements_2006})}

    Consider two variables $Z$ and $X$ with a joint distribution $\probat{Z,X}$, and a function $T$ on the states of $X$.
    We augment this joint distribution by a variable $\headvariableof{T}$, which is the head variable to the function $T$
    \begin{align*}
        \probat{Z,X,\headvariableof{T}} = \contractionof{\probat{Z,X},\bencodingofat{T}{\headvariableof{T},X}}{Z,X,\headvariableof{T}}
    \end{align*}
    Then we have
    \begin{align*}
        \condindependent{\headvariableof{T}}{Z}{X}
    \end{align*}
    since
    \begin{align*}
        \condprobof{\headvariableof{T}}{Z,X} = \bencodingofat{T}{\headvariableof{T},X} \otimes \onesat{Z} \, .
    \end{align*}
    Thus, the variables are a Markov Chain $Z\rightarrow X\rightarrow Y$.

    \begin{definition}
        We call $T$ sufficient statistic of $Z$, if and only if
        \begin{align*}
            I(Z;X) = I(Z;T(X)) \, .
        \end{align*}
    \end{definition}

    \begin{lemma}
        \label{lem:doubleDetSstat}
        If there is a function $Q$ such that
        \begin{align*}
            \probat{Z,X} = \contractionof{\probat{X},\bencodingofat{Q}{Z,X}}{Z,X} \, ,
        \end{align*}
        and $T$ is sufficient for $Z$, then there is a function $R$ such that
        \begin{align*}
            Q = R \circ T \, .
        \end{align*}
    \end{lemma}
    \begin{proof}
%    (See proof of Theorem 2.19 in the Report)
        Since $Z$ has a deterministic dependence on X we have $\sentropyof{Z|X} = 0$ and by the sufficient statistic assumption (using that $I(X;\headvariableof{T}) = H(\headvariableof{T})-H(X|\headvariableof{T})$) we have
        \begin{align*}
            \sentropyof{Z|\headvariableof{T}} = \sentropyof{Z|X} = 0 \, .
        \end{align*}
        Now, $\sentropyof{Z|\headvariableof{T}}$ is equal to the existence of a function $R$ mapping the states of $Y$ to $Z$, such that for any state $y$
        \begin{align*}
            \condprobat{Z}{\headvariableof{T}=y} = \onehotmapofat{R(y)}{Z} \, .
        \end{align*}
        Since $Y$ itself is computable by $X$ with the function $T$, and $Z$ with $Q$, we have
        \begin{align*}
            Q = R \circ T \, . & \qedhere
        \end{align*}
    \end{proof}

    This lemma is applied when characterizing sufficient statistics for $Z = \probat{X}$.

    \subsection{Mathematical Statistic (see Chapter 6 in \cite{casella_statistical_2001})}

    In mathematical statistic, sufficient statistics are used to characterize parameter estimation problems, i.e. where $Z$ is a parameter variable $\Theta$ of a parametrized family.
    The joint distribution of $\Theta$ and $X$ is constructed by drawing the parameter variable $\Theta$ first with outcome $\theta$ and then drawing $X$ from $\probof{\theta}$.


    \section{The Computation Mechanism of Tensor Network Decompositions}

    Sufficient statistics imply tensor network decompositions of joint distributions using basis encodings of them.
    The basis encoding of the sufficient statistics computes the sufficient statistic in the basis calculus scheme.
    We thus call this decomposition mechanism the computation mechanism.

    \begin{theorem}[Factorization Theorem of Fisher and Neyman]
        \label{the:factorizationFisherNeyman}
        Let $\probtensor$ be a joint distribution of variables $Z,X$ with values $\mathrm{val}(Z), \,\mathrm{val}(X)$ and let $T(X)$ be a statistic.
        The following are equivalent:
        \begin{itemize}
            \item[i)] The Data Processing Inequality holds straight, i.e.
            \begin{align*}
                I(Z;X) = I(Z;\headvariableof{T}) \, .
            \end{align*}
            \item[ii)] $Z\rightarrow \headvariableof{T}\rightarrow X$ is a Markov Chain, i.e.
            \begin{align*}
                \condindependent{Z}{X}{\headvariableof{T}}
            \end{align*}
            \item[iii)] There are functions $g : \imageof{T} \times \mathrm{val}(Z) \rightarrow \rr$ and $h: \mathrm{val}(X)\rightarrow\rr$ such that for any $(x,z)\in\mathrm{val}(Z)\times \mathrm{val}(X)$
            \begin{align*}
                \probat{Z=z,X=x} = g(T(x),z) \cdot h(x) \, .
            \end{align*}
        \end{itemize}
    \end{theorem}
    \begin{proof}
        $i) \Leftrightarrow ii)$:
        We have always
        \begin{align*}
            I(Z;X) = I(Z;X,\headvariableof{T}) = I(Z;\headvariableof{T}) + I(Z;X|\headvariableof{T})
        \end{align*}
        and thus if and only if $i)$ holds
        \begin{align*}
            I(Z;X|\headvariableof{T}) = 0 \, .
        \end{align*}
        Using the KL-divergence characterization of the mutual information, this is equal to
        \begin{align*}
            \condprobat{Z,X}{\headvariableof{T}} = \contractionof{\condprobat{Z}{\headvariableof{T}},\condprobat{X}{\headvariableof{T}} }{Z,X,\headvariableof{T}} \, .
        \end{align*}
        This is equivalent to the conditional independence statement $ii)$. \\

        $ii) \Rightarrow iii)$:
        For all $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$ we have
        \begin{align*}
            \condprobat{Z=z}{X=x}
            &= \condprobat{Z=z}{X=x,\headvariableof{T}=T(x)} \\
            &= \condprobat{Z=z}{\headvariableof{T}=T(x)}
        \end{align*}
        Here we used that $\headvariableof{T}$ has a deterministic dependence on $X$ and $ii)$.
        There is thus a function $g$ such that for all $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$
        \begin{align*}
            g(T(x),z) = \condprobat{Z=z}{X=x} \, .
        \end{align*}
        We further define a function $h(x)=\probat{X=x}$ and get
        \begin{align*}
            \probat{Z=z,X=x}
            &= \probat{X=x} \cdot \condprobat{Z=z}{X=x} \\
            &= g(T(x),z) \cdot h(x) \, .
        \end{align*}

        $iii) \Rightarrow ii)$:
        Using $iii)$ we have for all supported $(x,z)\in\mathrm{val}(Z)\times \mathrm{val}(X)$
        \begin{align*}
            \condprobat{Z=z}{X=x}
            &= \frac{\probat{Z=z,X=x}}{\probat{X=x}} \\
            &= \frac{g(T(x),z) \cdot h(x) }{\int g(T(x),z) \cdot h(x) \, dz} \\
            &= \frac{g(T(x),z)}{\int g(T(x),z)  \, dz} \\
            &= \frac{
                \left(\int_{\tilde{x}:T(x)=T(\tilde{x})} \, h(x) \, dx \right) \cdot g(T(x),z)
            }{
                \left(\int_{\{\tilde{x}:T(x)=T(\tilde{x})\}} \, h(x) \, dx \right) \cdot \int g(T(x),z)  \, dz
            } \\
            &= \frac{\probat{Z=z,\headvariableof{T}=T(x)}}{\probat{\headvariableof{T}=T(x)}} \\
            &= \condprobat{Z=z}{\headvariableof{T}=T(x)}
        \end{align*}

        We have at almost all $y\in\mathrm{val}(\headvariableof{T})$, $z\in\mathrm{val}(Z)$ and $x\in\mathrm{val}(X)$ that $y=T(x)$ and
        \begin{align*}
            \condprobat{Z=z}{X=x,\headvariableof{T}=y}
            = \condprobat{Z=z}{X=x}
        \end{align*}
        and with the above at thus at almost all such pairs
        \begin{align*}
            \condprobat{Z=z}{X=x,\headvariableof{T}=y} = \condprobat{Z=z}{\headvariableof{T}=y} \, .
        \end{align*}
        This is equivalent to $ii)$.
    \end{proof}

    \theref{the:factorizationFisherNeyman} thus states, that whenever a sufficient statistic $T$ of $X$ exists for a variable $Z$, then the joint distribution of $X$ and $Z$ decomposes as sketched in \figref{fig:comDecSufStat}.

    \begin{figure}[t]
        \begin{center}
            \input{./tikz_pics/computation_decomposition.tikz}
        \end{center}
        \caption{Sketch of the computation decomposition of a joint distribution of $X,Z$ given a sufficient statistic $T$.
        This decomposition follows from the Fisher-Neyman factorization \theref{the:factorizationFisherNeyman}.}\label{fig:comDecSufStat}
    \end{figure}


    \section{Sufficient Statistic for Parametrized Families}

    Sufficient statistics are treated in mathematical statistics and in information theory.
    We here choose a definition of information theory and apply a factorization theorem of mathematical statistics to relate with \ComputationActivationNetworks{}.
    The distribution of a canonical parameter is now drawn from a (possibly continuous) random variable $\Theta$, which takes values $\theta\in\Gamma$ with probability
    \begin{align*}
        \secprobat{\Theta=\theta} \, .
    \end{align*}

    \begin{definition}[Sufficient statistics for Parameters]
        Let $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ be a family of probability distributions and
        \begin{align*}
            \sstat\defcols\facstates\rightarrow\bigtimes_{\selindexin}[\seldimof{\selindex}]
        \end{align*}
        be a function.
        We say that $\sstat$ is sufficient for $\Theta$, if for any distribution $\secprobat{\Theta}$ of $\Theta$, when drawing $\shortcatvariables$ from $\probofat{\theta}{\shortcatvariables}$ with probability $\secprobat{\Theta=\theta}$, we have that
        \begin{align*}
            \condindependent{\Theta}{\shortcatvariables}{\sstatat{\shortcatvariables}} \, .
        \end{align*}
    \end{definition}

    We can characterize \ComputationActivationNetworks{} with arbitrary base measures based on sufficient statistics.

    \begin{theorem}[Characterization of \ComputationActivationNetworks{}]
        Let $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ be a family of probability distributions with a sufficient statistic $\sstat$.
        Then there is a non-negative (possibly non-Boolean) base measure $\basemeasurewith$ and a map
        \begin{align*}
            h : \Gamma \rightarrow \bigotimes_{\selindexin} \rr^{\seldimof{\selindex}}
        \end{align*}
        such that for all $\theta\in\Gamma$
        \begin{align*}
            \probofat{\theta}{\shortcatvariables}
            = \normalizationof{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith}{\shortcatvariables} \, .
        \end{align*}
        We further have that for a set $\{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}$ $\sstat$ is a sufficient statistic, if and only if there is a non-negative (possibly non-Boolean) base measure $\basemeasurewith$ with
        \begin{align*}
            \{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}
            \subset \realizabledistsof{\sstat,\maxgraph,\basemeasure} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        By the Fisher-Neyman Factorization \theref{the:factorizationFisherNeyman} we have that $\sstat$ is a sufficient statistic if and only if there are real-valued functions $g$ on $\left(\bigtimes_{\selindexin}[\seldimof{\selindex}]\right)\times \Gamma$ and $h$ on $\facstates$ such that
        \begin{align}
            \label{eq:conFactorizationSufStat}
            \probofat{\theta}{\indexedshortcatvariables}
            = g(\sstatat{\shortcatindices},\Gamma) \cdot h(\shortcatindices) \, .
        \end{align}
        We define a base measure by the coordinate encoding of $h$ by
        \begin{align*}
            \basemeasureat{\shortcatvariables} = \sum_{\shortcatindicesin} h(\shortcatindices) \onehotmapofat{\shortcatindices}{\shortcatvariables}
        \end{align*}
        and for each $\theta\in\Gamma$ an activation tensor
        \begin{align*}
            \acttensorofat{\theta}{\headvariables} = \sum_{\shortheadindices} g(\shortheadindices,\theta) \onehotmapofat{\shortheadindices}{\headvariables} \, .
        \end{align*}
        With this we have for any $\theta\in\Gamma$
        \begin{align*}
            \contraction{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith} = 1
        \end{align*}
        and thus for any $\shortcatindicesin$ applying basis calculus
        \begin{align*}
            \normalizationof{h(\Gamma)[\headvariables],\bencsstatwith,\basemeasurewith}{\indexedshortcatvariables}
            &= h(\Gamma)[\headvariables=\sstatat{\shortcatindices}] \cdot \basemeasureat{\indexedshortcatvariables} \\
            &= g(\sstatat{\shortcatindices},\Gamma) \cdot h(\shortcatindices) \\
            &= \probofat{\theta}{\indexedshortcatvariables}
            \, .
        \end{align*}
        We therefore find for any $\probofat{\theta}{\shortcatvariables}$ a representation as a \ComputationActivationNetwork{} in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ with the activation tensor $h(\Gamma)[\headvariables]$.

        To show the second claim, we are left to show that any set of \ComputationActivationNetworks{} in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ has $\sstat$ as a sufficient statistic.
        Let us thus consider a parametric family
        \begin{align*}
            \{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Gamma\}
            \subset \realizabledistsof{\sstat,\maxgraph,\basemeasure} \, .
        \end{align*}
        By this inclusion we find for any $\theta\in\Gamma$ an activation core $\actcoreofat{\theta}{\headvariables}$.
        We then construct functions $g$ and $h$ by
        \begin{align*}
            g(\shortheadindices,\Gamma) = \actcoreofat{\theta}{\indexedheadvariableof{[\seldim]}}
            \andspace
            h(\shortcatindices) = \basemeasureat{\indexedshortcatvariables}
        \end{align*}
        and notice that the equivalent condition \eqref{eq:conFactorizationSufStat} to $\sstat$ being a sufficient statistic is satisfied.
    \end{proof}


    \section{Sufficient Statistic for the Probability}

    % Compare with report version
    We here consider sufficient statistics for the parameter of a parametrized family, while in the report we considered sufficient statistics for the probability mass as a random variable.
    In both cases this results from the information theoretic viewpoint, that a function $T$ of $X$ is a sufficient statistic for a variable $Z$, if
    \begin{align*}
        \condindependent{Z}{X}{T(X)} \, .
    \end{align*}
    While we choose for $Z$ $\headvariableof{\theta}$ above, we now choose for $Z$ the variable $\headvariableof{\probtensor}$.
    This variable can be computed by contraction with
    \begin{align*}
        \bencodingofat{\probtensor}{\headvariableof{\probtensor},\shortcatvariables} \, .
    \end{align*}
    If $T$ is a sufficient statistic for $\headvariableof{\probtensor}$, we call it probability sufficient for $\probtensor$.

    \begin{theorem}[Theorem 2.19 in the report]
        If and only if a statistic $\sstat$ is probability sufficient for $\probwith$, then
        \begin{align*}
            \probwith \in \realizabledistsof{\sstat,\maxgraph,\trivbm} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        By \lemref{lem:doubleDetSstat} we have a function $R$ such that for all $\shortcatindicesin$
        \begin{align*}
            \probat{\indexedshortcatvariables} = (R \circ \sstat)(\shortcatindices) \, .
        \end{align*}
        By basis calculus it follows that
        \begin{align*}
            \probwith = \contractionof{R(\indexinterpretationof{\sstat}[\headvariables]),\bencsstatwith}{\shortcatvariables}
        \end{align*}
        and thus
        \begin{align*}
            \probwith \in \realizabledistsof{\sstat,\maxgraph,\trivbm} \, . & \qedhere
        \end{align*}
    \end{proof}

    Note that by this theorem w can restrict ourselves to the \ComputationActivationNetworks{} with trivial base measure for the characterization of distributions with a probability sufficient statistic.




    \section{Sufficient Statistics of Datasets}

    We now investigate sufficient statistics for random datasets, which are collections of $\catorder\cdot\datanum$ categorical variables $\catvariableof{[\catorder]\times[\datanum]}$.
    We first draw a model selection variable $\Theta$ from random and then, given the outcome $\theta$, draw independently for $\datindexin$
    \begin{align*}
        \catvariableof{[\catorder],\datindex} \sim \probofat{\theta}{\shortcatvariables} \, .
    \end{align*}
    Given a sufficient statistic $\sstat$ for $\Theta$ with respect to a single sample (i.e. size $\datanum=1$), we investigate the construction of sufficient statistics for arbitrary $\datanum$.
    The average statistic to $\sstat$ is a function
    \begin{align*}
        \sstat^{\datanum} \defcols \bigtimes_{\datindexin}\bigtimes_{\catenumeratorin} [\catdimof{\catenumerator}] \rightarrow
        \bigtimes_{\selindexin} \rr^{\seldimof{\selindex}}
    \end{align*}
    defined for any dataset $\catindexof{[\catorder]\times[\datanum]}$ by
    \begin{align*}
        \sstat^{\datanum}\left(\catindexof{[\catorder]\times[\datanum]}\right) =
        \bigtimes_{\selindexin} \left(\frac{1}{\datanum}\sum_{\datindexin} \bencodingofat{\sstatcoordinateof{\selindex}}{\headvariableof{\selindex},\shortcatvariables=\catindexof{[\catorder],\datindex}}\right) \, .
    \end{align*}

%    \begin{align*}
%        \sencodingofat{\sstat^{\datanum}}{\indexedcatvariableof{[\catorder]\times[\datanum]},\selvariable}
%        = \frac{1}{\datanum} \sum_{\datindexin} \sencodingof{\sstat}{\shortcatvariables=\catindexof{[\catorder],\datindex},\selvariable} \, .
%    \end{align*}

    We present two key results:
    \begin{itemize}
        \item If in addition to $\sstat$ being a sufficient statistic for single samples the activation tensors in a representation of the family in \ComputationActivationNetworks{} are all elementary, then the average statistic is sufficient for $\Theta$ and arbitrary $\datanum$.
        \item For any parametrized family with $\sstat$ being sufficient for $\Theta$ for $\datanum=1$ the average statistic of the indicator statistic $I(\sstat)$ is sufficient for $\Theta$ and arbitrary $\datanum$.
    \end{itemize}

    \subsection{Elementary Activation}

    \begin{theorem}
        Let $\sstat$ be a sufficient statistic for $\datanum=1$ and the activation tensors in a representation of the family by \ComputationActivationNetworks{} are all elementary, that is
        \begin{align*}
            \{\probofat{\theta}{\shortcatvariables}\wcols\theta\in\Theta\} \subset \realizabledistsof{\sstat,\elgraph,\basemeasure}
        \end{align*}
        for a non-negative tensor $\basemeasure$.
        Then the $\left(\sum_{\selindexin}\seldimof{\selindex}\right)$-dimensional average statistic
        \begin{align*}
            \sstat^{\datanum}\left(\catindexof{[\catorder]\times[\datanum]}\right) =
            \bigtimes_{\selindexin} \left(\frac{1}{\datanum}\sum_{\datindexin} \bencodingofat{\sstatcoordinateof{\selindex}}{\headvariableof{\selindex},\shortcatvariables=\catindexof{[\catorder],\datindex}}\right)
        \end{align*}
        is sufficient for $\Theta$ and arbitrary $\datanum$.
    \end{theorem}
    \begin{proof}
        This follows from the likelihood expression using
        \begin{align*}
            \datameanat{\selvariable} = \sencodingofat{\sstat^{\datanum}}{\indexedcatvariableof{[\catorder]\times[\datanum]},\selvariable} \, ,
        \end{align*}
        which we derive in the following.

        By assumption there is a non-negative base measure $\basemeasurewith$, to any $\theta\in\Theta$ we find for $\selindexin$ leg vectors $\actcoreofat{\selindex,\theta}{\headvariableof{\selindex}}$ such that
        \begin{align*}
            \probofat{\theta}{\shortcatvariables}
            = \contractionof{\{\bencsstatwith,\basemeasurewith\}\cup\{\actcoreofat{\selindex,\theta}{\headvariableof{\selindex}}\wcols\selindexin\}}{\shortcatvariables} \, .
        \end{align*}

        Using the elementary activation tensor, we have for the likelihood
        \begin{align*}
            \frac{1}{\datanum}\lnof{\prod_{\datindexin}\probofat{\theta}{\indexedcatvariableof{[\catorder],\datindex}}}
            = \frac{1}{\datanum}\sum_{\datindexin}\sum_{\selindexin} \lnof{\actcoreofat{\selindex,\theta}{\headvariableof{\selindex}=\sstatcoordinateofat{\selindex}{\catindexof{[\catorder],\datindex}}}}
            + \frac{1}{\datanum}\sum_{\datindexin} \lnof{\basemeasureat{\shortcatvariables=\catindexof{[\catorder],\datindex}}} \, .
        \end{align*}
        We notice, that the second sum does not depend on $\theta$, so it suffices to express the first sum by the average statistic.
        \begin{align*}
            \frac{1}{\datanum}\sum_{\datindexin}\sum_{\selindexin} \lnof{\actcoreofat{\selindex,\theta}{\headvariableof{\selindex}=\sstatcoordinateofat{\selindex}{\catindexof{[\catorder],\datindex}}}}
            = \sum_{\selindexin}
            \contraction{
                \sstat^{\datanum}\left(\catindexof{[\catorder]\times[\datanum]}\right)_{\selindex}\left[\headvariableof{\seldim}\right],\lnof{\actcoreofat{\selindex,\theta}{\headvariableof{\selindex}}}
            } \, .
        \end{align*}
        Here by $\sstat^{\datanum}(\cdot)_{\selindex}[\headvariableof{\selindex}]$ we denote the $[\seldimof{\selindex}]$-dimensional vector in the $\selindex$-th position of the cartesian product defining $\sstat^{\datanum}$.
        We thus have shown that the likelihood of a dataset factorizes into a function $g$ of $\sstat^{\datanum}$ and $\theta$ and a function $h$ of the dataset itself.
        By the Neyman-Fisher factorization theorem, $\sstat^{\datanum}$ is thus a sufficient statistic for $\theta$.
    \end{proof}

    \subsection{Indicator Statistics}

    \begin{definition} % Compare with report
        Given a statistic $\sstat$ we call the $\left(\prod_{\selindexin}\seldimof{\selindex}\right)$-dimensional statistic $I(\sstat)$ defined by selection variables $\selvariableof{\selindex}$ and slices
        \begin{align*}
            \sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{0}=\secselindex_0,\ldots,\selvariableof{\seldim-1}=\secselindex_{\seldim-1}}
            = \contractionof{\left\{\indicatorofat{\sstatcoordinateof{\selindex} = \secselindex_{\selindex}}{\shortcatvariables} \wcols \selindexin \right\}}{\shortcatvariables}
        \end{align*}
        the indicator statistic to $\sstat$.
    \end{definition}

    We call this the indicator statistic, since each feature indexed by $\secselindex_{[\seldim]}$ is the indicator $\onesofat{\sstat=\secselindex_{[\seldim]}}{\shortcatvariables}$.
    We now show two technical lemmata, which will result in an embedding theorem of any maximal graph family of \ComputationActivationNetworks{} into the family of \HybridLogicNetworks{} with the indicator statistic.

    \begin{lemma}
        \label{lem:sencToIndBenc}
        For any statistic $\sstat$ the selection encoding of the indicator statistics coincides with the basis encoding of $\sstat$, i.e.
        \begin{align*}
            \contractionof{
                \sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{[\seldim]}},\identityat{\selvariableof{[\seldim]},\headvariables}
            }{\shortcatvariables,\headvariables}
            = \bencsstatwith \, .
        \end{align*}
    \end{lemma}


    \begin{lemma}
        \label{lem:parStatSencToBenc}
        If $\sstat$ is a partition statistic (i.e. its features sum to the trivial feature $\onesat{\shortcatvariables}$), then for any $\hypercoreat{\selvariable}$
        \begin{align*}
            \contractionof{\sencsstatwith,\hypercoreat{\selvariable}}{\shortcatvariables}
            = \contractionof{\bencsstatwith \cup \{\actcoreofat{\selindex}{\headvariableof{\selindex}} \wcols \selindexin\}}{\shortcatvariables}
        \end{align*}
        where for $\selindexin$
        \begin{align*}
            \actcoreofat{\selindex}{\headvariableof{\selindex}}
            = \begin{bmatrix}
                  \hypercoreat{\selvariable=\selindex} \\
                  1
            \end{bmatrix} \, .
        \end{align*}
    \end{lemma}

    \begin{theorem}
        Any family of \ComputationActivationNetworks{} can be embedded into a family of \HybridLogicNetworks{} with respect to the indicator statistic of $\sstat$.
        In particular we have for any non-negative base measure
        \begin{align*}
            \realizabledistsof{\sstat,\maxgraph,\basemeasure} =
            \realizabledistsof{I(\sstat),\elgraph,\basemeasure} \, .
        \end{align*}
    \end{theorem}
    \begin{proof}
        To show $\realizabledistsof{\sstat,\maxgraph,\basemeasure} \subset \realizabledistsof{I(\sstat),\elgraph,\basemeasure}$ let $\acttensorat{\headvariables}$ be an arbitrary tensor.
        Using \lemref{lem:sencToIndBenc} and then \lemref{lem:parStatSencToBenc} on the indicator statistic we get
        \begin{align*}
            \contractionof{\acttensorat{\headvariables},\bencsstatwith}{\shortcatvariables}
            &= \contractionof{\acttensorat{\headvariables},\sencodingofat{I(\sstat)}{\shortcatvariables,\selvariableof{[\seldim]}},\identityat{\selvariableof{[\seldim]},\headvariables}}{\shortcatvariables}\\
            &= \contractionof{
                \actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}},\bencodingofat{I(\sstat)}{\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}},\shortcatvariables}
            }{\shortcatvariables}
        \end{align*}
        where by $\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}}$ we denote the elementary activation tensor constructed in \lemref{lem:parStatSencToBenc}.

        Conversely, to show $\realizabledistsof{I(\sstat),\elgraph,\basemeasure}\subset\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ and let $\probtensor$ be an arbitrary elementary activation core to an element in $\realizabledistsof{I(\sstat),\elgraph,\basemeasure}$.
        Since $I(\sstat)$ is a partition statistic, we can choose an elementary parametrizing tensor $\actcoreat{\headvariableof{\bigtimes_{\selindexin}[\seldimof{\selindex}]}}$ such that the first coordinate of the leg vectors does not vanish.
        By multiplication with a scalar, we can choose an elementary parametrizing tensor of $\probtensor$ where all first coordinates are $1$.
        Now we can apply \lemref{lem:parStatSencToBenc} and \lemref{lem:sencToIndBenc} to get a corresponding parametrization in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$.
    \end{proof}

    As a consequence of this lemma we get together with the Neyman-Fisher factorization theorem:

    \begin{theorem}
        Given any family of distributions with a sufficient statistic $\sstat$.
        Then there is a base measure $\basemeasure$ such that the family is a subset of the \HybridLogicNetworks{} with statistic $I(\sstat)$ and the base measure $\basemeasure$.
    \end{theorem}
    \begin{proof}
        By Neyman-Fisher factorization get a representation of the family by \ComputationActivationNetworks{}.
        Then the above theorem embeds this family into \HybridLogicNetworks{} to the indicator statistic.
    \end{proof}

    \subsection{Sufficient Average Indicator Statistic}

    We use the convention $1\cdot \lnof{0}=-\infty$ and $0\cdot\lnof{0}=0$.

    \begin{theorem}
        Given any by $\theta\in\Theta$ parametrized family of distributions with a sufficient statistic $\sstat$.
        Then the average of $I(\sstat)$ is sufficient for samples of arbitrary size.
    \end{theorem}
    \begin{proof}
        We use the representation of the family by \ComputationActivationNetworks{} with respect to $\sstat$ and a (possibly non-Boolean) base measure $\basemeasure$.
        In this parametrization, we choose for $\theta\in\Theta$ an activation tensor $\actcoreofat{\theta}{\headvariables}$ such that
        \begin{align*} % ! HERE NORMALIZED ACTIVATION
            \probofat{\theta}{\shortcatvariables} = \contractionof{\actcoreofat{\theta}{\headvariables},\bencsstatwith}{\shortcatvariables} \, .
        \end{align*}
        Let $\catvariableof{[\catorder]\times[n]}$ be a sample of length $n$.
        We then have for the likelihood for arbitrary $\theta$
        \begin{align*}
            \frac{1}{\datanum} \cdot \lnof{\prod_{\datindexin}\probofat{\theta}{\shortcatvariables=\catindexof{[\catorder],i}}} =
            \contraction{\lnof{\actcoreofat{\theta}{\headvariables}},\bencsstatwith,\empdistributionwith}
            + \frac{1}{\datanum} \cdot \sum_{\datindexin} \lnof{\basemeasureat{\shortcatvariables=\catindexof{[\catorder],i}}} \, .
        \end{align*}

        Now we notice that for any $\headindexof{[\seldim]}$ we have
        \begin{align*}
            \frac{1}{\datanum} I(\sstat)[\shortcatvariables=\catindexof{[\catorder],i},\selvariableof{[\seldim]}=\headindexof{[\seldim]}]
            = \contractionof{\bencsstatwith,\empdistributionwith}{\headvariables=\headindexof{[\seldim]}} \, .
        \end{align*}

        The likelihood thus depends on the data only on the average of the indicator statistic.
        The latter is thus a sufficient statistic for samples of arbitrary size.
    \end{proof}

    Let us strengthen that the average of the indicator statistic is of finite dimension $2^{\seldim}$.
    Comparison with Pitman-Koopman-Darmois:
    \begin{itemize}
        \item State the existence of a finite dimensional sufficient statistic, for arbitrary data sizes $\datanum$.
        \item Do not need to assume constant support in the parametrized family.
        \item Use \HybridLogicNetworks{} of indicator statistics instead of exponential families.
    \end{itemize}


    \section{Minimal Sufficient Statistics}

    Minimal sufficient statistics are defined by existences of functions from any sufficient statistics.

    \begin{definition}[Def. 6.2.11 in \cite{casella_statistical_2001}]
        A sufficient statistic $\sstat$ is called minimal, if for any other sufficient statistic $T$ there is a function $R$ such that
        \begin{align*}
            \sstat = R \circ T \, .
        \end{align*}
    \end{definition}

    % Interpretation: Sparsification of the activation tensor
    Note that by construction, we can choose the same base measure $h$ when factorizing with respect to different sufficient statistics.
    The activation cores $g^{(U)}$ to an arbitrary sufficient statistic $U$ can thus be further decomposed by the basis encoding of $R$ and an activation core $g^{(T)}$ to a minimal sufficient statistic as
    \begin{align*}
        g^{(U)}[\headvariableof{U},Z] = \contractionof{\bencodingofat{R}{\headvariableof{T},\headvariableof{U}},g^{(T)}[\headvariableof{T},Z]}{\headvariableof{U},Z} \, .
    \end{align*}

    Minimal sufficient statistics thus provide the best embedding into a \ComputationActivationNetworks{}, by decomposing the activation tensor into refining \ComputationActivationNetwork{}.

    \begin{theorem}[Thm. 6.2.13 in \cite{casella_statistical_2001}]
        A sufficient statistic $\sstat$ is minimal, if and only if
        \begin{align*}
            \forall_{x,y} : \left( \frac{\probofat{\theta}{\shortcatvariables=x}}{\probofat{\theta}{\shortcatvariables=y}} \quad  \text{constant among $\theta\in\Theta$} \Leftrightarrow \sstat(x) = \sstat(y) \right)
        \end{align*}
    \end{theorem}

    \begin{definition}
        Let $\Gamma$ be a set of tensors in $\bigotimes_{\selindexin}\rr^{\seldimof{\catenumerator}}$.
        We say it is coordinate expressive, if for any two $\shortheadindices,\secheadindexof{[\seldim]}$ we find two $\actcoreofat{1}{\headvariables},\actcoreofat{2}{\headvariables}\in\Gamma$ with
        \begin{align*}
            \frac{\actcoreofat{1}{\headvariables=\shortheadindices}}{\actcoreofat{1}{\headvariables=\secheadindexof{[\seldim]}}} \neq
            \frac{\actcoreofat{2}{\headvariables=\shortheadindices}}{\actcoreofat{2}{\headvariables=\secheadindexof{[\seldim]}}} \, .
        \end{align*}
        Here we allow for division by $0$, where we define $\frac{0}{0}=1$ and $\frac{\lambda}{0}=sign(\lambda)\cdot\infty$ for $\lambda\neq 0$. % CHECK, whether this convention works!
    \end{definition}

    \begin{theorem}\label{the:coordinateExpressiveFollowsMinimal}
        If there is a parametrization of a family of distributions in $\realizabledistsof{\sstat,\maxgraph,\basemeasure}$ with activation tensors $\Gamma$, which are coordinate expressive, then $\sstat$ is minimal.
    \end{theorem}
    \begin{proof}
        We show that the condition by the above cited Thm. 6.2.13 in \cite{casella_statistical_2001} is satisfied.
        Since $\sstat$ is a sufficient statistic by assumption, for any $x,y$ with $\sstat(x) = \sstat(y)$ we have $ \frac{\probofat{\theta}{\shortcatvariables=x}}{\probofat{\theta}{\shortcatvariables=y}}$ constant among $\theta$.
        Conversely, let $x,y$ be such that $ \frac{\probofat{\theta}{\shortcatvariables=x}}{\probofat{\theta}{\shortcatvariables=y}}$ does not depend on $\theta$.
        Since $\Gamma$ is coordinate expressive, it follows that $\sstat(x) = \sstat(y)$, since otherwise the quotient would differ for the distributions parametrized by corresponding $\actcoreofat{1}{\headvariables},\actcoreofat{2}{\headvariables}$.
        Thus, the condition of Thm. 6.2.13 in \cite{casella_statistical_2001} is satisfied and $\sstat$ is minimal.
    \end{proof}


    \begin{example}[Logical Formula as Statistic]
        \label{exa:singleFormulaMinimalSstat}
        Let us consider the set of distributions
        \begin{align*}
            \convhullof{\exformulaat{\shortcatvariables|\varnothing},\lnot\exformulaat{\shortcatvariables|\varnothing}}
        \end{align*}
        If $\exformula$ and $\lnot\exformula$ are satisfiable, then $\exformula$ (respectively $\lnot \exformula$) is a minimal sufficient statistic.
        This can be shown, since the activation tensors
        \begin{align*}
            \Gamma = \left\{
                         \begin{bmatrix}
                             \frac{\mu}{\contraction{\exformula}} \\
                             \frac{1-\mu}{\contraction{\lnot\exformula}}
                         \end{bmatrix}
                         \wcols \mu\in[0,1] \right\}
        \end{align*}
        parametrize this family of distributions in $\realizabledistsof{\exformula,\maxgraph,\trivbm}$ (respectively $\realizabledistsof{\lnot\exformula,\maxgraph,\trivbm}$ when exchanging the coordinates of the activation vectors) and $\Gamma$ is coordinate expressive.
    \end{example}


    \begin{example}[Statistic of \ComputationActivationNetworks{} is always minimal]
        More generally, for any family $\realizabledistsof{\sstat,\graph,\trivbm}$ the function $\sstat$ is a minimal sufficient statistic, since any contain to each basis tensor a parallel tensor, and this set is coordinate expressive.
    \end{example}

    \begin{example}[Non-minimal Sufficient Statistic]
        To provide an example of a non-minimal statistic take \exaref{exa:singleFormulaMinimalSstat} with the minimal sufficient statistic $\exformula$.
        Let $\secexformula$ be another formula, which is neither entailed nor contradicted by $\exformula$.
        It follows, that there is no map $R$ s.t. $\secexformula=R\circ\exformula$.
        Consider now the sufficient statistic for the family $\realizabledistsof{\exformula,\maxgraph,\trivbm}$ by $\hlnstat=(\exformula,\secexformula)$.
        One can easily see that this is a sufficient statistic by parametrizing the family with
        \begin{align*}
            \tilde{\Gamma} = \left\{ \acttensorat{\headvariableof{0}} \otimes \onesat{\headvariableof{1}}
                         \wcols \acttensorat{\headvariableof{0}} \in \Gamma \right\} \, .
        \end{align*}
        Consistent with \theref{the:coordinateExpressiveFollowsMinimal} we notice, that $\tilde{\Gamma}$ fails to be coordinate expressive, since for all activation tensors the quotient of the coordinates $\headindexof{[2]}=(0,0)$ and $\headindexof{[2]}=(0,1)$ is $1$ (using $\frac{0}{0}=1$). % WATCH OUT WITH 0/0 = 1!
    \end{example}


    \begin{remark}[Comparison with minimality of statistics in exponential families]
        The minimality does for exponential families not coincide with the minimality defined in \cite{wainwright_graphical_2008}.
        For example in \exaref{exa:singleFormulaMinimalSstat} the minimality is conserved, when choosing the two-dimensional statistic $(\exformula,\lnot\exformula)$.
        This can be shown, since $(\exformula,\lnot\exformula)$ is expressible by a function of the minimal statistic $\exformula$ (i.e. $R(y)=(y,1-y)$).
        Since $\exformula+\lnot\exformula=\ones$, this statistic would not be minimal in the definition of \cite{wainwright_graphical_2008}.
    \end{remark}


    Observations of minimality given datasets:
    \begin{itemize}
        \item For $\datanum=1$, the indicator statistic $I(\sstat)$ is a minimal sufficient statistic, when $\sstat$ is a minimal sufficient statistic.
        Proof:
        This follows from $I(\sstat)(\catindex)$ being the one-hot encoding of $\sstat(\catindex)$.
        For any sufficient statistic, since $\sstat$ is a function of that statistic, also $I(\sstat)$ is a function of that statistic.
        \item For $\datanum>1$ and elementary activation tensors, $\sstat^{\datanum}$ is a minimal sufficient statistic if $\sstat$ is minimal for $\datanum=1$.
        Also $I(\sstat)^{\datanum}$ is a sufficient statistic, but not minimal.
        \item For $\datanum>1$ and non-elementary activation tensors, $\sstat^{\datanum}$ is (in general) not a sufficient statistic if $\sstat$ is minimal for $\datanum=1$.
        Also $I(\sstat)^{\datanum}$ is a sufficient statistic and in most cases minimal.
    \end{itemize}


    \section{Point Estimation}

    We here derive a Tensor Network contraction representing a Rao-Blackwellized estimator to an arbitrary estimator $W$.

    For any $\catindex$ we have
    \begin{align*}
        \sum_{\tilde{\catindex} \wcols \sstat(\tilde{\catindex})=\sstat(\catindex)} \onehotmapofat{\tilde{\catindex}}{\catvariable}
        = \contractionof{\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\tilde{\catvariable},\catvariable=\catindex}
    \end{align*}

    Given a \ComputationActivationNetwork{} Representation of a parametrized family with sufficient statistic $\sstat$ (i.e. activation tensors $\acttensorofat{\theta}{\headvariableof{\sstat}}$ for any $\theta$ and a base measure $\basemeasurewith$), we have for any $\theta$
    \begin{align*}
        \probofat{\theta}{\headvariableof{\sstat}=\sstatat{\catindex}}
        = \acttensorofat{\theta}{\headvariableof{\sstat}=\sstatat{\catindex}} \cdot
        \contractionof{\basemeasureat{\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\indexedcatvariable} \, .
    \end{align*}


    For any estimator $W$ the Rao-Blackwellized estimator is thus for an arbitrary $\theta$ with
    \begin{align*}
        \tilde{W}[\indexedcatvariable]
        = \contraction{W[\tilde{\catvariable}],\condprobwrtof{\theta}{\tilde{\catvariable}}{\headvariableof{\sstat}=\sstat(\catindex)}}
        = \contractionof{W[\tilde{\catvariable}],\condprobwrtof{\theta}{\tilde{\catvariable}}{\headvariableof{\sstat}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\indexedcatvariable}
    \end{align*}

    We use that for any $\theta$ with positive activation tensor
    \begin{align*}
        \condprobwrtof{\theta}{\tilde{\catvariable}}{\headvariableof{\sstat}}
        = \normalizationofwrt{\basemeasureat{\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}}}{\tilde{\catvariable}}{\headvariableof{\sstat}}
    \end{align*}
    and get for the Rao-Blackwellized estimator
    \begin{align*}
        \tilde{W}[\catvariable]
        &= \contractionof{W[\tilde{\catvariable}],\normalizationofwrt{\basemeasureat{\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}}}{\tilde{\catvariable}}{\headvariableof{\sstat}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable} \\
        &= \frac{
        \contractionof{W[\tilde{\catvariable}],\basemeasureat{\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable}
        }{
        \contractionof{\basemeasureat{\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\tilde{\catvariable}},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable} \, .
        }
    \end{align*}

    Further:
    \begin{itemize}
        \item The Rao-Blackwellized estimator coincides with the estimator, if and only if $W$ depends on $\catvariable$ only through $\sstat$.
        That is, if and only if $W$ is itself a \ComputationActivationNetwork{} with statistic $\sstat$ and trivial base measure.
    \end{itemize}

    \section{Comments}

    \begin{itemize}
        \item In which cases is the average indicator statistic minimal?
        Hypothesis: If and only if the affine hull of the activation cores (chosen on the span of the one-hot encoded statistic image) is the span of the one-hot encoded statistic image.
        However, for boolean $\sstat$ and elementary activation tensors, there is a smaller statistic by $\sstat$.
        But the elementary activation tensors span the whole tensor space, contradicting the hypothesis.
        \item Is there a relation with indicator statistic being cube-like?
        Actually cube-likeness is required for full expressivity.
        \item Since HLNs these CANets are maximum entropy distributions with respect to indicator statistics.
        Using that we have an indicator statistic and therefore $\contraction{\datamean}=1$ we have an optimal activation tensor
        \begin{align*}
            \acttensorat{\selvariableof{[\seldim]}}
            = \contractionof{
                \left(\contractionof{\bencsstatwith,\basemeasurewith}{\shortcatvariables}\right)^{-1},\datameanat{\selvariableof{[\seldim]}}
            }{\selvariableof{[\seldim]}}
        \end{align*}
        Actually this is well-defined also in some situations, where the likelihood score is for all activation tensors $\infty$ (more precisely, when a datapoint is not in the base measure support, but its statistic coincides with the statistic of a supported state).
        \item Investigate minimality of sufficient statistic as a criterion for inductive reasoning.
        That is choose the statistic $\sstat$ which is minimal for a family of distributions.
        \item Is the statistic $\sstat^{\datanum}$ is sufficient for all maximum entropy distributions, i.e. also for those with non-elementary activation (non cube-like faces)?
        This should be the case, since the likelihood can be expressed using only $\datamean$.
    \end{itemize}

    \bibliographystyle{plainnat}
    \bibliography{../../references.bib}

\end{document}