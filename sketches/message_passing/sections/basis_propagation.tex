\section{Basis Propagation}\label{sec:basisPropagation}

\begin{theorem}
    \label{the:mpGuaranteeBasis}
    Given a directed acyclic hypergraph and a tensor network of directed boolean tensors respecting the hypergraph.
    Then the final messages of \algoref{alg:contractionPropagation} in the directed implementation are exact.
\end{theorem}
\begin{proof}
    Just show inductively that the messages are one-hot encodings of the function evaluation.
\end{proof}

%% Basis calculus interpretation
We have an interpretation of the messages by one-hot encodings of function evaluation at each node.
The basis vectors at the leafs (i.e. edges with no incoming nodes) are understood as one-hot encodings of inputs.
Passing a message $\onehotmapof{i}$ in direction thus gives the message $\onehotmapof{\exfunction(i)}$.

%% Acyclicity
Note, that the assumptions of \theref{the:mpGuaranteeBasis} are met, whenever the graph is directed and acyclic.
We do not need acyclicity of the underlying undirected graph.

%% Needed? -> just basis calculus. SVD Perspective
This is because any basis encoding of a function, the decomposition
\begin{align*}
    \bencodingof{\exfunction} = \sum_{y \in\imageof{\exfunction}} ( \sum_{i: \exfunction(i)=y}\onehotmapof{i} )  \otimes \onehotmapof{y}
\end{align*}
is a SVD of the matrification of $\bencodingof{\exfunction}$ with respect to incoming and outgoing legs.

%\red{Interpretation as dataset evaluation.}

\begin{example}[Function evaluation on a dataset]
    Let the only non-basis vectors be the incoming ones, e.g. by a dataset (i.e. averaging by $\onesat{\decvariable}$).
    Message Passing of directed and boolean message by basis encoding of functions can be interpreted as function evaluation.
    Each subfunction evaluation is passed in its one-hot encoding.
    Distinguish:
    \begin{itemize}
        \item Single inference: Basis propagation (see \figref{fig:basPropDatapoint})
        \item Batchwise inference: "Tensor Parallelism", but in the most interesting cases not exact.
        If the graph is minimally connected, we are guaranteed that the averages are exact.
        Also we can apply boolean theory to have sound messages: If in a message a state is not supported, this state cannot be reached by any data point
    \end{itemize}

    \begin{figure}
        \begin{center}
            \input{tikz_pics/bas_prop_datapoint}
        \end{center}
        \caption{
            Behavior of the contraction propagation algorithm \algoref{alg:contractionPropagation} for the evaluation of the formula $(\catvariableof{a}\land\catvariableof{b})\lor(\catvariableof{b}\Rightarrow\catvariableof{c})$ on a datapoint $(a^{\datindex},b^{\datindex},c^{\datindex})$, which is selected from a dataset $\datamap$.
            The algorithm is executed in the directed implementation, in which each message is sent exactly once.
            We sketch the $9$ iterations of the $\mathrm{While}$ loop until the algorithm terminates in the epochs $1)$ (selection of the datapoint from the dataset) $2)$ (computation of finest formula components) and $3)$  (computation of coarser formula components) .
            In each epoch we sketch in gray the received messages at the previous epoch and in blue the sent messages.
            Since the hypercores are directed and boolean, the messages are one-hot encodings interpreted as evaluations of associated functions.
        }\label{fig:basPropDatapoint}
    \end{figure}

\end{example}
% Tensor Parallelism outlook


%After having established a one-to-one connection between the directed and binary tensors with the encoding of functions, we now interpret contractions as evaluations of the respective functions.
%Applying this insight iteratively on composed functions we show the following theorem.

\begin{remark}[Basis Calculus as Message Passing]
    Given a tensor network of directed and binary tensor cores, each representing a function $\exfunctionof{\edge}$ depending on variables $\incomingnodes$.
    When there are not directed cycles, we define the compositions of $\exfunctionof{\edge}$ to be the function $\exfunction$ from the nodes $\nodesone$ not appearing as incoming nodes to the nodes $\nodestwo$ not appearing as outgoing nodes in an edge.
    Choosing arbitrary $\catindexof{\node}\in[\catdimof{\node}]$ for $\node\in\nodesone$ we have
    \begin{align*}
        \contractionof{\{\bencodingofat{\exfunctionof{\edge}}{\catvariableof{\outgoingnodes},\catvariableof{\incomingnodes}} \, : \edge=(\outgoingnodes,\incomingnodes)\in\edges\}}{\nodestwo}
        = \onehotmapof{\exfunction(\catindexof{\node} \wcols \node\in\nodesone)}\, .
    \end{align*}
\end{remark}

