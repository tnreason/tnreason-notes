\chapter{\chatextconcentration}\label{cha:concentration}

When drawing data independently from a random distribution, we are limited by random effects.
We in this chapter derive guarantees, that the learning methods introduced in \charef{cha:probReasoning} and \charef{cha:networkReasoning} are robust against such effects.

\sect{Preparations}

A random tensor is a random element of a tensor space $\facspace$, drawn from a probability distribution on $\facspace$.
In contrast to the discrete distributions investigated previously in this work, the random tensors are in most generality continuous distributions. % However, when drawing data they are 

\subsect{Fluctuation of the Empirical Distribution}

% Random one hot encodings
When drawing random states $\datapoint\in\facstates$ by a distribution $\gendistribution$, we use the one-hot encoding to forward each random state to the random tensor
\begin{align*}
    \onehotmapofat{\datapoint}{\shortcatvariables} \, .
\end{align*}
The expectation of this random tensor is
\begin{align*}
    \expectationof{\onehotmapof{\datapoint}}
    = \sum_{\shortcatindices\in\facstates} \gendistributionat{\indexedshortcatvariables} \onehotmapofat{\shortcatindices}{\shortcatvariables}
    = \gendistributionat{\shortcatvariables} \, .
\end{align*}

The empirical distribution is then the average of independent random one-hot encodings, namely the random tensor
\begin{align*}
    \empdistributionwith = \frac{1}{\datanum} \sum_{\datindexin}  \onehotmapofat{\datapoint}{\shortcatvariables} \, .
\end{align*}
To avoid confusion let us strengthen, that in this chapter we interpret $\empdistribution$ as a random tensor taking values in $\facspace$, whereas each supported value of $\empdistribution$ is an empirical distribution taking values in $\facstates$.
The forwarding of $\facstates$ under the one-hot encoding is a multinomial random variable, see \theref{the:multinomialEmpdistFluctuation}.


% Expectation -> Does not make use of independence here!
When the marginal of each datapoint is $\gendistribution$, the expectation of the empirical distribution is
\begin{align*}
    \expectationof{\empdistribution}
    = \frac{1}{\datanum} \sum_{\datindexin}  \expectationof{\onehotmapof{\datapoint}}
    = \gendistribution \, .
\end{align*}

% Law of large numbers
From the law of large numbers it follows, that in the limit of $\datanum\rightarrow\infty$ at any coordinate $\catindex\in\facstates$ almost everywhere
\[ \empdistributionat{\indexedshortcatvariables} \rightarrow \expectationof{\empdistributionat{\indexedshortcatvariables}} =  \gendistributionat{\indexedshortcatvariables} \, . \]

% Fluctuation
At finite $\datanum$ the empirical distribution differs from the by the difference
\[ \empdistribution - \gendistribution \]
which we call a fluctuation tensor.

\subsect{Mean Parameter of the Empirical Distribution}

We now investigate the empirical mean parameter
\[
    \datameanat{\selvariable} = \contractionof{\sencsstatwith,\empdistributionat{\shortcatvariables}}{\selvariable} \, .
\]

Each coordinate of $\datamean$ is decomposed as
\[ \datameanat{\indexedselvariable} = \frac{1}{\datanum}\sum_{\datindexin} \sstatcoordinateofat{\selindex}{\datapointof{\datindex}} \]
and thus stores the empirical average of the feature $\sstatcoordinateof{\selindex}$ on the dataset $\data$.

% Expectation of the empirical mean
Since the mean parameter depends linearly on the corresponding distribution, we can show the following correspondence between the empirical and the expected mean parameter.

\begin{theorem}
    \label{the:expectedMeanParameter}
    When drawing data independently from $\gendistribution$, we have $\expectationof{\datameanat{\selvariable}}=\genmeanat{\selvariable}$, where we call
    \[
        \genmeanat{\selvariable} = \contractionof{\sencsstatwith,\empdistributionat{\shortcatvariables}}{\selvariable} \,
    \]
    the expected mean parameter.
\end{theorem}
\begin{proof}
    Since the expectation commutes with linear functions.
%    Since the mean parameter of a distribution depends linearly on the distribution.
\end{proof}


% Convergence by Law of Large Numbers and issues
For each $\selindexin$ the law of large numbers guarantees that $\datameanat{\indexedselvariable}$ converges almost surely against $\genmeanat{\indexedselvariable}$ when $\datanum\rightarrow\infty$.
To utilize these we need to approach the following issues:
\begin{itemize}
    \item \textbf{Non-asymptotic bounds:} We need non-asymptotic convergence bounds, since one has access to finite data when learning
    \item \textbf{Uniform convergence:} The convergence has to happen uniformly for all $\selindexin$.
    \item \textbf{Interpretability:} Guarantees on the result of an estimated model are more accessible when provided for quantities like the canonical parameter and KL-divergences of the learning result.
    Those, however, depend nonlinearly on $\datameanat{\selvariable}$ and therefore require further investigation.
\end{itemize}


\subsect{Face Recovery}

\begin{lemma}
    Let $\dataset$ be drawn from a distribution $\gendistribution$ with mean parameter $\genmeanat{\selvariable}$ and let us denote by $\genfaceset$ the unique face of $\genmeanset$, such that $\genmeanat{\selvariable}\in\sbinteriorof{\genfaceset}$.
    We always have $\datameanat{\selvariable}\in\genfaceset$.
    If and only if $\datameanat{\selvariable}\in\sbinteriorof{\genfaceset}$, then the support of $\stanexpdistof{\datacanparam}$ coincides with the face measure of $\genfaceset$.%support of the maximum entropy distribution reproducing $\genmeanat{\selvariable}$.
\end{lemma}
\begin{proof}
    Since $\dataset$ is drawn from $\gendistribution$, the support of $\empdistribution$ is contained in the support of $\gendistribution$.
    Thus, the support of $\empdistribution$ is contained in the support of the face measure to $\genfaceset$ and $\datameanat{\selvariable}\in\genfaceset$.
    The second claim follows from the fact, that the maximum entropy distribution reproducing $\datacanparam$ is positive with respect to the face measure of the face containing $\datameanat{\selvariable}$ in its relative interior.
\end{proof}

% Interpretation
To find probabilistic guarantees on the recovery of the support of the generating distribution, we thus need to show probabilistic bounds on the event $\datameanat{\selvariable}\in\sbinteriorof{\genfaceset}$.
We call this event \emph{face recovery}.

\subsect{Mode Recovery}

Another interesting recovery scenario is whether the mode of the approximating distribution coincides with the mode of the generating distribution.
This event is called \emph{mode recovery}.

Let $\estcanparam$ be the estimator of the canonical parameter $\gencanparam$, then the mode set of both coincide, if and only if they are elements in the same max cone (see \secref{sec:modeCones}), i.e.
\begin{align*}
    \genmaxconeof{\estcanparam} = \genmaxconeof{\gencanparam} \, .
\end{align*}
Equivalently, this is the case if $\genmeanat{\selvariable}$ and $\datameanat{\selvariable}$ are elements of the image of the same normal cone under the forward mapping.


\subsect{Noise Tensor and its Width}

% Definition of noise tensors
Motivated by \theref{the:expectedMeanParameter}, we build our derivation of probabilistic guarantees on non-asymptotic and uniform convergence bounds for $\datameanat{\selvariable}$.
Let us first define the fluctuations of the empirical mean parameter, when drawing the data independently from a random distribution, as the noise tensor.

\begin{definition}
    \label{def:noiseTensor}
    Given a statistic $\sstat$, $\datanum\in\nn$ and a distribution $\gendistribution$, we call
    \begin{align*}
        \sstatnoise = \contractionof{(\empdistribution-\gendistribution),\sencsstat}{\selvariable}
    \end{align*}
    the \emph{noise tensor}, where $\datamap$ is a collection of $\datanum$ independent samples of $\gendistribution$.
\end{definition}

% Minterm
The fluctuation of the empirical distribution around the generating distribution corresponds in this notation with the universal exponential family, taking the identity as statistics.
% Appearances
Besides this, fluctuation tensors appears in \MarkovLogicNetworks{} as fluctuations of random mean parameters and in proposal distributions as fluctuation of random energy tensor.
We will discuss these examples in the following sections.


% Fluctuation of mean parameter
We notice that the fluctuation tensor $\sstatnoise$ is the centered mean parameter to the empirical distribution, that is
\begin{align*}
    \datamean - \expectationof{\datamean} =  \contractionof{\sencsstat,\empdistribution-\gendistribution}{\selvariable} \, .
\end{align*}

% Widths
In the following we will use the supremum of contractions with random tensors in the derivation of success guarantees for learning problems.
Such quantities are called widths.

\begin{definition}
    \label{def:width}
    Given a set $\canparamhypothesis\subset\facspace$ and $\sstatnoise$ a random tensor taking values in $\facspace$ we define the width as the random variable
    \begin{align*}
        \widthwrtof{\canparamhypothesis}{\sstatnoise} = \sup_{\canparamin} \absof{\contraction{\canparam,\sstatnoise}} \, .
    \end{align*}
\end{definition}

% Uniform concentration events
Bounds on the widths are also called uniform concentration bounds \cite{goessmann_uniform_2021} and generic probabilistic bounds will be provided in \secref{sec:widthBounds}.

Atomic norms induce metrics, which are widths (see \cite{chandrasekaran_convex_2012} and Chapter~5 in \cite{goessmann_uniform_2021}).

\begin{definition}
    Given a set $\Gamma\subset\parspace$ such that an open neighborhood of the origin is contained in $\convhullof{\Gamma}$.
    Then
    \begin{align*}
        \|\canparam\|_{\Gamma} = \widthwrtof{\Gamma}{\canparam}
    \end{align*}
    is the dual atomic norm and
    \begin{align*}
        \canmetricwrtof{\Gamma}{\canparam}{\seccanparam} = \widthwrtof{\Gamma}{\canparam-\seccanparam}
    \end{align*}
    is the dual atomic distance to $\Gamma$.
\end{definition}

Examples of atomic norms are:
\begin{itemize}
    \item Euclidean distance $\ell_2$, when $\Gamma=\subsphere$.
    \item Supremum distance $\ell_{\infty}$, when $\Gamma=\{\lambda\cdot \onehotmapofat{\selindex}{\selvariable} \wcols \lambda\in\{-1,+1\}\ncond \selindexin\}$.
\end{itemize}


\sect{Generic Guarantees based on Noise Widths}

We now derive error bounds for parameter estimation and structure learning, as introduced in \charef{cha:networkReasoning}.
When combined with probabilistic bounds on the noise width, they are probabilistic success guarantees.

\subsect{Parameter Estimation in Exponential Families}

%\red{We in this section always assume, that $\empdistribution$ is representable by the base measure $\basemeasure$ of the respective exponential families.}

Parameter Estimation is the M-projection of the empirical distribution onto an exponential family.
In \charef{cha:probReasoning} we have characterized those by the backward mapping acting on the mean parameter.
Thus, while we are interested in the expected canonical parameter
\[
    \gencanparamat{\selvariable} = \backwardmapof{\genmeanat{\selvariable}}
\]
we get an estimation by the empirical canonical parameter
\[
    \datacanparamat{\selvariable}  = \backwardmapof{\datameanat{\selvariable}} \, .
\]

% Nonlinearity
Unfortunately, since the backward mapping is not linear, we in general do not have that $\expectationof{\backwardmapof{\datamean}}$ coincides with $\backwardmapof{\genmean}$.
To build intuition on the concentration we recall the expression of the backward mapping as
% Concentration
\begin{align*}
    \backwardmapof{\meanparam}
    = \argmax_{\canparam} -\centropyof{\meanrepprob}{\stanexpdistof{\canparam}}
\end{align*}
where $\meanrepprob$ is any distribution reproducing the mean parameter.
We want to compare the solutions $\backwardmapof{\datamean}$ and $\backwardmapof{\genmean}$, in which case $\meanrepprob$ can be chosen as $\empdistribution$ and $\gendistribution$.
It is common to call the objectives $\centropyof{\empdistribution}{\stanexpdistof{\canparam}}$ and $\centropyof{\gendistribution}{\stanexpdistof{\canparam}}$ empirical and expected risk \cite{shalev-schwartz_shai_understanding_2014}.
Since the empirical risk has a linear dependence on $\datamean$, we have at each $\canparam$
\begin{align*}
    \expectationof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}}}
    &= \expectationof{\contraction{\datamean,\canparam} - \cumfunctionof{\canparam}} \\
    &= \contraction{\expectationof{\datamean},\canparam} - \cumfunctionof{\canparam} \\
    &= \centropyof{\gendistribution}{\stanexpdistof{\canparam}}
\end{align*}
By the law of large numbers, in the limit $\datanum\rightarrow\infty$ we thus have at each $\canparam$ a convergence of the empirical risk to the expected risk.
However, since the backward mapping is defined by the minima of these risks, we need a uniform and non-asymptotical concentration guarantee to get more useful bounds.
To this end, we now consider constrained parameter estimation and relate the supremum on the differences between expected and empirical risks with the width of the noise tensor.

\begin{lemma}
    \label{lem:centropyWidthCharacterization}
    For any $\canparamhypothesis$ and $\datamap$ we have
    \begin{align*}
        \widthwrtof{\canparamhypothesis}{\sstatnoise}
        = \sup_{\canparamin} \absof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}} - \centropyof{\gendistribution}{\stanexpdistof{\canparam}}} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    For any $\canparam\in\canparamhypothesis$ and by $\meanrepprob$ realizable mean parameter $\meanparam$ we have
    \begin{align*}
        \centropyof{\meanrepprob}{\stanexpdistof{\canparam}}
        = - \contraction{\meanparam,\canparam} + \cumfunctionof{\canparam} \, .
    \end{align*}
    It follows that
    \begin{align*}
        \centropyof{\empdistribution}{\stanexpdistof{\canparam}} - \centropyof{\gendistribution}{\stanexpdistof{\canparam}}
        = -\contraction{(\datamean-\genmean),\canparam}
    \end{align*}
    and the claim follows from comparison with \defref{def:noiseTensor} and \defref{def:width}.
\end{proof}

% This is just useful for Constrained Parameter Estimation!
As a direct consequence, we have at any $\canparam\in\canparamhypothesis$
\begin{align*}
    \absof{\centropyof{\empdistribution}{\stanexpdistof{\canparam}} - \centropyof{\gendistribution}{\stanexpdistof{\canparam}}}
    \leq \widthwrtof{\canparamhypothesis}{\sstatnoise} \, .
\end{align*}
Thus, the absolute difference of the expected risk and the empirical risk is bounded by the width of the noise tensor.
This is especially useful for the solution $\datamean$ of the empirical risk minimization, where we can state
\begin{align*}
    \centropyof{\gendistribution}{\stanexpdistof{\datacanparam}}
    \leq \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}} + \widthwrtof{\canparamhypothesis}{\sstatnoise} \, .
\end{align*}
At the solution of a empirical risk minimization problem over $\canparamhypothesis$, the expected risk exceeds the empirical risk at most by the noise tensor width.

% Further KL divergence bound when assuming gendistribution in the hypothesis
When the generating distribution is in the hypothesis, we can further show the following KL-divergence bound for the estimated distribution.

\begin{theorem}
    Let us assume that for $\gencanparam\in\canparamhypothesis$ we have $\gendistribution=\stanexpdistof{\gencanparam}$. %and that $\partitionfunctionof{\canparam}$ is constant among $\canparamin$.
    Then for any solution $\datacanparam$ of the empirical problem
    \begin{align*}
        \argmin_{\canparamin} \centropyof{\empdistribution}{\stanexpdistof{\canparam}} \,
    \end{align*}
    we have
    \begin{align}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} \leq 2\widthwrtof{\canparamhypothesis}{\sstatnoise} \, .
    \end{align}
\end{theorem}
\begin{proof}
    For the solution $\datacanparam$ of the empirical risk minimization on $\canparamhypothesis$ we have since $\gencanparam\in\canparamhypothesis$ that
    \begin{align*}
        \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}
        \leq \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}} \, .
    \end{align*}
    It follows that
    \begin{align*}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}}
        & \leq \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}}
        + \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}
        - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}} \\
        & = \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}\right) \\
        & \quad - \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\gencanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}\right) \, ,
    \end{align*}
    where we expanded the KL-divergence as a difference of cross entropies.
    We apply \lemref{lem:centropyWidthCharacterization} to estimate the terms in brackets and get
    \begin{align*}
        & \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}\right)
        - \left(\centropyof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\gencanparam}} - \centropyof{\empdistribution}{\stanexpdistof{\gencanparam}}\right) \\
        & \quad\quad \leq 2 \widthwrtof{\canparamhypothesis}{\sstatnoise} \, .
    \end{align*}
    Combined with the above inequality we arrive at
    \begin{align*}
        \kldivof{\stanexpdistof{\gencanparam}}{\stanexpdistof{\datacanparam}} \leq 2\widthwrtof{\canparamhypothesis}{\sstatnoise} \, . &  \qedhere
    \end{align*}
\end{proof}

% Unconstrained parameter estimation
One technical issue arises from the fact, that when we allow for $\canparamhypothesis=\parspace$, then $\widthwrtof{\canparamhypothesis}{\sstatnoise}$ vanishes or is infinity.
To apply the result on the unconstrained parameter estimation, we therefore need to argue on bounded sets for the canonical parameter.
When restricting to the sphere $\subsphere\subset\parspace$ we have
\begin{align*}
    \normof{\datamean-\genmean} = \widthwrtof{\subsphere}{\mlnnoise} \, ,
\end{align*}
We apply this insight to state the following guarantee for unconstrained parameter estimation.
\begin{theorem}
    \label{the:detGuaranteeUnconstrained}
    For any $\datacanparam\in\parspace$ we have
    \begin{align*}
        \absof{\centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}-\centropyof{\gendistribution}{\stanexpdistof{\datacanparam}}}
        \leq \widthwrtof{\subsphere}{\mlnnoise} \cdot \normof{\datacanparam} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    As in the proof of \lemref{lem:centropyWidthCharacterization} we use that
    \begin{align*}
        \centropyof{\empdistribution}{\stanexpdistof{\datacanparam}}-\centropyof{\gendistribution}{\stanexpdistof{\datacanparam}}
        = \contraction{\datamean-\genmean,\datacanparam}  \, .
    \end{align*}
    By Cauchy-Schwartz we further have
    \begin{align*}
        \absof{\contraction{\datamean-\genmean,\datacanparam}} \leq \normof{\datamean-\genmean}\cdot\normof{\datacanparam} \, .
    \end{align*}
    Using that $\normof{\datamean-\genmean}=\widthwrtof{\subsphere}{\mlnnoise}$ we arrive at the claim.
\end{proof}

\subsect{Structure Learning}

In the gradient heuristic of structure learning, one selects the statistic to the maximal coordinate of the energy tensor of the proposal distribution.
This tensor coincides with the mean parameter of a \MarkovLogicNetwork{} and has thus a fluctuation by the noise tensor.
We now use these insights to show a guarantee, that the formula chosen by grafting with respect to the empirical proposal distribution coincides with the formula chosen with respect to the expected proposal distribution.
To this end, we need to define the max gap, which is the difference between the maximal coordinate of a tensor to the second maximal coordinate.

\begin{definition}
    The max gap of a tensor $\hypercoreat{\shortcatvariables}$ is the quantity
    \begin{align*}
        \maxgapof{\hypercore} =
        \left(\max_{\shortcatindices} \hypercoreat{\indexedshortcatvariables}\right) -
        \left(\max_{\shortcatindices\notin\argmax_{\shortcatindices}\hypercoreat{\indexedshortcatvariables}}
        \hypercoreat{\indexedshortcatvariables}\right) \, .
    \end{align*}
\end{definition}

When comparing the gap with the noise width, we get the following guarantee.

\begin{theorem}
    \label{the:detGuaranteeProposalDist}
    Whenever
    \begin{align*}
        \maxgapof{\genmean}
        > 2 \cdot \widthwrtof{\{\onehotmapof{\shortcatindices}:\shortcatindices\in\facstates\}}{\sstatnoise} \, ,
    \end{align*}
    then any mode $\shortcatindices$ of the empirical proposal distribution is a mode of the expected proposal distribution.
\end{theorem}
\begin{proof}
    Let us assume that for a mode $\selindex^{\datamap}\in\argmax_{\selindexin}\datameanat{\indexedselvariable}$ of the empirical mean parameter we have
    \begin{align*}
        \selindex^{\datamap}\notin\argmax_{\selindexin}\genmeanat{\indexedselvariable} \, .
    \end{align*}
    For a mode $\selindex^{*}\in\argmax_{\selindexin}\genmeanat{\indexedselvariable}$ of the expected mean parameter we then have
    \begin{align*}
        \genmeanat{\selvariable=\selindex^{\datamap}} \leq \genmeanat{\selvariable=\selindex^{*}} - \maxgapof{\genmean}
    \end{align*}
    and
    \begin{align*}
        \datameanat{\selvariable=\selindex^{\datamap}} \geq \datameanat{\selvariable=\selindex^{*}} \, .
    \end{align*}
    Comparing both inequalities we get
    \begin{align*}
        \left(\datameanat{\selvariable=\selindex^{\datamap}} - \genmeanat{\selvariable=\selindex^{\datamap}}\right)
        + \left( - \datameanat{\selvariable=\selindex^{*}} + \genmeanat{\selvariable=\selindex^{*}} \right)
        \geq \maxgapof{\genmean} \, .
    \end{align*}
    Estimating the terms in the bracket by the width of the noise tensor with respect to basis vectors, we get
    \begin{align*}
        2 \cdot  \widthwrtof{\{\onehotmapof{\shortcatindices}:\shortcatindices\in\facstates\}}{\sstatnoise}
        \geq \maxgapof{\genmean} \, ,
    \end{align*}
    which is a contradiction to the assumption.
    Thus, any mode of the empirical mean parameter is also a model of the expected mean parameter.
\end{proof}


\subsect{Mode Recovery}

%Let us now consider a more general problem than in the section above, namely the estimation of the modes of a distribution.


Let us now consider the more general mode recovery problem.
To find a probabilistic bound on the mode recovery event, we generalize the gap at $\gencanparam$ as the minimal distance to other cones and bound uniform concentration events implying that the distance between $\estcanparam$ and $\gencanparam$ is smaller than the gap.

\begin{definition}
    Let $\canmetricof{\cdot}{\cdot}$ be a metric on $\parspace$, then the generalized gap of $\canparam\in\parspace$ is defined as
    \begin{align*}
        \maxgapofat{\canmetric}{\canparam} = \inf_{\seccanparam\notin\genmaxconeof{\canparam}} \canmetricof{\seccanparam}{\canparam} \, .
    \end{align*}
\end{definition}

Let us now show, that the mode recovery event holds, whenever the width of the atom set $\Gamma$ is smaller than the generalized gap.

\begin{theorem}
    Let $\Gamma\subset\parspace$ induce an atomic norm.
    If
    \begin{align*}
        \widthwrtof{\Gamma}{\estcanparam-\gencanparam} < \maxgapofat{\canmetricwrt{\Gamma}}{\gencanparam}
    \end{align*}
    then the modes of $\stanexpdistof{\estcanparam}$ and $\stanexpdistof{\gencanparam}$ coincide.
\end{theorem}
\begin{proof}
    If $\estcanparam\notin\genmaxconeof{\gencanparam}$, then
    \begin{align*}
        \maxgapofat{\canmetricwrt{\Gamma}}{\gencanparam} \geq \canmetricwrtof{\Gamma}{\estcanparam}{\gencanparam} = \widthwrtof{\Gamma}{\estcanparam-\gencanparam} \, ,
    \end{align*}
    which contradicts the assumption.
    Therefore if the assumption holds, then $\estcanparam\in\genmaxconeof{\gencanparam}$ and the modes of $\stanexpdistof{\estcanparam}$ and $\stanexpdistof{\gencanparam}$ coincide.
\end{proof}

The guarantee on structure learning is the special case, where $\Gamma=\{\lambda\cdot \onehotmapofat{\selindex}{\selvariable} \wcols \lambda\in\{-1,+1\}\ncond \selindexin\}$ and
\begin{align*}
    \maxgapofat{\canmetricwrt{\Gamma}}{\canparam}
    = \frac{1}{2} \max_{\selindex\notin\argmax_{\selindex}\canparamat{\indexedselvariable}} \left| \canparamat{\indexedselvariable} - \max_{\selindex}\canparamat{\indexedselvariable} \right| \, .
\end{align*}





\sect{Probabilistic Guarantees for \HybridLogicNetworks{}}

%\red{In case of logical formulas being statistics, the coordiantes of the mean parameter are satisfaction rates to the formulas.}

For \HybridLogicNetworks{} we have statistics consistent of boolean statistics $\enumformula$, which can be interpreted as formulas in propositional logics.
In this case the marginal distributions of the coordinates of $\sstatnoise$ are scaled and centered binomials, which we show now.

\begin{theorem}
    \label{the:noiseTensorBinomial}
    For any $\hlnstat$ the marginal distribution of the coordinate $\mlnnoiseat{\indexedselvariable}$ is the scaled and centered binomial distribution
    \begin{align*}
        \frac{1}{\datanum}\left(\bidistof{\datanum,\meanparamat{\indexedselvariable}}- \meanparamat{\indexedselvariable}\right)
    \end{align*}
    with parameters $\datanum$ and $\meanparamat{\indexedselvariable}$.
\end{theorem}
\begin{proof}
    We notice that when forwarding a random sample $\datapoint$ of $\gendistribution$ is the random tensor
    \[ \onehotmapofat{\datapoint}{\shortcatvariables} \, \]
    and since $\imageof{\sstatcoordinate}\subset \{0,1\}$ the contraction
    \[ \contraction{\sstatcoordinate, \onehotmapofat{\datapoint}{\shortcatvariables}} \]
    is a random variable taking values in $\{0,1\}$.
    The variable therefore follows a Bernoulli distribution with mean parameter
    \begin{align*}
        \meanparamat{\indexedselvariable}
        = \expectationof{\contraction{\sstatcoordinate, \onehotmapofat{\datapoint}{\shortcatvariables}}}
        = \contraction{\sstatcoordinate, \gendistribution}  \, & \qedhere
    \end{align*}
\end{proof}

The mean parameter of the M-projection of the empirical distribution on the family of \MarkovLogicNetworks{} with statistic $\fselectionmap$ is the random tensor
\begin{align*}
    \datameanat{\selvariable}
    = \contractionof{\sencmlnstat,\empdistribution}{\selvariable} \, .
\end{align*}

The expectation of this random tensor is
\begin{align*}
    \expectationof{\datamean}
    =  \contractionof{\sencmlnstat,\expectationof{\empdistribution}}{\selvariable}
    =  \contractionof{\sencmlnstat,\gendistribution}{\selvariable}
    =  \genmean \, ,
\end{align*}
where we used that the expectation and contraction operation can be commuted due to the multilinearity of contractions.

\subsect{Energy Tensor of Proposal Distributions}

The fluctuation tensor appears as a fluctuation of the energy of the proposal distribution.
The expectation of the energy of the proposal distribution is
\begin{align*}
    \expectationof{\energytensorof{\proposalstat,\empdistribution-\currentdistribution}}
    &= \expectationof{\contractionof{\sencproposalstat,\empdistribution-\currentdistribution}{\selvariable}}
    = \contractionof{\sencproposalstat,\expectationof{\empdistribution-\currentdistribution}}{\selvariable}
    = \contractionof{\sencproposalstat,\gendistribution-\currentdistribution}{\selvariable}\\
    &= \expectationof{\energytensorof{\proposalstat,\gendistribution-\currentdistribution}} \, .
\end{align*}

% Fluctuation
The fluctuation of this random tensor is
\begin{align*}
    \expectationof{\energytensorof{\proposalstat,\empdistribution-\currentdistribution}}  - \expectationof{\energytensorof{\proposalstat,\gendistribution-\currentdistribution}}
    = \expectationof{\energytensorof{\proposalstat,\empdistribution-\gendistribution}}
\end{align*}
and coincides with $\mlnnoise$.

\subsect{Universal Exponential Family} % Interesting, since here is the connection with probability tensors: Forwarding of each random datapoint by the one hot encoding to get a multinomial random tensor.

In case of the universal exponential family, we have $\sstat=\identityat{\shortcatvariables,\selvariable}$ and the noise tensor is
\begin{align*}
    \mintermnoise = \empdistribution - \gendistribution \, .
\end{align*}

% Multinomial as a more detailed characterization
This noise tensor follows a multinomial distribution as we show next.
To this end, we notice that a multinomial distribution can be defined as the average of one-hot encodings of independently and identically distributed datapoints.
When drawing $\data$ independently from $\gendistribution$ we denote
\begin{align*}
    \sum_{\datindexin}\onehotmapofat{\datapointof{\datindex}}{\shortcatvariables}
    \distassymbol \multidistof{\datanum,\gendistribution} \, .
\end{align*}

\begin{theorem}
    \label{the:multinomialEmpdistFluctuation}
    The noise tensor $\mintermnoise$ is a by $\frac{1}{\datanum}$ rescaled centered multinomial random tensor with parameters $\gendistribution$ and $\datanum$, that is
    \begin{align*}
        \mintermnoise \distassymbol \frac{1}{\datanum} \left( \multidistof{\datanum,\gendistribution} - \gendistribution \right) \, .
    \end{align*}
\end{theorem}
\begin{proof}
    By the above construction we have
    \begin{align*}
        \empdistribution - \gendistribution
        = \dataaverage \left( \onehotmapofat{\datapoint}{\shortcatvariables} - \expectationof{\onehotmapofat{\datapoint}{\shortcatvariables}} \right)
    \end{align*}
    We further have
    \begin{align*}
        \expectationof{\onehotmapofat{\datapoint}{\shortcatvariables}} = \gendistributionat{\shortcatvariables} \, .
    \end{align*}
\end{proof}

The noise tensor characterization by multinomial distributions, which holds for universal statistics, is a more detailed characterization compared to the characterization of its marginals by binomial distribution in \theref{the:noiseTensorBinomial}, which holds for generic statistics $\hlnstat$.

\subsect{Guarantees for the Mode of the Proposal Distribution}

Let us now derive probabilistic guarantees, that the mode of the proposal distribution at the empirical and the generating distribution are equal.

\begin{theorem}
    \label{the:probGuaranteeProposalDist}
    Whenever the energy tensor of the expected proposal distribution has a gap of $\maxgap$, then for every $\failprob>0$ any mode of the empirical proposal distribution coincides is also a mode of the expected proposal distribution with probability at least $1-\expof{-\frac{1}{\failprob^2}}$, provided that
    \begin{align*}
        \datanum > C\frac{(1+\lnof{\seldim})}{\maxgap^2}
    \end{align*}
    where $C$ is a universal constant.
\end{theorem}
\begin{proof}
    To prove the theorem we combine the deterministic guarantee \theref{the:detGuaranteeProposalDist} with the width bound of \theref{the:basisTensorWidthBound}, which we show in the next section.
    Given the assumed bound, the sub-gaussian norm of the width is upper bounded by $C_2\cdot \maxgap$, thus for any $\failprob>0$ we have
    \begin{align*}
        \widthwrtof{\selbasislong}{\mlnnoise}  < 2 \maxgap
    \end{align*}
    with probability at least $1-\expof{-\frac{1}{\failprob^2}}$.
    The claim thus follows with \theref{the:detGuaranteeProposalDist}.
\end{proof}


\begin{example}[Gap of a MLNs with single formulas]
    Let there be the MLN of a maxterm $\formula$ with $\atomorder$ variables, and let $\formulaset$ be the maxterm selecting tensor, then
    \[ \maxgapof{
        \energytensorof{(\formulaset, \expdistof{(\{\formula\},\singlecanparam)} - \normalizationof{\ones}{\shortcatvariables} )}
    } = \frac{1}{2^{\atomorder}-1 + \expof{-\singlecanparam}}  \]
    If $\singlecanparam>0$ we have an exponentially small gap.
    Thus, for the above Lemma to apply, the width needs to be exponentially in $\atomorder$ small.


    Let there be the MLN of a minterm $\formula$ with $\atomorder$ variables, then
    \[ \maxgap(
    \energytensorof{(\formulaset, \expdistof{(\{\formula\},\singlecanparam)} - \normalizationof{\ones}{\shortcatvariables} )}
    ) = \frac{1}{1+(2^{\atomorder}-1)\cdot\expof{-\singlecanparam}}  \]
    For large $\singlecanparam$ and $\atomorder$, the gap tends to $1$.
\end{example}

\subsect{Guarantees for Unconstrained Parameter Estimation}

%
We here use the sphere bounds and combine with \theref{the:detGuaranteeUnconstrained} to derive a probabilistic guarantee for unconstrained parameter estimation.

\begin{theorem}
    For any $\failprob\in(0,1)$ we have the following with probability at least $1-\failprob$.
    Let $\datacanparam$ be the estimated canonical parameter, then we have for any $\precision>0$
    \[ \absof{\centropyof{\gendistribution}{\mlnexpdistof{\datacanparam}} - \centropyof{\empdistribution}{\mlnexpdistof{\datacanparam}}} \leq \tau \cdot \normof{\datacanparam} \]
    provided that
    \[ \datanum \geq \frac{\contraction{\genmean}-\contraction{(\genmean)^2}}{\failprob \precision^2} \, . \]
\end{theorem}
\begin{proof}
    The claim follows from the deterministic guarantee \theref{the:detGuaranteeUnconstrained} with the probabilistic width bound \theref{the:sphereBoundVariance} to be shown in the next section.
\end{proof}

\subsect{Face Recovery in \HybridLogicNetworks{}}

\begin{theorem}
    Let $\hlnstat$ be a statistic such that $\meansetof{\hlnstat,\trivbm}$ is a cube-like polytope (see \defref{def:elementaryRealizableMeanParams}).
    Let $\dataset$ be drawn from a distribution $\probof{\hlnstat,\genhybridparam}$ and let
    \begin{align*}
        \maxgap^* \coloneqq \min
        \left\{\genmeanat{\indexedselvariable} \wcols \selindex\notin\hardlegset^* \right\} \cup   \left\{(1-\genmeanat{\indexedselvariable}) \wcols \selindex\notin\hardlegset^* \right\} \, .
    \end{align*}
    For any $\failprob\in(0,1)$ we have the following with probability at least $1-\failprob$ that
    \begin{align*}
        \hardlegset^{\datamean} = \hardlegset^* \, ,
    \end{align*}
    provided that
    \begin{align*}
        \datanum \geq C \frac{(1+\lnof{\seldim})^2 \cdot (1-\lnof{1-\failprob}^2}{(\maxgap^*)^2} \, ,
    \end{align*}
    where $C$ is a universal constant.
\end{theorem}
\begin{proof}
    We notice that there is a $\ell_\infty$ ball of radius $\maxgap^*$ around $\genmean$ contained in the face, in which interior $\genmean$ lies.
    It therefore suffices to show, that within the claimed probability the width of the noise tensor with respect to the basis vectors is smaller than $\maxgap^*$.
    To this end, we apply the sub-Gaussian norm bound of \theref{the:basisTensorWidthBound}, which states that
    \begin{align*}
        \sgnormof{\widthwrtof{\selbasislong}{\mlnnoise}} \leq C_1 \frac{\sqrt{1+\lnof{\seldim}}}{\sqrt{\datanum}} \, .
    \end{align*}
    Thus, we have for any $t>0$ (see \lemref{lem:subGaussianTailBound})
    \begin{align*}
        \probat{\widthwrtof{\selbasislong}{\mlnnoise} \geq C_1 \frac{\sqrt{1+\lnof{\seldim}}}{\sqrt{\datanum}} \cdot t} \leq \expof{1-t} \, .
    \end{align*}
    We choose $t=1-\lnof{1-\failprob}$ and notice, that the claimed bound holds with the universal constant $C=(C_1)^2$, since in this case
    \begin{align*}
        C_1 \frac{\sqrt{1+\lnof{\seldim}}}{\sqrt{\datanum}} \cdot (1-\lnof{1-\failprob}) \leq \maxgap^* \, . & \qedhere
    \end{align*}
\end{proof}

\sect{Width Bounds for \HybridLogicNetworks{}}\label{sec:widthBounds}

We here provide width bounds on the noise tensors $\mlnnoise$ to \HybridLogicNetworks{}, which coordinates have marginal distributions by Binomials, as shown in \theref{the:noiseTensorBinomial}.
All bounds hold for arbitrary statistics $\hlnstat$ of propositional formulas and number $\datanum$ of data and the appearing constants are universal, that is independent of particular choices of $\hlnstat$ and $\datanum$.

\subsect{Basis Vectors}

We first introduce the sub-Gaussian Norm and show how we can exploit it to state concentration inequalities.

\begin{definition}[Sub-Gaussian Norm, see Def.~2.5.6 in \cite{vershynin_high-dimensional_2018}]
    The sub-Gaussian norm of a random variable $X$ is defined as
    \begin{align*}
        \sgnormof{X} = \inf \left\{ C > 0 \wcols \expectationof{\expof{\frac{X^2}{C^2}}} \leq 2 \right\} \, .
    \end{align*}
\end{definition}

The moment bound used to define the sub-Gaussian norm can then be combined with the Markov inequality to state concentration bounds.

\begin{lemma}
    \label{lem:subGaussianTailBound}
    For a random variable $X$ with finite sub-Gaussian norm $\sgnormof{X}$ we have for any $t>0$
    \begin{align*}
        \probat{\absof{X} > \sgnormof{X} \cdot t} \leq \expof{1-t^2} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    By the Markov inequality we have for any $t>0$ and $C>\sgnormof{X}$
    \begin{align*}
        \probat{\absof{X} > C \cdot t}
        & = \probat{\expof{\frac{X^2}{C^2}} > \expof{t^2}} \\
        & \leq \frac{\expectationof{\expof{\frac{X^2}{C^2}}}}{\expof{t^2}} \leq 2\cdot\expof{-t^2} = \expof{1-t^2} \, .
    \end{align*}
    The claim follows by taking the limit $C\searrow\sgnormof{X}$.
\end{proof}

% CONFUSING: \hypercore would be the vector of values, \prob the probabilities to a given variable X.
%% Relate with the contraction formalism, but not needed?
%Before showing the utility of these norm, let us first connect with the contraction formalism of this work.
%When $X$ is a random coordinate of $\hypercoreat{\shortcatvariables}$, selected by a probability tensor $\probat{\shortcatvariables}$ we have
%\begin{align*}
%    \expectationof{\expof{\frac{X^2}{C^2}}}
%    = \contraction{\probat{\shortcatvariables},\expof{\frac{1}{C^2}\cdot\hypercoreat{\shortcatvariables}}}
%\end{align*}
%and thus
%\begin{align*}
%    \sgnormof{X} = \inf \left\{ C > 0 \wcols \contraction{\probat{\shortcatvariables},\expof{\frac{1}{C^2}\cdot\hypercoreat{\shortcatvariables}}} \leq 2 \right\} \, .
%\end{align*}

We now show a sub-Gaussian norm bound on the coordinates of the noise tensor in case of \HybridLogicNetworks{}.

\begin{lemma}
    \label{lem:mlnMeanSubGaussianCoordinates}
    The marginal distribution of any coordinate of $\mlnnoiseat{\selvariable}$ is sub-Gaussian with
    \begin{align*}
        \sgnormof{\mlnnoiseat{\indexedselvariable}} \leq C_0 \frac{1}{\sqrt{\datanum}} \,
    \end{align*}
    where $C_0>0$ is a universal constant .
\end{lemma}
\begin{proof}
    Any centered Bernoulli variable is bounded and therefore sub-Gaussian with
    \begin{align*}
        \sgnormof{\contraction{\enumformulaat{\shortcatvariables},\onehotmapofat{\datapoint}{\shortcatvariables}}-\contraction{\enumformulaat{\shortcatvariables},\gendistribution}}
        \leq \frac{1}{\sqrt{\lnof{2}}} \, .
    \end{align*}
    Binomial variables are sums of independent Bernoulli variables.
    We apply the sub-Gaussian norm bound for sums from Proposition~2.6.1 in \cite{vershynin_high-dimensional_2018}, which states that for a universal constant $C>0$ we have
    \begin{align*}
        \sgnormof{\contraction{\enumformulaat{\shortcatvariables},\left(\sum_{\datindexin}\onehotmapofat{\datapoint}{\shortcatvariables} - \gendistribution\right)}}
        \leq \frac{C\cdot\sqrt{\datanum}}{\sqrt{\lnof{2}}} \, .
    \end{align*}
    We therefore have
    \begin{align*}
        \sgnormof{\mlnnoiseat{\indexedselvariable}} =
        \frac{1}{m}\sgnormof{\contraction{\enumformulaat{\shortcatvariables},\left(\sum_{\datindexin}\onehotmapofat{\datapoint}{\shortcatvariables} - \gendistribution\right)}}
        \leq \frac{C}{\sqrt{\lnof{2} \cdot \datanum}} \, .
    \end{align*}
    We arrive at the claimed bound with a transform of the universal constant to $C_0=\frac{C}{\sqrt{\lnof{2}}}$.
\end{proof}

Based on this norm bound, we now show a bound on the sub-Gaussian norm of the width with respect to basis vectors.

\begin{theorem}
    \label{the:basisTensorWidthBound}
    For the set of basis vectors
    \begin{align*}
        \selbasisshort = \selbasislong
    \end{align*}
    we have
    \begin{align*}
        \sgnormof{\widthatof{\selbasisshort}{\mlnnoise}} \leq C_1 \sqrt{\frac{1+\lnof{\seldim}}{\datanum}} \, ,
    \end{align*}
    where $C_1>0$ is a universal constant.
\end{theorem}
\begin{proof}
    We first notice, that
    \begin{align*}
        \widthatof{\selbasisshort}{\noisetensor} = \max_{\selindexin} \absof{\mlnnoiseat{\indexedselvariable}}
    \end{align*}
    By a generic bound on the supremum of sub-Gaussian variables (see Exercise~2.5.10 in \cite{vershynin_high-dimensional_2018}) we have for a universal constant $C>0$
    \begin{align*}
        \sgnormof{\max_{\selindexin}\absof{\mlnnoiseat{\indexedselvariable}}}
        \leq C \left(\max_{\selindexin}\sgnormof{\mlnnoiseat{\indexedselvariable}}\right) \sqrt{1 + \lnof{\seldim}} \, .
    \end{align*}
    We now apply \lemref{lem:mlnMeanSubGaussianCoordinates} and get with $C_1=C\cdot C_0$ that
    \begin{align*}
        \sgnormof{\widthatof{\selbasisshort}{\mlnnoise}} \leq C_1 \sqrt{\frac{1+\lnof{\seldim}}{\datanum}}  \, . & \qedhere
    \end{align*}
\end{proof}

% Sharpness comment
The bound in \theref{the:basisTensorWidthBound} is furthermore sharp, see the construction of an identically scaling lower bound in Exercise~2.5.11 in \cite{vershynin_high-dimensional_2018}.
Note that the binomials used here tend to normal distributed variables used in the construction therein.

\subsect{Sphere}

For any tensor $\noiseat{\selvariable}$ and the sphere $\subsphere\subset\parspace$ we have
\begin{align*}
    \widthwrtof{\subsphere}{\noiseat{\selvariable}}
    = \normof{\noiseat{\selvariable}} \, .
\end{align*}
To show probabilistic width bounds with respect to the sphere, we therefore apply in the following Chebyshevs inequality on the norm of random tensors.

\begin{theorem}
    \label{the:sphereBoundVariance}
    Let $\meanparamat{\selvariable}$ be a deterministic vector with coordinates in $[0,1]$ and $\noiseat{\selvariable}$ a random vector, which coordiantes are for $\selindexin$ marginally distributed as
    \begin{align*}
        \noiseat{\indexedselvariable} \distassymbol \bidistof{\datanum,\meanparamat{\indexedselvariable}} \, .
    \end{align*}
    Then we have for any $\failprob>0$, $\precision>0$ and $\datanum\in\nn$ with probability at least $1-\failprob$
    \begin{align*}
        \normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} \leq \precision
    \end{align*}
    provided that
    \begin{align*}
        \datanum \geq \frac{\contraction{\meanparamat{\selvariable},(\onesat{\selvariable}-\meanparamat{\selvariable})}}{\failprob\cdot\precision^2} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    Since the squared norm of the noise is the sum of squared centered and averaged Binomials, we have
    \begin{align*}
        \expectationof{\normof{\noiseat{\selvariable}-\expectationof{\noiseat{\selvariable}}}^2}
        = \datanum \cdot \left(\sum_{\selindexin} \meanparamat{\indexedselvariable}(1-\meanparamat{\indexedselvariable})\right)
    \end{align*}
    Here we used that the variance of a variable distributed by $\bidistof{\datanum,\meanparamat{\indexedselvariable}}$ is $\datanum\cdot\meanparamat{\indexedselvariable}(1-\meanparamat{\indexedselvariable})$.

    If follows, that
    \[ \expectationof{\left(\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}}\right)^2}
    = \frac{ \contraction{\meanparamat{\selvariable},(\onesat{\selvariable}-\meanparamat{\selvariable})}}{\datanum} \, . \]

    Then we apply a Chebyshev Bound to get for any $\precision>0$
    \begin{align}
        \probat{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision}
        = \probat{\left(\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}}\right)^2 > \precision^2}
        \leq \frac{ \contraction{\meanparamat{\selvariable},(\onesat{\selvariable}-\meanparamat{\selvariable})}}{\datanum \cdot \precision^2}
    \end{align}
    For a $\failprob>0$ we choose any $\datanum$ with
    \[ \datanum \geq  \frac{ \contraction{\meanparamat{\selvariable},(\onesat{\selvariable}-\meanparamat{\selvariable})}}{\precision^2 \failprob} \, \]
    and get
    \begin{align}
        \probat{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision} \leq \failprob \, .
    \end{align}
    Thus, we have
    \begin{align}
        \probat{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} \leq \precision}
        = 1 - \probat{\normof{\frac{\noisetensor-\expectationof{\noisetensor}}{\datanum}} > \precision}  \geq 1-\failprob \, . & \qedhere
    \end{align}
\end{proof}


% Multinomial
For the universal exponential family where $\hlnstat = \naivestat$ the noise tensor is a rescaled and centered multinomial.
In that case, the bound of \theref{the:sphereBoundVariance} can be simplified by
\begin{align*}
    \contraction{\meanparamat{\selvariable},(\onesat{\selvariable}-\meanparamat{\selvariable})}
    = 1- \contraction{\meanparamat{\selvariable}^2} \,.
\end{align*}


\sect{Discussion}

We in this chapter only provided probabilistic width bounds for \HybridLogicNetworks{}, which are characterized by boolean statistics.
Similar recovery bounds for parameter estimation and structure learning for more general exponential families would require width bounds in these generic cases.
A general approach towards width bounds are chaining techniques on stochastic processes, see \cite{talagrand_upper_2014}.
While we showed bounds based on the sub-Gaussian norm, more general sub-exponential bounds could be used, see \cite{wainwright_high-dimensional_2019}.

We further assumed that our random tensors to be projected are empirical distributions.
More general random tensor networks and corresponding width bounds have been developed in \cite{goessmann_uniform_2021}.
% Refer to FOL Models, extraction query stuff?