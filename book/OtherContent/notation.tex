\chapter{Notation and Basic Concepts}\label{cha:TensorNetworks}\label{cha:notation}

We here provide the fundamental definitions of tensors, which are essential for the content in \parref{par:one} and \parref{par:two}.
In \parref{par:three} we will further investigate the properties of tensors focusing on their contractions.

\sect{\bncategoricals}

We will in this work investigate systems, which are described by a set of properties, each called categorical variables.
This is called an ontological commitment, since it defines what properties a system has.

\begin{definition}
	An atomic representation of a system is described by a categorical variables $\catvariable$ taking values $\catindex$ in a finite set
		\[  [\catdim]\coloneqq \{0,\ldots, \catdim-1\} \]
	of cardinality $\catdim$.
\end{definition}

% Notation: Large and small literals
We will in this work always notate categorical variables by large literals and indices by small literals, possible with other letters such as $\catvariable,\selvariable,\indvariable,\datvariable$ and corresponding values $\catindex,\selindex,\indindex,\datindex$.

\begin{definition}
	A factored representation of a system is a set of categorical variables $\catvariableof{\atomenumerator}$, where $\atomenumeratorin$, taking values in $[\catdimof{\atomenumerator}]$.
\end{definition}

\sect{\bntensors}

% Gentle introduction sentences
Tensors are multiway arrays and a generalization of vectors and matrices to higher orders.
We will first provide a formal definition as real maps from index sets enumerating the coordinates of vectors, matrices and larger order tensors.

\begin{definition}[Tensor]\label{def:tensor}
	Let there be numbers $\catdimof{\atomenumerator}\in\nn$ for $\atomenumeratorin$ and categorical variables $\catvariableof{\atomenumerator}$ taking their values in $[\catdimof{\atomenumerator}]$.
	A Tensors $\hypercoreat{\catvariables}$ of order $\catorder$ and leg dimensions $\catdimof{\atomorder}$ is defined through its coordinates
	\begin{align*}
		\hypercoreat{\indexedcatvariableof{0},\ldots,\indexedcatvariableof{\catorder-1}} \in \rr
	\end{align*}
	for index tuples
	\begin{align*}
		\catindexof{0},\ldots,\catindexof{\catorder-1} \in \facstates \, .
	\end{align*}
	Tensors $\hypercoreat{\catvariables}$ are elements of the tensor space
	\begin{align*}
		\bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,
	\end{align*}
	which is a linear space, enriched with the operations of coordinatewise summation and scalar multiplication.
%	We call maps
%	\begin{align*}
%		\hypercoreat{\catvariables}\defcols\bigtimes_{\atomenumeratorin} [\catdimof{\atomenumerator}] \rightarrow \rr
%	\end{align*}
%	tensor of order $\atomorder$ and leg dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
%	Evaluations of these maps at indices $\catindices$ are denoted by
%	\begin{align*}
%		\hypercoreat{\indexedcatvariables} = \hypercoreat{\catvariables}(\catindices)\, .
%	\end{align*}
%%	with coordinates denoted by $\hypercore_{\catindices}$ is called a tensor of order $\atomorder$ and legs with the dimensions $\catdimof{0},\ldots,\catdimof{\atomorder-1}$.
%	Tensors $\hypercoreat{\catvariables}$ are elements of the space
%	\begin{align*}
%		\bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,
%	\end{align*}
%	which is, with the operations of coordinatewise summation and scalar multiplication, a linear space called a tensor space.
\end{definition}

% Non-canonical 
We here introduced tensors in a non-canonical way based on categorical variables assigned to its axis.
While coming as syntactic sugar at this point, this will allow us to define contractions without further specification of axes, based on comparisons of shared categorical variables.
Especially, this eases the implementation of tensor network contractions without the need to further specify a graph (see Appendix~\ref{cha:implementation}).

% Further abbreviations
We abbreviate lists $\catvariables$ of categorical variables by $\shortcatvariables$, that is denote $\hypercoreat{\catvariables}$ by $\hypercoreat{\shortcatvariables}$.
Occasionally, when the categorical variables of a tensor are clear from the context, we will omit the notation of the variables. %further abbreviate $\hypercoreat{\catvariables}$ by $\hypercore$.

\begin{example}[Trivial Tensor]\label{exa:trivialTensor}
	The trivial tensor
	\begin{align*}
		 \onesat{\shortcatvariables} \in \facspace
	\end{align*}
	is defined by all coordinates being $1$, that is for all $\catindices\in\facstates$
	\begin{align*}
		\onesat{\indexedshortcatvariables} = 1 \, .
	\end{align*}
\end{example}

%\sect{Properties of Tensors}

%% Boolean
We will often encounter situations, where the coordinates of tensors are in $\ozset=[2]$.

\begin{definition}\label{def:booleanTensor}
	We call a tensor $\hypercoreat{\shortcatvariables}$ boolean, when $\imageof{\hypercore}\subset[2]$, i.e. all coordinates are either $0$ or $1$.
\end{definition}

%\sect{One-hot encodings}

We are now ready to provide the link between tensors and states of systems with factored representations.
To this end, we define the one-hot encoding of a state, which is a bijection between the states and the basis elements of a tensor space.

\begin{definition}[One-hot encodings to Atomic Representations]
	Given an atomic system described by the categorical variable $\catvariable$, we define for each $\catindex\in[\catdim]$ the basis vector $\onehotmapofat{\catindex}{\catvariable}$ by the coordinates
	\begin{align}
		\onehotmapofat{\catindex}{\catvariable=\tilde{\catindex}} = \begin{cases}
			1 & \ifspace \catindex=\tilde{\catindex} \\
			0 & \text{else} \, .
		\end{cases}
	\end{align}
	The one-hot encoding of states $\catindex\in[\catdim]$ of the atomic system described by the categorical variable $\catvariable$ is the map
		\[ \onehotmap: [\catdim] \rightarrow \rr^\catdim \]
	which maps $\catindex \in [\catdim]$ to the basis vectors $\onehotmapofat{\catindex}{\catvariable}$.
\end{definition}

% Coordinatewise representation
The basis vectors $\onehotmapofat{\catindex}{\catvariable}$ are tensors of order $1$ and leg dimension $\catdim$ of the structure
\begin{align}
	\onehotmapofat{\catindex}{\catvariable} = \begin{bmatrix}
	0 & \cdots & 0 & 1 & 0 & \cdots & 0
	\end{bmatrix}^T \, ,
\end{align}
where the $1$ is at the $\catindex$th coordinate of the vector.

% Atomic -> Factored system
We have so far described one-hot representations of the states of a single categorical variable, which would suffice to encode the state of an atomic system.
In a factored system on the other hand, we are dealing with multiple categorical variables.

\begin{definition}[One-hot encodings to Factored Representations]\label{def:oneHotEncoding}
	Let there be a factored system defined by a tuple $(\catvariables)$ of variables taking values in $\facstates$.
	The one-hot encoding of its states is the tensor product of the one-hot encoding to each categorical variables, that is the map
		\[ \onehotmap\defcols\facstates \rightarrow  \facspace \]
	defined by mapping $\catindices=\shortcatindices$ to
	\begin{align*}
		 \onehotmapofat{\shortcatindices}{\shortcatvariables}
		=: \bigotimes_{\atomenumeratorin} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, .
	\end{align*}
	We will call one-hot representations \emph{tensor representations} and depict them as
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_tensorproduct.tex}
	\end{center}
\end{definition}

In \charef{cha:coordinateCalculus} we will investigate the image of $\onehotmap$ in more detail and show that it is an orthonormal basis of the tensor space $\facspace$.

\begin{remark}[Flattening of Tensors]
	The use of the tensor product to represent states of factored systems can be motivated by the reduction to atomic systems by enumeration of the states.
	We have this property reflected in the state encoding of factored systems, since the tensor space $\facspace$ is isomorphic to the vector spaces $\rr^{\prod_{\atomenumeratorin}\catdimof{\atomenumerator}}$.
	This operation is called flattening (or unfolding) of tensors with many axes to tensors of less axes.
\end{remark}

\sect{\bncontractions}

Contractions are the central manipulation operation on sets of tensors.
To introduce them, we will develop a graphical illustration of sets of tensors, which we also call tensor networks.
In \parref{par:three} we will further investigate the utility of contractions in representing specific calculations, which demand different encoding schemes.


\subsect{Graphical Illustrations}

% Hypergraph as capturing the categorical variable assignment to tensors
Sets of tensor with categorical variables assigned to each legs implicitly carry a notion of a hypergraph.
This perspective is especially useful, when some categorical variables are assigned to axis of multiple tensors, as it will often be the case in the applications considered in this work.
Each variable can then be labeled by a node and each tensor as a hyperedge containing the nodes to its axis variables.
Let us first formally introduce hypergraphs, which are generalizations of graphs allowing edges to be arbitrary nonempty subsets of the nodes, whereas canonical graphs demand a cardinality of two.

\begin{definition}\label{def:hypergraphs}
	A hypergraph is a pair $\graph=(\nodes,\edges)$ of a set of nodes $\nodes$ and a set of edges $\edges$, where each hyperedge $\edge\in\edges$ is a subset of the nodes $\nodes$.
	A directed hypergraph is a pair $\graph=(\nodes,\edges)$, such that each hyperedge $\edge\in\edges$ is the tuple of two disjoint sets $\incomingnodes,\outgoingnodes\subset\nodes$, that is
		\[ \edge = (\incomingnodes,\outgoingnodes)  \, . \]
\end{definition}

% Diagrammatic representation in factor graphs
We will use the standard visualization by factor graphs as a diagrammatic illustration of sets of tensors, where tensors are represented by block nodes and each axis assigned with by a categorical variable $\catvariableof{\atomenumerator}$ represented by a node, see \figref{fig:tensors}a).
%We further denote on Each axis of the tensor is represented by a node representing the variable $\catvariableof{\atomenumerator}$ and the tensor $\hypercore$ is associated with the hyperedge $\edge$ connecting all variables.
 %representing the choice of an element in the set $[\catdimof{\atomenumerator}]$
% Hyperedge view
Different simplifications of these factor graph depictions have been evolved in different research fields.
In the tradition of graphical models, which started with the work \cite{pearl_probabilistic_1988}, the categorical variables are highlighted and the tensor blocks just depicted by hyperedges.
To depict dependencies with causal interpretations, the edges are further decorated by directions in the depiction of Bayesian networks, see for example \cite{pearl_causality_2009}.

In the tensor network community on the other hand, a simplification scheme highlighting the tensors as blocks and omitting the depiction of categorical variables has been evolved.
The variables, or sometimes their index or dimension, are then directly assigned to the lines depicting the axes of the tensor blocks.
This depiction scheme has been established in the literature as wiring diagrams (see \cite{landsberg_tensors_2011} and dates back at least to the work \cite{penrose_spinors_1987}.

Both depiction schemes are simplifications of factor graphs, by highlighting the categorical variables in the depiction in \figref{fig:tensors}b) and the tensors in the depiction in \figref{fig:tensors}c).
In this work, we will prefer the simplification of the tensor network community, depicted in \figref{fig:tensors}b).

% Duality
In another interpretation (see \cite{robeva_duality_2019}), both simplification schemes are different hypergraphs, which are dual to each other.

\begin{figure}[t]
	\begin{center}
		\input{OtherContent/tikz_pics/notation/hypercore.tex}
	\end{center}
	\caption{Depiction of Tensors
	a) As a factor in a factor graph, depicted by a block, and connected to categorical variables assigned to nodes.
	b) Highlighting only the variable dependencies by a hyperedge connecting the variables $\catvariableof{\atomenumerator}$ to each axis $\atomenumeratorin$.
	c) Highlighting the tensor by a blockwise notation with axes denoted by open legs represented by the variables $\catvariableof{\atomenumerator}$.
	}\label{fig:tensors}
\end{figure}


% Diagramatic representation of vectors
To depict vector calculus and its generalizations, we will apply the graphical notation (mainly version b) introduced in \charef{cha:TensorNetworks}.
Along this line, we represent vectors and their generalization to tensors by blocks with legs representing its indices.
The basis vectors being one-hot encodings of states are in this scheme represented by
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_atomic.tex}
	\end{center}
where $\tilde{\catindex}$ is an indexed represented by an open leg.
Assigning $\catindex$ to this index will retrieve the $\catindex$th coordinate (with value $1$), whereas all other assignments will retrieve the coordinate values $0$.


Drawing on the interpretation of tensors by hyperedges we can continue with the definition of tensor networks.

\begin{definition}\label{def:tensorNetwork}
	Let $\graph=(\nodes,\edges)$ be a hypergraph with nodes decorated by categorical variables $\catvariableof{\node}$ with dimensions
		\[ \catdimof{\node} \in \nn \]
	and hyperedges $\edge\in\edges$ decorated by core tensors
		\[ \hypercoreofat{\edge}{\catvariableof{\edge}} \in \bigotimes_{\node\in\edge}\rr^{\catdimof{\node}} \, , \]
	where we denote by $\catvariableof{\edge}$ the set of categorical variables $\catvariableof{\node}$ with $\node\in\edge$.
	Then we call the set
		\[ \tnetofat{\graph}{\catvariableof{\nodes}} = \{\hypercoreofat{\edge}{\catvariableof{\edge}}  \wcols \edge\in\edges\} \]
	the Tensor Network of the decorated hypergraph $\graph$.
	The set of tensor networks on $\graph$, such that all tensors have non-negative coordinates, is denoted by $\tnsetof{\graph}$.
\end{definition}


\begin{figure}
	\begin{center}
		\input{OtherContent/tikz_pics/notation/network.tex}
	\end{center}
	\caption{
	Example of a tensor network on a
	a) hypergraph with edges $\edge_0=\{\catvariableof{0},\catvariableof{1},\catvariableof{2}\}$, $\edge_1=\{\catvariableof{1},\catvariableof{2}\}$ and $\edge_2=\{\catvariableof{2},\catvariableof{3}\}$,
	which is decorated by the tensor cores b), representing a contraction with leaving all variables open.
	}\label{fig:network}
\end{figure}

%%
%Diagrammatic notation: Best to do version a) as used in the definition, highlighting that tensors have shared categorical variables with fixed dimensions.




\subsect{Tensor Product}

% Diagrams -> Contractions
Let us now exploit the developed graphical representations to define contractions of tensor networks.
The simplest contraction is the tensor product, which maps a pair of two tensors with distinct variables onto a third tensor and has an interpretation by coordinatewise products.
Such a contraction corresponds with a tensor network of two tensors with disjoint variables, depicted as:
\begin{center}
	\input{OtherContent/tikz_pics/notation/tensor_product.tex}
\end{center}

\begin{definition}[Tensor Product]\label{def:tensorProduct}
	Let there be two tensor
	\begin{align*}
		\hypercoreofat{\edge_0}{\shortcatvariables} \in \facspace  \andspace \hypercoreofat{\edge_1}{\secshortcatvariables} \in \secfacspace
	\end{align*}
	with different categorical variables assigned to its axes.
	Then there tensor product is the map
	\begin{align*}
		\contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\hypercoreofat{\edge_1}{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}
		\in \left(\facspace\right) \otimes \left(\secfacspace \right)
		%:  \left(\facstates\right) \times \left(\secfacstates\right) \rightarrow \rr
	\end{align*}
	defined coordinatewise for tuples of $\catindices\in\facstates$ and $\seccatindices\in\secfacstates$ as
	\begin{align*}
		& \contractionof{\hypercore,\sechypercore}{\indexedcatvariables,\indexedseccatvariables} \\
		&\quad\quad :=  \hypercoreofat{\edge_0}{\indexedcatvariables}\cdot \hypercoreofat{\edge_1}{\indexedseccatvariables} \, .
	\end{align*}
\end{definition}

% Other notations
Other popular standard notations of tensor products (see \cite{kolda_tensor_2009,hackbusch_tensor_2012,cichocki_tensor_2015})
	\[ \left(\hypercore \otimes \sechypercore\right) = \left(\hypercore \circ \sechypercore\right)
	= \contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\hypercoreofat{\edge_1}{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}  \, . \]
We will avoid these notations in this work in favor of a consistent notation capable of depicting generic tensor network contractions.

When the tensor $\hypercoreofat{\edge_1}{\secshortcatvariables}$ coincides with the trivial tensor $\onesat{\secshortcatvariables}$ (see Example~\ref{exa:trivialTensor}), we further make a notation convention to omit that tensor, that is
\begin{align*}
	\contractionof{\hypercoreofat{\edge_0}{\shortcatvariables},\onesat{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}
	= \contractionof{\hypercoreofat{\edge_0}{\shortcatvariables}}{\shortcatvariables,\secshortcatvariables} \, .
\end{align*}


\subsect{Generic Contractions}


Contractions of Tensor Networks $\extnet$ are operations to retrieve single tensors by summing products of tensors in a network over common indices.
We will define contractions formally by specifying just the indices not to be summed over.

When some of the variables are not appearing as leg variables, we define the contraction as being a tensor product with the trivial tensor $\ones$ carrying the legs of the missing variables.

\begin{definition}\label{def:contraction}
	Let $\tnetof{\graph}$ be a tensor network on a decorated hypergraph $\graph=(\nodes,\edges)$.
	For any subset $\secnodes\subset\nodes$ we define the contraction to be the tensor (for an example see \figref{fig:contraction})
	\begin{align}
		\contractionof{\tnetof{\graph}}{\secnodevariables} \in \bigotimes_{\node\in\secnodes} \rr^{\catdimof{\node}}
	\end{align}
	defined coordinatewise by the sum
	\begin{align}
		\contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}} =
		\sum_{\catindexof{\nodes/\secnodes} \in\,\nodestatesof{\nodes/\secnodes}}
		\left( \prod_{\edge\in\edges}\hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right) \, .
	\end{align}
	We call $\secnodevariables$ the open variables of the contraction.
\end{definition}

To ease notation, we will often omit the set notation by brackets $\{\cdot\}$ and specify the tensors to be contracted with the delimiter "," (see e.g. \exaref{exa:matrixProduct}).

\begin{figure}
	\begin{center}
		\input{OtherContent/tikz_pics/notation/contraction.tex}
	\end{center}
	\caption{
		Example of a tensor network contraction of all but the variables $\catvariableof{1},\catvariableof{3}$.
		Contraction of variables can always be depicted by closing the open legs with trivial tensors $\ones$ performing index sums.
	}\label{fig:contraction}
\end{figure}

%%
%Diagrammatic notation: Best to do version b), since this is easiest to see how tensors combine to new tensors by contractions.

\begin{remark}[Alternative Notations]
	% Einstein summations
	Contractions can also denoted by the Einstein summations of the indices along connected edges, understood as scalar product in each subspace.
	This is as in \defref{def:contraction}, just omitting the sums.
	We found it useful in this work to do the diagrammatic representation instead, since it offers a better possibility to depict hierarchical arrangements of shared variables.
\end{remark}


% Mode products
Further notations without usage of axis variables are mode products (see \cite{kolda_tensor_2009,hackbusch_tensor_2012,cichocki_tensor_2015}), often denoted by the operation $\times_n$.
With our more generic variable-based notations, we can capture these more specific contractions by coloring the tensor axes, that is assignment of axis variables.

% Examples
To further gain familiarity with the generic contractions, we show the connection to two more popular examples.

%% Diagrammatic representation of Matrix Vector
\begin{example}{Matrix Vector Products}\label{exa:matrixProduct}
	The matrix vector product is a special case of tensor contractions, where a matrix $\matrixat{\exrandom,\secexrandom}$ shares a categorical variable with a vector $\vectorat{\secexrandom}$.
	When leaving the variable unique to the matrix open we get the matrix vector product as
		\[ \contractionof{\matrixat{\exrandom,\secexrandom},\vectorat{\secexrandom}}{\exrandom=\exrandind} = \sum_{\secexrandind\in[\secexranddim]} \matrixat{\exrandom=\exrandind,\secexrandom=\secexrandind} \cdot \vectorat{\secexrandom=\secexrandind} \, .  \]

	Exploiting the diagramatic tensor network visualization we depict matrix vector by: %in \figref{fig:matrixProduct}.
	\begin{center}
		\input{OtherContent/tikz_pics/notation/one_hot_matrix.tex}
	\end{center}
%	Here the index $j$ is represented by a closed edge, which means that it is eliminated by a sum.
\end{example}

%% Hadamard Product 
\begin{example}{Hadamard products of vectors}\label{exa:hadamard}
	A node appearing in arbitrarily many hyperedges denotes a Hadamard product of the axis of the respective decorating tensors.
	To give an example, let $\vectorofat{\catenumerator}{\catvariable}\in\rr^\catdim$ be vectors for $\catenumeratorin$. Their hadamard product is the vector
		\[ \contractionof{\{\vectorofat{\catenumerator}{\catvariable} \wcols \catenumeratorin\}}{\catvariable}  \in \rr^\catdim \]
	defined by
		\[ \contractionof{\{\vectorofat{\catenumerator}{\catvariable} \wcols \catenumeratorin\}}{\indexedcatvariable}
		= \prod_{\atomenumeratorin} \vectorofat{\atomenumerator}{\indexedcatvariable}\, . \]
	In a contraction diagram the Hadamard product is depicted by: % in \figref{fig:hadamard}.
	\begin{center}
		\input{OtherContent/tikz_pics/notation/hadamard.tex}
	\end{center}
\end{example}



\subsect{Decompositions}

Tensors can be represented by tensor network decompositions, when the contraction of the network retrieves the tensor.

\begin{definition}\label{def:tnDecomposition}
	%Let $\hypercoreat{\nodevaraibles}$ be a tensor in $\extensorspace$.
	A Tensor Network Decomposition of a tensor $\hypercoreat{\nodevariables}$ is a Tensor Network $\tnetof{\graph}$ such that
		\[ \hypercoreat{\nodevariables}= \contractionof{\tnetof{\graph}}{\nodevariables} \, . \]
	We call the hypergraph $\graph$ the format of the decomposition.
\end{definition}





\subsect{Directed Tensors and Normalizations}

%% Directionality
Directionality represents constraints on the structure of tensors, namely that the sum over outgoing trivializes the tensor.

\begin{definition}\label{def:directedTensor}
	A Tensor
	\begin{align*}
		\hypercoreat{\nodevariables} \in\bigotimes_{\nodein}\rr^{\catdimof{\node}} 
	\end{align*}
	is said to be directed with incoming variables $\innodes$ and outgoing variables $\outnodes$, where $\nodes=\innodes\dot{\cup}\outnodes$, when
	\begin{align*}
		\contractionof{\hypercore}{\catvariableof{\innodes}} =  \onesat{\catvariableof{\innodes}}
	\end{align*}
	where $\onesat{\catvariableof{\innodes}}$ denoted the trivial tensor in  $\bigotimes_{\node\in\innodes}\rr^{\catdimof{\node}}$ which coordinates are all $1$.
\end{definition}

By the normalization operation, tensors are turned into directed tensors.

\begin{definition}\label{def:normalization}
	A tensor $\hypercoreat{\nodevariables}$ is said to be normalizable on $\innodes\subset\nodes$, if for any $\catindexof{\innodes}\in\nodestatesof{\innodes}$ we have
	\begin{align*}
		 \contraction{\hypercoreat{\nodevariables},\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}}} > 0 \, .
	\end{align*}
	The normalization of an on $\innodes\subset\nodes$ normalizable tensor is the tensor
	\begin{align*}
		\normalizationofwrt{\hypercoreat{\nodevariables}}{\catvariableof{\outnodes}}{\catvariableof{\innodes}} =
		\sum_{\catindexof{\innodes}\in\nodestatesof{\innodes}}
		\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}} \otimes \frac{
		\contractionof{\hypercoreat{\nodevariables},\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}}}{\catvariableof{\outnodes}}
		}{
		\contraction{\hypercoreat{\nodevariables},\onehotmapofat{\catindexof{\innodes}}{\catvariableof{\innodes}}}
		}
	\end{align*}
	where $\outnodes = \nodes/\innodes$.
\end{definition}

We will investigate the contractions of directed tensors in \parref{par:three}, where we show in \theref{the:normalizationDirected} that normalizations are directed tensors.
%% Diagrammatic notation
In our graphical tensor notation, we depict directed tensors by directed hyperedges (a), which are decorated by directed tensors (b), for example:
	\begin{center}
		\input{OtherContent/tikz_pics/notation/directed_core.tex}
	\end{center}



\sect{\bnencoding}

The tensor formalism, which treats tensors as collections of coordinates, can be used to encode functions.
While we introduce only the basic notation here, which will be applied later in \parref{par:one} and \parref{par:two}, a more detailed discussion of these representation schemes is deferred to \parref{par:three}.

\subsect{Coordinate encodings}

The most direct encoding scheme represents real valued functions on a discrete set of states by tensors.
Since it identifies each function evaluation at a state with a coordinate, we refer to it as coordinate encoding.

\begin{definition}[Coordinate encoding of real-valued functions]\label{def:notationCoordinateEncoding}
	Let there be a factored representation of a system by variables $\shortcatvariables$ and a real-valued function
	\begin{align*}
		\realvaluedfunction\defcols\facstates \rightarrow \rr \
	\end{align*}
	on the systems state set.
	Then the coordinate encoding of $\realvaluedfunction$ is the tensor $\cencodingofat{\realvaluedfunction}{\shortcatvariables}$ with the coordinates
	\begin{align*}
		\cencodingofat{\realvaluedfunction}{\indexedshortcatvariables} = \realvaluedfunctionev{\shortcatindices}
	\end{align*}
	for any $\shortcatindicesin$.
\end{definition}

To ease the notation, we will denote the coordinate encoding of a real-valued function by
\begin{align*}
	\realvaluedfunctionat{\shortcatvariables}
	\coloneqq \cencodingofat{\realvaluedfunction}{\shortcatvariables} \, .
\end{align*}
For a more detailed discussion of coordinate encodings, see \charef{cha:coordinateCalculus}.


\subsect{Basis encodings}

Let us now encode functions between the state sets of systems in factored representation.
The scheme is described in more generality and detail (encoding of subsets and relations) in \charef{cha:basisCalculus}, see \defref{def:functionRelationEncoding}.

\begin{definition}[Basis encoding of maps between state sets]\label{def:functionRepresentation}
	Let there be two systems with factored representations by variables $\shortcatvariables$ and $\headvariables$, and a map
	\begin{align*}
		 \statesetfunction\defcols\facstates \rightarrow  \headstates
	\end{align*}
	between there state sets.
	Then the basis encoding of $\statesetfunction$ is a tensor
	\begin{align*}
		\bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables}
		\in \left(\secfacspace\right) \otimes \left(\facspace\right)
	\end{align*}
	defined by
	\begin{align*}
		& \bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables} \\
		& \quad = \sum_{\catindices\in\facstates}
		\onehotmapofat{\statesetfunctionev{\shortcatindices}}{\headvariables} \otimes  \onehotmapofat{\shortcatindices}{\shortcatvariables} \, .
	\end{align*}
\end{definition}

Basis encodings are directed tensors (see \theref{the:bencodingDirected}) and are thus depicted as decorations of directed edges in hypergraphs:
\begin{center}
	\input{OtherContent/tikz_pics/notation/bencoding.tex}
\end{center}

% Generalization to arbitrary image sets
To extend the basis encoding scheme to represent any function on the state set of a system in factored representation, we now define enumeration variables (for more detail see \charef{cha:basisCalculus}).

\begin{definition}[Set enumeration variables]\label{def:indexinterpretationNotation}
    We say that an arbitrary finite set $\arbset$ is enumerated by an enumeration variable $\indvariableof{\arbset}$ taking values in $[\inddimof{\arbset}]$, when $\inddimof{\arbset}=\absof{\arbset}$ and there is a bijective index interpretation function
    \begin{align*}
        \indexinterpretation \defcols [\inddimof{\arbset}] \rightarrow \arbset \, .
    \end{align*}
\end{definition}

Given an arbitrary function
\begin{align*}
	\exfunction \defcols \facstates \rightarrow \arbset
\end{align*}
and an enumeration of $\arbset$ by the variable $\indvariableof{\arbset}$, we define the basis encoding of $\exfunction$ as
\begin{align*}
	\bencodingofat{\exfunction}{\indvariableof{\arbset},\shortcatvariables}
	\coloneqq \bencodingofat{\invindexinterpretation\circ\exfunction}{\indvariableof{\arbset},\shortcatvariables} \, .
\end{align*}

We further define the basis encoding to vector valued functions
\begin{align*}
	\sstat \defcols \facstates \rightarrow \bigtimes_{\selindexin} \arbsetof{\selindex}
\end{align*}
using multiple variables $\indvariableof{[\seldim]}=\{\indvariableof{0},\ldots,\indvariableof{\seldim-1}\}$ as
\begin{align*}
	\bencodingofat{\sstat}{\indvariableof{[\seldim]},\shortcatvariables}
	= \contractionof{\{\bencodingofat{\sstatcoordinateof{\selindex}}{\indvariableof{\selindex},\shortcatvariables}\wcols\selindexin\}}{\indvariableof{[\seldim]},\shortcatvariables} \, .
\end{align*}
Here we denote for $\selindexin$ by $\sstatcoordinateof{\selindex}$ the restriction of $\sstat$ to the coordinate $\selindex$ of the image.

\subsect{Selection encodings}\label{sec:selectionEncodingNotation}

We now turn to tensor-valued functions, for which we introduce the selection encodings next.
We call these tensor representation selection encodings, since the tensor structure of the image is captured by additional selection variables.

\begin{definition}[Selection encoding of tensor-valued functions]\label{def:selectionEncoding}
	Let there be a system with factored representation by the variables $\shortcatvariables$, a tensor space $\selspace$ described by categorical variables $\selvariables$ and a tensor-valued function
	\begin{align*}
		\tenvaluedfunction \defcols \facstates \rightarrow \selspace \, .
	\end{align*}
	The selection encoding of $\tenvaluedfunction$ is the tensor
	\begin{align*}
		\sencodingofat{\tenvaluedfunction}{\shortcatvariables,\shortselvariables}
		\in \left(\facspace\right) \otimes \left(\selspace\right)
	\end{align*}
	defined as the sum
	\begin{align*}
		 \sencodingofat{\tenvaluedfunction}{\shortcatvariables,\shortselvariables}
		 = \sum_{\shortcatindicesin} \onehotmapofat{\shortcatindices}{\shortcatvariables} \otimes \tenvaluedfunctionev{\shortcatindices}\left[\shortselvariables\right] \, .
	\end{align*}
\end{definition}

%% Basis encoding
We exploit the selection encoding formalism to further define basis encodings of tensor-valued functions.
To this end, we enumerate the coordinates
\begin{align*}
	\left\{ \sencodingofat{\tenvaluedfunction}{\indexedshortcatvariables,\indexedshortselvariables} \wcols \shortcatindicesin\ncond \shortselindicesin \right\}
\end{align*}
by an interpretation variable $\indvariableof{\tenvaluedfunction}$ and a map $\indexinterpretationof{\tenvaluedfunction}$.
Let us construct a function $\invindexinterpretationof{\tenvaluedfunction}\circ\sencodingof{\shortcatvariables,\shortselvariables}$ by
\begin{align*}
	\left(\invindexinterpretationof{\tenvaluedfunction}\circ\sencodingof{\shortcatvariables,\shortselvariables}\right)\left(\shortcatindices,\shortselindices\right)
	= \invindexinterpretationofat{\tenvaluedfunction}{\sencodingofat{\tenvaluedfunction}{\indexedshortcatvariables,\indexedshortselvariables}} \, .
\end{align*}
The basis encoding of $\sencodingof{\tenvaluedfunction}$ is then defined as the basis encoding of this function, that is
\begin{align*}
	\bsencodingofat{\tenvaluedfunction}{\indvariableof{\tenvaluedfunction},\shortcatvariables,\shortselvariables}
	\coloneqq \bencodingofat{\invindexinterpretationof{\tenvaluedfunction}\circ\sencodingof{\shortcatvariables,\shortselvariables}}{\indvariableof{\tenvaluedfunction},\shortcatvariables,\shortselvariables} \, .
\end{align*}
This encoding scheme differs from the basis encoding of $\tenvaluedfunction$, which has been defined in the previous section.