\begin{example}[Sparse representation of Sudoku rule Knowledge Base]
    \label{exa:sudokuDecomposition}%{\alex{Attempt to match the above Sudoku example with our notation of boolean variables and the entailment formalism}}
    We now exploit \theref{the:kbDecomposition} to find efficient tensor network representation of the Sudoku knowledge base from \exaref{exa:sudokuEntailment}.
    We directly get, that the knowledge base $\kb^{n}$ of Sudoku rules is a tensor network of the $4\cdot n^4$ constraint formulas using the $n^2$-ary connective $\woneoplus$, and the evidence $E^{\mathrm{start}}$ can be encoded by vectors $\tbasisat{\catvariableof{(r_0,r_1,c_0,c_1,i)}}$.
    To get a representation by vectors matrices instead of tensors of order $n^2$, we introduce a hidden variable $\decvariable$ taking values in $[n^2]$ for each of the constraints, one can further increase the sparsity of the representation.
    With the usage of matrices
    \begin{align*}
        \hypercoreofat{\catenumerator}{\catvariableof{\catenumerator},\decvariable}
        = \fbasisat{\catvariableof{\catenumerator}} \otimes \onesat{\decvariable} + (\tbasisat{\catvariableof{\catenumerator}}-\fbasisat{\catvariableof{\catenumerator}}) \otimes \onehotmapofat{\catenumerator}{\decvariable}
    \end{align*}
    we have the decomposition
    \begin{align*}
        \woneoplus[\catvariableof{[n^2]}]
        = \contractionof{\{\hypercoreofat{\catenumerator}{\catvariableof{\catenumerator},\decvariable} \wcols \catenumerator\in[n^2]\}}{\catvariableof{[n^2]}} \, ,
    \end{align*}
    which is a $\cpformat$ decomposition depicted as:
    \begin{center}
        \begin{tikzpicture}[scale=0.35,thick]

            \draw (-6,-1) rectangle (-12,1);
            \node[anchor=center] (A) at (-9,0) {\corelabelsize $\woneoplus$};
            \draw (-11.5,-1)--(-11.5,-2.5) node[midway,left] {\colorlabelsize $\catvariableof{0,0,0,0}$};
            \draw (-11,-1)--(-11,-2.5) node[midway,right] {\colorlabelsize $\catvariableof{0,0,0,1}$};
            \node[anchor=center] (A) at (-9,-2.5) {$\cdots$};
            \draw (-7,-1)--(-7,-2.5) node[midway,right] {\colorlabelsize $\catvariableof{0,0,0,n^2\shortminus 1}$};

            \node[anchor=center] (A) at (-3.5,0) {$=$};

            \draw (-1,-1) rectangle (1,1);
            \node[anchor=center] (A) at (0,0) {\corelabelsize $\hypercoreof{0}$};
            \draw (0,-1)--(0,-2.5) node[midway,right] {\colorlabelsize $\catvariableof{0,0,0,0}$};

            \draw (3,-1) rectangle (5,1);
            \node[anchor=center] (A) at (4,0) {\corelabelsize $\hypercoreof{1}$};
            \draw (4,-1)--(4,-2.5) node[midway,right] {\colorlabelsize $\catvariableof{0,0,0,1}$};

            \node[anchor=center] (text) at (8,0) {$\hdots$};

            \draw (10.75,-1) rectangle (13.25,1);
            \node[anchor=center] (A) at (12,0) {\corelabelsize $\hypercoreof{n^2\shortminus1}$};
            \draw (12,-1)--(12,-2.5) node[midway,right] {\colorlabelsize $\catvariableof{0,0,0,n^2\shortminus1}$};

            \drawvariabledot{6}{4}
            \node[anchor=south] (text) at (6,4) {\colorlabelsize $\decvariableof{0,0,0,:}$};

            \draw (6,4) to[bend right= 20] (0,1);
            \draw (6,4) to[bend right= 10] (4,1);
            \draw (6,4) to[bend right= -20] (12,1);

        \end{tikzpicture}
    \end{center}

    We model the Sudoku Knowledge Base as a tensor network on a hypergraph $\graphof{\mathrm{Sudoku},n}$
    \begin{itemize}
        \item $n^6+4\cdot n^4$ Nodes by $n^6$ categorical variables $\catvariableof{(r0,r1,c0,c1,i)}$ and by $4\cot n^4$ decomposition variables to the constraints
        \item $4\cdot n^6$ Edges
        \begin{align*}
            \edges=
            \bigcup_{r0,r1,c0,c1\in[n]}
            \big\{
            &\{\catvariableof{(r0,r1,c0,c1,i)}\},\{\catvariableof{(r0,r1,c0,c1,i)},\decvariableof{r0,r1,c0,c1,:}\},\{\catvariableof{(r0,r1,c0,c1,i)},\decvariableof{r0,r1,:,:,i}\},\\
            &\{\catvariableof{(r0,r1,c0,c1,i)},\decvariableof{:,:,c0,c1,i}\},
            \{\catvariableof{(r0,r1,c0,c1,i)},\decvariableof{r0,:,c0,:,i}\}\big\}
        \end{align*}
        We denote the decomposition variables to the position, row, column and square constraints by $\decvariableof{r0,r1,c0,c1,:},\decvariableof{r0,r1,:,:,i},\decvariableof{:,:,c0,c1,i}$ and $\decvariableof{r0,:,c0,:,i}$.
    \end{itemize}
    Each edge containing a decomposition variable is decorated by a matrix $\hypercoreof{\catenumerator}{\catvariable,\decvariable}$ corresponding to an core in the $\cpformat$ decomposition of a constraint.
    Here $\catenumerator$ is determined by the tuple $(r0,r1,c0,c1,i)$ and the type of the constraint (for example, for the variable $\catvariableof{(0,1,1,2,1)}$ and the row constraint $\decvariableof{(0,1,:,:,1)}$ we have $\catenumerator=1\cdot n + 2$.
    We further assign to each edge containing a single variable $\{\catvariableof{(r0,r1,c0,c1,i)}\}$ a either the vector $\tbasisat{\catvariableof{(r0,r1,c0,c1,i)}}$ if $(r0,r1,c0,c1,i)\in E^{\mathrm{start}}$ or the trivial vector $\onesat{\catvariableof{(r0,r1,c0,c1,i)}}$.
\end{example}