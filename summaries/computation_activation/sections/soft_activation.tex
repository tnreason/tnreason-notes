\section{Decomposition of Probability Distributions}\label{sec:prob_rep}

We here investigate tensor network decomposition mechanisms of probability distributions.
After introducing probability distributions as tensors we derive tensor network decompositions based on conditional independencies (applying the Hammersley-Clifford theorem \cite{clifford_markov_1971}) to motivate graphical models.
Further we present the \ComputationMechanism{}, in which the Fisher-Neyman Factorization Theorem is used to decompose distributions in the presence of sufficient statistics.

\subsection{Basic concepts}

Distributions $\probtensor$ over a discrete state space can be represented by tensors, where each entry corresponds to the probability of a corresponding state. The joint probability distribution for a set of categorical variables as in definition~\ref{def:atomic-factored-representation} is defined here.

\begin{definition}[Joint Probability Distribution]
    \label{def:probabilityDistribution} % From the axioms of Kolmogorov!
    Let there be for each $\catenumeratorin$ a categorical variable $\catvariableof{\catenumerator}$ taking values in $[\catdimof{\atomenumerator}]$.
    A joint probability distribution of these categorical variables is a function
    \begin{align*}
        \probwith \defcols \facstates \rightarrow \rr
    \end{align*}
    which is non-negative, that is for any $\shortcatindicesin$ it holds
    \begin{align*}
        \probat{\indexedshortcatvariables} \geq 0 \, ,
    \end{align*}
    and which is normalized, that is
    \begin{align*}
        \contraction{\probat{\shortcatvariables}} = 1 \, .
    \end{align*}
    Let $\thirdcatvariable$ be another variable taking values in a possibly infinite set $\mathrm{val}(\thirdcatvariable)$.
    Then a tensor $\condprobat{\shortcatvariables}{\thirdcatvariable}$ is a family of joint probability distributions, if for any $\thirdcatindex\in\mathrm{val}(\thirdcatvariable)$ the slice $\condprobat{\shortcatvariables}{\thirdcatvariable=\thirdcatindex}$ is a joint probability distribution.
\end{definition}

\input{../examples/coin_toss_family}

%\subsection{Reasoning based on contractions}

%\alex{The most central inference operation on probability distributions is the computation of marginals, which is just the contraction leaving only the marginalized variable open.
%Conditional probabilities are marginal proabilities of the distribution with added boolean tensors marking the evidence (e.g. one-hot encodings to states of some variables).
%With this formalism we can answer queries like "What is the probability of booking this account when having observed this feature on the invoice?".
%}

%\alex{Marginals are missing -> Add them and normalizations here!}

%In most of the analysis, we assume that every state in the space is assigned a positive probability.
%We say, a distribution $\probwith$ is \emph{positive} if $\mathbb{P}[X_{[d]} = x_{[d]}]>0$ for every configuration $x_{[d]}$.
%
%Note that in this tensor centered notation the calculation of marginal distributions reduces to contracting the open legs, not considered in the marginal with tensors $\ones$ only containing ones, see~\cite[Section 2.1.3]{goessmann_tensor-network_2025}.

A basic inference operation on probability distributions is the computation of marginal and conditional distribution. %, which is just the contraction leaving only the marginalized variable open.
%Conditional probabilities are marginal proabilities of the distribution with added boolean tensors marking the evidence (e.g. one-hot encodings to states of some variables).

\begin{definition}
    For any distribution $\probat{\catvariable,\seccatvariable}$ the marginal distribution is given by the contraction
    \begin{align*}
        \probat{\catvariable} \coloneqq \contractionof{\probat{\catvariable,\seccatvariable}}{\catvariable}
    \end{align*}
    which is depicted by the diagram
    \begin{center}
        \input{../tikz_pics/probability_representation/marginalized_probability.tex}
    \end{center}
    The conditional distribution of $\catvariable$ on $\seccatvariable$ is a tensor $\condprobat{\catvariable}{\seccatvariable}$ defined for $\seccatindex\in[\catdimof{1}]$
    \begin{align*}
        \condprobat{\catvariable}{\seccatvariable=\seccatindex} \coloneqq
        \begin{cases}
            \frac{1}{\catdim} \cdot \onesat{\catvariable} & \ifspace \contraction{\probat{\catvariable,\seccatvariable=\seccatindex}} = 0 \\
            \frac{1}{\contraction{\probat{\catvariable,\seccatvariable=\seccatindex}}} \cdot \probat{\catvariable,\seccatvariable=\seccatindex} & \text{else}
        \end{cases} \,
    \end{align*}
    and in the second case depicted by the diagram
    \begin{center}
        \input{../tikz_pics/probability_representation/conditional_probability.tex}
    \end{center}
\end{definition}


% \textcolor{gray}{One of them is by employing the chain rule.
% \begin{theorem}[Chain Rule]
%     \label{the:chainRule}
%     For any probability distribution $\probwith$ we have
%     \begin{align*}
%         \probat{\shortcatvariables}
%         = \contractionof{\{\margprobat{\catvariableof{0}}\} \cup
%         \left\{\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}} \wcols \catenumeratorin \ncond \catenumerator\geq 1\right\}
%         }{\shortcatvariables} \, ,
%     \end{align*}
%     provided that all conditional probability distributions exist.
% \end{theorem}
% \textcolor{darkgray}{ graphical notation of chain rule/Markov chain?}
% In case of Markov chains, where each random variable only depends on the previous random variable, this leads to a efficient representation.
% In general, the chain rule does not lead to a complexity reduction as the distribution $\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}}$ has the same dimensions as the original probability distribution $\probat{\shortcatvariables}$.
% Therefore, other decompositions can be considered.
% }
% \alex{Its important to note, that the chain rule for itself does not provide a more efficient representation of the distribution (since it is a generic decomposition). 
% To achieve efficient tensor network decomposition, one needs to combine that with conditional independence assumptions (as in the Markov Chain case), such that conditioned variables in the conditional probability distributions can be dropped.}

\subsection{The \IndependenceMechanism{}: Graphical Model Factorization}

%% Need for decomposition mechanisms: Motivation for independence considerations
The number of coordinates in a tensor representation of probability distributions is the product
\begin{align*}
    \prod_{\catenumeratorin}\catdimof{\catenumerator} \, ,
\end{align*}
and therefore scales exponentially in the number of coordinates.
To find efficient representation schemes of probability distributions by tensor networks, we need to exploit additional properties of the distribution.
%% Independence
Independence leads to severe sparsifications of conditional probabilities and is therefore the key assumption to gain sparse decompositions of probability distributions.

%Markov Networks on hypergraphs $\graph$ are exactly those \CompActNets{}, which are computable with respect to the identity and the graph $\graph$.
%We here show how the sets of Markov Networks can be characterized by conditional independence assumptions.
%\alex{We can present graphical models and independences as decomposition schemes of activation tensors.
%Usually in graphical models there are no computation cores, hence the "features" of the statistic are the same as the variables.
%In that case the activation tensor is the same as the joint distribution tensor, that is a contraction of tensors colored by subsets of the variables (building the edges of the graphical models).

\begin{definition}[Independence]
    \label{def:independence} %% Was {the:independenceProductCriterion} in the report!
    We say that $\catvariableof{0}$ is independent of $\catvariableof{1}$ with respect to a distribution $\probat{\catvariableof{0},\catvariableof{1}}$, if the distribution is the tensor product of the marginal distributions, that is
    \begin{align*}
        \probat{\catvariableof{0},\catvariableof{1}}
        = \probat{\catvariableof{0}} \otimes \probat{\catvariableof{1}} \, .
    \end{align*}
    In this case we denote $\independent{\catvariableof{0}}{\catvariableof{1}}$.
\end{definition}

Thus, independence appears directly as a tensor–product decomposition of probability distribution.
Using tensor network diagrams we depict this property by
\begin{center}
    \input{../tikz_pics/probability_representation/independent_decomposition}
\end{center}
Let us notice, that the assumption of independence reduces the degrees of freedom from $\exranddim\cdot\secexranddim-1$ to $(\exranddim-1)+(\secexranddim-1)$.
The decomposition into marginal distributions furthermore exploits this reduced freedom and provides an efficient storage.
Having a joint distribution of multiple variables, which disjoint subsets are independent, we can iteratively apply the decomposition scheme.
As a result, we can reduce the scaling of the degrees of freedom from exponential to linear by the assumption of independence.

Independence is, as we observed, a strong assumption, which is often too restrictive.
Conditional independence instead is a less demanding assumption, which still implies efficient tensor network decompositions schemes.
We introduce conditional independence as independence of variables with respect to conditional distributions.

\begin{definition}[Conditional Independence]
    \label{def:condIndependence} %% Was {the:condIndependenceProductCriterion} in the report!
    Given a joint distribution of variables $\catvariableof{0}$, $\catvariableof{1}$ and $\catvariableof{2}$, such that $\margprobat{\catvariableof{2}}$ is positive.
    We say that $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$ if for any states $\exrandindin,\secexrandindin$ and $\thirdexrandindin$
    \begin{align*}
        \condprobof{\catvariableof{0},\catvariableof{1}}{\catvariableof{2}}
        = \contractionof{
            \condprobof{\catvariableof{0}}{\catvariableof{2}},\condprobof{\catvariableof{1}}{\catvariableof{2}}
        }{\catvariableof{0},\catvariableof{1},\catvariableof{2}} \, .
    \end{align*}
    In this case we denote $\condindependent{\catvariableof{0}}{\catvariableof{1}}{\catvariableof{2}}$.
\end{definition}

Conditional independence stated in \defref{def:condIndependence} has a close connection with independence stated in \defref{def:independence}.
To be more precise, $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$, if and only if $\catvariableof{0}$ is independent of $\catvariableof{1}$ with respect to any slice $\condprobof{\catvariableof{0},\catvariableof{1}}{\indexedcatvariableof{2}}$ of the conditional distribution $\condprobof{\catvariableof{0},\catvariableof{1}}{\catvariableof{2}}$.

We can further exploit conditional independence to ﬁnd tensor network decompositions of probabilities, as we show as the next corollary.
\begin{corollary}% NEEDED?
    \label{cor:secCriterionCondIndepencence}
    Let $\probat{\catvariableof{0},\catvariableof{1},\catvariableof{2}}$ be a joint distribution.
    If and only if $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$ the distribution satisfies
    \begin{align*}
        \probat{\catvariableof{0},\catvariableof{1},\catvariableof{2}}
        = \contractionof{\condprobof{\catvariableof{0}}{\catvariableof{2}},\condprobof{\catvariableof{1}}{\catvariableof{2}},\margprobat{\catvariableof{2}}}{\catvariableof{0},\catvariableof{1},\catvariableof{2}} \, .
    \end{align*}
    In a diagrammatic notation this is depicted by
    \begin{center}
        \input{../tikz_pics/probability_representation/cond_independence_decomposition}
    \end{center}
\end{corollary}

%\begin{figure}[t]
%    \begin{center}
%        \input{../tikz_pics/probability_representation/cond_independence_decomposition}
%    \end{center}
%    \caption{Diagrammatic visualization of the contraction equation in \corref{cor:secCriterionCondIndepencence}.
%    Conditional independence of $\catvariableof{0}$ and $\catvariableof{1}$ given $\catvariableof{2}$ holds if the contraction on the right side is equal to the probability tensor on the left side.}
%    \label{fig:condIndependenceDecomposition}
%\end{figure}

%%% Repetition!
%By \theref{the:condIndependenceProductCriterion} and \corref{cor:secCriterionCondIndepencence}, the conditional independence $\catvariableof{0} \perp X_1 \mid X_2$ is equivalent to the factorization
%\[
%    \mathbb{P}[\catvariableof{0},X_1,X_2]
%    =
%    \bigl\langle
%    \mathbb{P}[\catvariableof{0}\mid X_2],\,\mathbb{P}[X_1\mid X_2],\,\mathbb{P}[X_2]
%    \bigr\rangle[\catvariableof{0},X_1,X_2],
%\]
%which is precisely the three–node graphical model with edges $(\catvariableof{0},X_1)$ and $(X_1,X_2)$ depicted in \figref{fig:condIndependenceDecomposition}.
%In the graphical–model CAN with identity statistic, the joint activation tensor $\xi[\catvariableof{0},X_1,X_2]$ thus decomposes into three smaller cores: a unary core on $X_2$ and two binary cores on $(\catvariableof{0},X_2)$ and $(X_1,X_2)$.


This conditional–independence pattern is the basic local building block that is generalized in Markov networks, which we define in the following.

\begin{definition}[Markov Network]
    \label{def:markovNetwork}
    Let $\tnetof{\graph}$ be a tensor network of non-negative tensors decorating a hypergraph $\graph$.
    Then the Markov Network $\probof{\graph}$ to $\tnetof{\graph}$ is the probability distribution of $\catvariableof{\node}$ defined by the tensor
    \begin{align*}
        \probofat{\graph}{\nodevariables} = \frac{
            \contractionof{\{\hypercoreof{\edge} \wcols \edgein\}}{\nodevariables}
        }{
            \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
        } = \normalizationof{\tnetof{\graph}}{\nodevariables} \, .
    \end{align*}
    We call the denominator
    \begin{align*}
        \partitionfunctionof{\tnetof{\graph}} = \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
    \end{align*}
    the partition function of the tensor network $\tnetof{\graph}$.
\end{definition}

We directly defined here graphical models on hypergraphs.
%% Graphical Models as Tensor Networks
It is known that probabilistic graphical models are dual to tensor networks \cite{robeva_duality_2019,glasser_expressive_2019}.
%By our hypergraph based definition of tensor networks, markov networks are equivalent to tensor networks of positive tensors.
We define graphical models based on hypergraphs, to establish a direct connection with tensor network decorating the hypergraph.
In a more canonical way, Markov Networks are instead defined by graphs, where instead of the edges the cliques are decorated by factor tensors (see for example \cite{koller_probabilistic_2009}).

%% CompActNets as Markov Networks - Check for redundancy!
We can interpret the factors $\hypercorewith$ as activation cores placed on the hyperedges $\edge$ of the graph.
The global activation tensor (and hence the joint distribution) is obtained by contracting this activation network and normalizing by its partition function.

%We call $\probwith$ \emph{positive} if $\probat{\indexedshortcatvariables}> 0$ for all states $\shortcatindices$.
%The marginalization of a Markov Network to $\tnetof{\graph}$ on subsets of variables $\catvariableof{\secnodes}$ is
%\begin{align*}
%    \probofat{\graph}{\catvariableof{\secnodes}}
%    = \normalizationof{\tnetof{\graph}}{\catvariableof{\secnodes}} \, .
%\end{align*}

%This can be derived from a commutativity of contractions, which established an equivalence of contractions with sequences of consecutive contractions.
%\begin{theorem}[Commutativity of Contractions] % NEEDED?
%    \label{the:splittingContractions}
%    Let $\tnetof{\graph}$ be a tensor network on a hypergraph $\graph=(\nodes,\edges)$.
%    Let us now split the $\graph$ into two graphs $\graph_1=(\nodesone,\edges_1)$ and $\graph_2=(\nodestwo,\edges_2)$, such that $\edges_1\dot{\cup}\edges_2=\edges$, $\nodesone\cup\nodestwo=\nodes$ and all nodes in $\nodestwo$ are contained in an hyperedge of $\edges_2$.
%    We then have for any $\secnodes\subset\nodes$
%    \begin{align*}
%        \contractionof{\tnetof{\graph}}{\catvariableof{\secnodes}}
%        = \contractionof{
%            \tnetofat{\graph_1}{\catvariableof{\nodesone}}
%            \cup \{\contractionof{\tnetof{\graph_2}}{\catvariableof{\nodestwo\cap(\nodesone\cup\secnodes)}}\}
%        }{\catvariableof{\secnodes}}   \, .
%    \end{align*}
%\end{theorem}

%Further, the distribution of $\catvariableof{\secnodes}$ conditioned on $\catvariableof{\thirdnodes}$, where $\secnodes,\thirdnodes$ are disjoint subsets of $\nodes$, is
%\begin{align*}
%    \probtensor^{\graph}\left[\catvariableof{\secnodes}|\catvariableof{\thirdnodes}\right]
%    = \normalizationofwrt{\tnetof{\graph}}{\catvariableof{\secnodes}}{\catvariableof{\thirdnodes}} \, .
%\end{align*}

While we have directly defined Markov Networks as decomposed probability distributions, we now want to derive assumptions on a distribution assuring that such decompositions exist.
As we will see, the sets of conditional independencies encoded by a hypergraph are captured by its seperation properties, as we define next.

\begin{definition}[Separation of Hypergraph]
    A path in a hypergraph is a sequence of nodes $\node_{\atomenumerator}$ for $\atomenumeratorin$, such that for any $\atomenumerator\in[\atomorder-1]$ we find a hyperedge $\edgein$ such that $(\node_{\atomenumerator}, \node_{\atomenumerator+1})\subset \edge$.
    Given disjoint subsets $\nodesa$, $\nodesb$, $\nodesc$ of nodes in a hypergraph $\graph$ we say that $\nodesc$ separates $\nodesa$ and $\nodesb$ with respect to $\graph$, when any path starting at a node in $\nodesa$ and ending in a node in $\nodesb$ contains a node in $\nodesc$.
\end{definition}

To characterize Markov Networks in terms of conditional independencies we need to further define the property of clique-capturing.
This property of clique-capturing established a correspondence of hyperedges with maximal cliques in the more canonical graph-based definition of Markov Networks \cite{koller_probabilistic_2009}.

\begin{definition}[Clique-Capturing Hypergraph]
    \label{def:ccHypergraph}
    We call a hypergraph $\graph$ clique-capturing, when each subset $\secnodes\subset\nodes$ is contained in a hyperedge, if for any $a,b\in\secnodes$ there is a hyperedge $\edgein$ with $a,b\in\secnodes$.
\end{definition}

Let us now show a characterization of Markov Networks in terms of conditional independencies.

% Characterization
\begin{theorem}[Hammersley-Clifford Factorization Theorem]
    \label{the:factorizationHammersleyClifford}
    Given a clique-capturing hypergraph $\graph$, the set of positive Markov Networks on the hypergraph coincides with the set of positive probability distributions, such that for each disjoint subsets of variables $\nodesa$, $\nodesb$, $\nodesc$ we have $\catvariableof{\nodesa}$ is independent of $\catvariableof{\nodesb}$ conditioned on $\catvariableof{\nodesc}$, when $\nodesc$ separates $\nodesa$ and $\nodesb$ in the hypergraph. % called d-separation
\end{theorem}
\begin{proof}
    Shown in Appendix~\secref{sec:proofFactorizationTheorems}.
\end{proof}


\input{../examples/coin_toss_hc.tex}


Equivalently, \theref{the:factorizationHammersleyClifford} states that for any strictly positive joint distribution $\mathbb{P}[X_V]$ whose conditional independencies are exactly those encoded by a clique–capturing hypergraph $G=(V,E)$, there exist non–negative activation cores $\tau_e[X_e]$ such that
\[
    P[X_V]
    =
    \frac{1}{Z}\,
    \contractionof{}{}
    \bigl\langle \{\tau_e : e \in E(G)\} \bigr\rangle[X_V],
\]
for a suitable normalizing constant $Z>0$.
Thus, the conditional–independence structure of $\mathbb{P}$ determines a global tensor–network decomposition of its activation (and hence joint) tensor.
We refer to this correspondence between independence structure and tensor–network factorization as the \emph{independence mechanism}, in analogy to the computation mechanism provided by sufficient statistics in Section 3.1.

\subsection{The \ComputationMechanism{}: Factorization in presense of Sufficient Statistics}
%\subsection{Sufficient statistics leading to tensor network decompositions ("the computation mechanism")}

\begin{definition}
    \label{def:sufStatistic}
    Let $\probat{\catvariable,\thirdcatvariable}$ be a joint distribution of the $\catdim$-dimensional variable $\catvariable$ and the $\thirdcatdim$-dimensional variable $\thirdcatvariable$ and let
    \begin{align*}
        \sstat \defcols [\catdim] \rightarrow [\headdim]
    \end{align*}
    be a statistic.
    We are interested in the distribution $\probat{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}=\contractionof{\probat{\catvariable,\thirdcatvariable},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}$.
    We say that $\sstat$ is a sufficient statistic for $\thirdcatvariable$ if and only if $\catvariable$ is independent of $\thirdcatvariable$ conditioned on $\headvariableof{\sstat}$.
\end{definition}

Note that the independence in \defref{def:sufStatistic} is true if and only if
\begin{align*}
    \condprobat{\catvariable}{\thirdcatvariable,\headvariableof{\sstat}}
    =   \condprobat{\catvariable}{\headvariableof{\sstat}} \otimes \onesat{\thirdcatvariable} \, .
\end{align*}

\input{../examples/sufstat_probability}

\exaref{exa:sufStatProb} hints at a connection between sufficient statistics and decompositions into \CompActNets{}.
More generally, such decompositions are provided by the Fisher-Neyman Factorization Theorem.

\begin{theorem}[Fisher-Neyman Factorization Theorem] % See appendix of CompAct Nets paper!
    \label{the:factorizationFisherNeyman}
    Let $\probtensor$ be a joint distribution of variables $\catvariable,\thirdcatvariable$ with values $\mathrm{val}(\catvariable), \,\mathrm{val}(\thirdcatvariable)$.
    Let there further be a finite set $\mathrm{val}(\headvariableof{\sstat})$.
    Then $\sstat\defcols \mathrm{val}(\catvariable) \rightarrow \mathrm{val}(\headvariableof{\sstat})$ is a sufficient statistic for $\thirdcatvariable$ if and only if there are tensors $\basemeasureat{\catvariable}$ and $\acttensorat{\headvariableof{\sstat},\thirdcatvariable}$ such that
    \begin{align*}
        \probat{\catvariable,\thirdcatvariable}
        = \contractionof{
            \acttensorat{\headvariableof{\sstat},\thirdcatvariable}\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable},\basemeasureat{\catvariable}
        }{\catvariable,\thirdcatvariable} \, .
    \end{align*}
    We depict this equation diagrammatically by
    \begin{center}
        \input{../tikz_pics/probability_representation/computation_decomposition.tikz}
    \end{center}
\end{theorem}
\begin{proof}
    Shown in more generality in the appendix, see \theref{the:generalFactorizationFisherNeyman}.
\end{proof}

%%
Notice, that the definition of sufficient statistic does not make use of the marginal distribution $\probat{\thirdcatvariable}$.
We therefore can define sufficient statistics also for families of distributions $\condprobat{\catvariable}{\thirdcatvariable}$, with respect to arbitrary non-degenerate marginal distribution $\probat{\thirdcatvariable}$.
We then use the \theref{the:factorizationFisherNeyman} to embed such families in \CompActNets{}.

\begin{corollary}
    Let $\condprobof{\shortcatvariables}{\thirdcatvariable}$ be an arbitrary family of distributions of $\shortcatvariables$, which is .
    Then there is a tensor $\basemeasurewith$ and a activation tensors $\acttensorat{\headvariables,\thirdcatvariable}$ such that for any $\thirdcatindex\in\mathrm{val}(\thirdcatvariable)$
    \begin{align*}
        \condprobat{\shortcatvariables}{\thirdcatvariable=\thirdcatindex} \in\cansof{\sstat,\maxgraph,\basemeasure} \, .
    \end{align*}
\end{corollary}

\input{../examples/coin_toss_ft}

The Fisher-Neyman Theorem is the fundamental motivation for the \CompActNets{} Architecture:
\begin{itemize}
    \item The tensors in a decomposition of $\acttensorwith$ are called \emph{activation cores}.
    \item The tensors in a decomposition of $\bencodingofat{\sstat}{\headvariables,\shortcatvariables}$ are called \emph{computation cores}.
\end{itemize}

\begin{example}[Graphical Models as a special case of \CompActNets{}]
    Recall \defref{def:realizableStatDistributions}.
    Given a statistic $\sstat:\facstates \to \parspace$ and a hypergraph $\graph=(\nodes,\edges)$ on the image coordinates $\headvariables$, any by $\sstat$ computable and by $\graph$ activated \CompActNets{} has the form
    \begin{align*}
        \probwith =  \normalizationof{\acttensorwith,\bencsstatwith}{\shortcatvariables}
    \end{align*}
    where $\acttensorwith$ is an arbitrary non-negative tensor.
    For graphical models we take the \emph{identity statistic}
    \[
        \identity\big(\shortcatindices\big)
        = \shortcatindices,
    \]
    so that the image coordinates coincide with the variables and there are no non-trivial computation cores.
    The associated basis encoding is just the identity tensor
    \begin{align*}
        \bencodingofat{\identity}{\headvariableof{[\catorder]},\shortcatvariables} = \identityat{\shortcatvariables,\headvariableof{[\catorder]}} \, .
    \end{align*}
    and therefore, for any activation tensor $\xi[Y[d]]$ we obtain
    \begin{align*}
        \probwith
        &= \normalizationof{\acttensorwith,\bencodingofat{\identity}{\headvariableof{[\catorder]},\shortcatvariables}}{\shortcatvariables}
        & = \normalizationof{\acttensorat{\shortcatvariables}}{\shortcatvariables}
    \end{align*}
    In other words, in the graphical–model case the activation tensor coincides with the joint distribution tensor.
    In this setting, structural properties of the distribution such as (conditional) independences can be read off as algebraic factorization patterns of the activation (and hence joint) tensor.

\end{example}

\subsection{Exponential families in case of elementary activation tensors}

% Universal properties
A classical theorem by Pitman-Koopman-Darmois (see \cite{pitman_sufficient_1936}) states, that whenever a family with constant support and a finite sufficient statistic for arbitrary large data sets is in an exponential family.
We now restrict the activation cores to specific elementary tensors, which correspond with further assumptions on the dependence of $\probtensor$ and $\sstat$ made by exponential families.
For a discussion of further universal properties of exponential families, such that the existence of priors and entropy maximizers, see \cite{murphy_probabilistic_2022}.

\begin{definition}[Exponential Family]
    %\alex{was \theref{the:expFamilyTensorRep}}
    Given a base measure $\basemeasure$ and a statistic $\sstat:\facstates\rightarrow\parspace$ we enumerate for each coordinate $\selindexin$ the image $\imageof{\sstatcoordinateof{\selindex}}$ by an interpretation map
    \begin{align*}
        \indexinterpretationof{\selindex} \defcols
        [\cardof{\imageof{\sstatcoordinateof{\selindex}}}] \rightarrow \imageof{\sstatcoordinateof{\selindex}} \, .
    \end{align*}
    For any canonical parameter vector $\canparamwithin$ we build the activation cores $\softactlegwith$ for each coordinate $\headindexof{\selindex}\in[\cardof{\imageof{\sstatcoordinateof{\selindex}}}]$ by
    \begin{align*}
        \softactlegat{\indexedheadvariableof{\selindex}}
        = \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}}} \,
    \end{align*}
    and define the distribution % Could do parametrization to thirdcatvariable to make connection with the family definition..
    \begin{align*}
        \expdistwith =
        \normalizationof{\{\basemeasurewith\} \cup \{\bencodingofat{\sstatcoordinateof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}\cup\{\softactlegwith \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
    We then call the tensor $\probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta}$ with $\valof{\Theta}=\parspace$ and slices for $\canparamin$ by
    \begin{align*}
        \probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}
        = \expdistwith
    \end{align*}
    the exponential family to the statistic $\sstat$ and the base measure $\basemeasure$.
\end{definition}

%% In Elementary Activations
Note that by construction each member of an exponential family is an element in a \CompActNet{} with elementary activation cores (see \defref{def:elementaryActivation}), that is
\begin{align*}
    \probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}
    \in \cansof{\sstat,\elgraph,\basemeasure} \, .
\end{align*}

\begin{figure}
    \begin{center}
        \input{/Users/goessmann/Documents/tnreason/notes/summaries/computation_activation/tikz_pics/probability_representation/expdist_elementary}
    \end{center}
    \caption{
        Tensor Network diagram of a member of an exponential family $\probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}$ before normalization as an \CompActNet{} with elementary activation, that is an element in $\cansof{\sstat,\elgraph,\basemeasure}$.}\label{fig:expdistElementary}
\end{figure}

\input{../examples/two_boolean_cascade}


%% BAD NOTATION
%Having described how independence structures induce sparse tensor–network factorizations of the activation (and hence joint) tensor via the Hammersley-Clifford theorem (\theref{the:factorizationHammersleyClifford}) in the previous section, we now turn to inference.
%In this section we explain how standard inference queries, namely marginals and conditional probabilities, are realized as tensor contractions of the resulting decomposed network.
%
%In the graphical–model CAN with identity statistic, the joint distribution is represented by the activation tensor $\xi[X_V] = \mathbb{P}[X_V]$ on the variable set $V$.
%For any subset $A \subseteq V$, the marginal distribution on $X_A$ is obtained by contracting all non–query legs with trivial tensors $I[X_v]$:
%\[
%    \mathbb{P}[X_A]
%    =
%    \bigl\langle
%    \mathbb{P}[X_V],
%    \{ I[X_v] : v \in V \setminus A \}
%    \bigr\rangle[X_A].
%\]
%In words, we close all legs corresponding to variables outside $A$ with identity tensors and read off the resulting tensor on the remaining open legs $X_A$.
%In the Markov/activation network representation of Section~3.3, computing a marginal thus corresponds to closing all non–query nodes and edges and evaluating the resulting contracted network on the query variables.
%
%We can interpret conditionals as normalized evidence slices. Let $A,C \subseteq V$ and fix evidence $X_C = x_C$.
%Using the one–hot encodings $\epsilon_{x_c}[X_c]$ from \defref{def:onehotenc}, we define the unnormalized slice
%\begin{equation*}
%    \mathbb{\tilde{P}}[X_A \mid X_C = x_C]:=
%    \bigl\langle
%    \mathbb{P}[X_V],
%    \{\epsilon_{x_c}[X_c] : c \in C\},
%    \{ I[X_v] : v \in V \setminus (A \cup C) \}
%    \bigr\rangle[X_A].
%\end{equation*}
%The corresponding conditional distribution is obtained by normalization,
%\begin{equation}
%    \mathbb{P}[X_A \mid X_C = x_C] = \frac{\mathbb{\tilde{P}}[X_A \mid X_C = x_C]}{\mathbb{\tilde{P}}[X_A \mid X_C = x_C][\emptyset]}
%    = \frac{\mathbb{\tilde{P}}[X_A \mid X_C = x_C]}
%    {\displaystyle \sum_{x_A} \mathbb{\tilde{P}}[X_A = x_A \mid X_C = x_C]}.
%\end{equation}
%
%
%Thus conditional probabilities are obtained by contracting the activation/joint tensor with one–hot evidence tensors and renormalizing the resulting slice.
%In a Computation–Activation Network, inference with evidence therefore corresponds to contracting the activation network with one–hot tensors encoding observed variables and reading out the resulting activation on the query legs.
%
%\maxf{Accounting example here?}
