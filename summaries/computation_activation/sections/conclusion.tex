\section{Conclusion \& outlook}\label{sec:conclusion}

% Conclusion: Representation
This work developed a tensor network formalism to capture in a unifying way the main concepts of AI, which build the core of the probabilistic, neural and logical approaches.
We introduced \ComputationActivationNetworks{} (\CompActNets{}) as a generic architecture to represent classes of propositional knowledge bases, graphical models and more generic exponential families.
Moreover, we demonstrate the representation and training of hybrid models combining logical and probabilistic aspects, illustrating that \CompActNets{} represent a powerful, versatile and mathematically grounded framework for Neuro-Symbolic AI.

% Conclusion: Reasoning by Contraction
We have shown that model inference such as the calculating marginal distributions and deciding entailment correspond with tensor network contractions.
To efficiently perform these inferences, we presented message passing schemes, which have been shown to be exact in specific cases.
In general, however, the efficient computation of contractions is not possible, since they are related to the $\mathrm{NP}$-hardness of probabilistic inferences in graphical models (see \cite{koller_probabilistic_2009}) and of logical reasoning (see \cite{russell_artificial_2021}).
In cases where exact inference is not feasible, the derivation of error bounds for approximate inference schemes on \CompActNets{} is an interesting direction for future research.

% Outlook: Variational inference schemes
Further approximation schemes to overcome this bottleneck are summarized under the umbrella of variational inference (see \cite{wainwright_graphical_2008}), such as generic expectation-propagation methods or mean field methods.
While these schemes are developed either for graphical models or more general exponential families, we plan to derive similar methods for more general \CompActNets{}, such as \HybridLogicNetworks{}.
% Outlook: Monte-Carlo inference schemes
Further frequently applied schemes are particle-based inference schemes such as Gibbs sampling.

% Outlook: NeSy Architectures
The integration of symbolic and neural methods is an active research area (see \cite{colelough_neuro-symbolic_2024} for a systematic review).
The \CompActNets{} framework enables both, symbolic logical as well as probabilistic models, but enables also the representation of generic functions.
\CompActNets{} based on architectures combining symbolically verbalizable and more generic neural parts are thus a promising direction for Neuro-Symbolic AI.

% Outlook: LLMs
The \CompActNets{} framework offers an immediate practical application as a verifiable reasoning engine for AI agents in high-stakes domains such as regulatory compliance, clinical decision support, accounting, process planning and security.
By leveraging the framework's inherent flexibility, Large Language Models (see \cite{vaswani_attention_2017}) can be adapted to function as semantic translators that dynamically construct problem-specific tensor networks in the form of \CompActNets{} from natural language descriptions, effectively treating the reasoning engine as an external tool.
This approach mitigates the hallucination risks of probabilistic models by delegating complex logical execution to the exact linear algebra of the tensor network, ensuring that the inference process is both rigorous and reproducible.
Consequently, this synergy enables the deployment of reliable AI systems where the intuitive power of the Large Language Model is grounded by the explainable, instance-adaptive topology of the \CompActNets{}.