%\section{Decomposition of Probability Distributions}
\section{The Probabilistic Paradigm}\label{sec:probPar}

We here investigate tensor network decomposition mechanisms of probability distributions.
After introducing probability distributions as tensors we derive tensor network decompositions based on conditional independencies (applying the Hammersley-Clifford theorem \cite{clifford_markov_1971}) to motivate graphical models.
Further we present the \ComputationMechanism{}, in which the Fisher-Neyman Factorization Theorem is used to decompose distributions in the presence of sufficient statistics.

\subsection{Basic concepts}

As defined next, distributions $\probtensor$ over a discrete state space can be represented by tensors, where each entry corresponds to the probability of a corresponding state.

\begin{definition}[Joint Probability Distribution]
    \label{def:probabilityDistribution} % From the axioms of Kolmogorov!
    Let there be for each $\catenumeratorin$ a categorical variable $\catvariableof{\catenumerator}$ taking values in $[\catdimof{\atomenumerator}]$.
    A joint probability distribution of these categorical variables is a function
    \begin{align*}
        \probwith \defcols \facstates \rightarrow \rr
    \end{align*}
    which is non-negative, that is for any $\shortcatindicesin$ it holds
    \begin{align*}
        \probat{\indexedshortcatvariables} \geq 0 \, ,
    \end{align*}
    and which is normalized, that is
    \begin{align*}
        \contraction{\probat{\shortcatvariables}} = 1 \, .
    \end{align*}
    Let $\thirdcatvariable$ be another variable taking values in a possibly infinite set $\mathrm{val}(\thirdcatvariable)$.
    Then a tensor $\condprobat{\shortcatvariables}{\thirdcatvariable}$ is a family of joint probability distributions, if for any $\thirdcatindex\in\mathrm{val}(\thirdcatvariable)$ the slice $\condprobat{\shortcatvariables}{\thirdcatvariable=\thirdcatindex}$ is a joint probability distribution.
\end{definition}

\input{../examples/probabilistic_paradigm/coin_toss_family}

A basic inference operation on probability distributions is the computation of marginal and conditional distribution.

\begin{definition}\label{def:marginalConditionalDistribution}
    For any distribution $\probat{\catvariable,\seccatvariable}$ the marginal distribution is given by the contraction
    \begin{align*}
        \probat{\catvariable} \coloneqq \contractionof{\probat{\catvariable,\seccatvariable}}{\catvariable}
    \end{align*}
    which is depicted by the diagram
    \begin{center}
        \input{../tikz_pics/probability_representation/marginalized_probability.tex}
    \end{center}
    The conditional distribution of $\catvariable$ on $\seccatvariable$ is a tensor $\condprobat{\catvariable}{\seccatvariable}$ defined for $\seccatindex\in[\catdimof{1}]$
    \begin{align*}
        \condprobat{\catvariable}{\seccatvariable=\seccatindex} \coloneqq
        \begin{cases}
            \frac{1}{\catdim} \cdot \onesat{\catvariable} & \ifspace \contraction{\probat{\catvariable,\seccatvariable=\seccatindex}} = 0 \\
            \frac{1}{\contraction{\probat{\catvariable,\seccatvariable=\seccatindex}}} \cdot \probat{\catvariable,\seccatvariable=\seccatindex} & \text{else}
        \end{cases} \,
    \end{align*}
    and in the second case depicted by the diagram
    \begin{center}
        \input{../tikz_pics/probability_representation/conditional_probability.tex}
    \end{center}
\end{definition}


% \textcolor{gray}{One of them is by employing the chain rule.
% \begin{theorem}[Chain Rule]
%     \label{the:chainRule}
%     For any probability distribution $\probwith$ we have
%     \begin{align*}
%         \probat{\shortcatvariables}
%         = \contractionof{\{\margprobat{\catvariableof{0}}\} \cup
%         \left\{\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}} \wcols \catenumeratorin \ncond \catenumerator\geq 1\right\}
%         }{\shortcatvariables} \, ,
%     \end{align*}
%     provided that all conditional probability distributions exist.
% \end{theorem}
% \textcolor{darkgray}{ graphical notation of chain rule/Markov chain?}
% In case of Markov chains, where each random variable only depends on the previous random variable, this leads to a efficient representation.
% In general, the chain rule does not lead to a complexity reduction as the distribution $\condprobof{\catvariableof{\catenumerator}}{\catvariableof{0},\ldots,\catvariableof{\catenumerator-1}}$ has the same dimensions as the original probability distribution $\probat{\shortcatvariables}$.
% Therefore, other decompositions can be considered.
% }
% \alex{Its important to note, that the chain rule for itself does not provide a more efficient representation of the distribution (since it is a generic decomposition). 
% To achieve efficient tensor network decomposition, one needs to combine that with conditional independence assumptions (as in the Markov Chain case), such that conditioned variables in the conditional probability distributions can be dropped.}

\subsection{The Independence Mechanism: Graphical Model Factorization}

%% Need for decomposition mechanisms: Motivation for independence considerations
The number of coordinates in a tensor representation of probability distributions is the product
\begin{align*}
    \prod_{\catenumeratorin}\catdimof{\catenumerator} \, ,
\end{align*}
and therefore scales exponentially in the number of coordinates.
To find efficient representation schemes of probability distributions by tensor networks, we need to exploit additional properties of the distribution.
%% Independence
Independence leads to severe sparsifications of conditional probabilities and is therefore the key assumption to gain sparse decompositions of probability distributions.

%Markov Networks on hypergraphs $\graph$ are exactly those \CompActNets{}, which are computable with respect to the identity and the graph $\graph$.
%We here show how the sets of Markov Networks can be characterized by conditional independence assumptions.
%\alex{We can present graphical models and independences as decomposition schemes of activation tensors.
%Usually in graphical models there are no computation cores, hence the "features" of the statistic are the same as the variables.
%In that case the activation tensor is the same as the joint distribution tensor, that is a contraction of tensors colored by subsets of the variables (building the edges of the graphical models).

\begin{definition}[Independence]
    \label{def:independence} %% Was {the:independenceProductCriterion} in the report!
    We say that $\catvariableof{0}$ is independent of $\catvariableof{1}$ with respect to a distribution $\probat{\catvariableof{0},\catvariableof{1}}$, if the distribution is the tensor product of the marginal distributions, that is
    \begin{align*}
        \probat{\catvariableof{0},\catvariableof{1}}
        = \probat{\catvariableof{0}} \otimes \probat{\catvariableof{1}} \, .
    \end{align*}
    In this case we denote $\independent{\catvariableof{0}}{\catvariableof{1}}$.
\end{definition}

Thus, independence appears directly as a tensor–product decomposition of probability distribution.
Using tensor network diagrams we depict this property by
\begin{center}
    \input{../tikz_pics/probability_representation/independent_decomposition}
\end{center}
Let us notice, that the assumption of independence reduces the degrees of freedom from $\exranddim\cdot\secexranddim-1$ to $(\exranddim-1)+(\secexranddim-1)$.
The decomposition into marginal distributions furthermore exploits this reduced freedom and provides an efficient storage.
Having a joint distribution of multiple variables, which disjoint subsets are independent, we can iteratively apply the decomposition scheme.
As a result, we can reduce the scaling of the degrees of freedom from exponential to linear by the assumption of independence.

% Weakening of independence to conditional independence
Independence is, as we observed, a strong assumption, which is often too restrictive.
Conditional independence instead is a less demanding assumption, which still implies efficient tensor network decompositions schemes.
We introduce conditional independence as independence of variables with respect to conditional distributions.

\begin{definition}[Conditional Independence]
    \label{def:condIndependence} %% Was {the:condIndependenceProductCriterion} in the report!
    Given a joint distribution of variables $\catvariableof{0}$, $\catvariableof{1}$ and $\catvariableof{2}$, such that $\margprobat{\catvariableof{2}}$ is positive.
    We say that $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$ if for any states $\exrandindin,\secexrandindin$ and $\thirdexrandindin$
    \begin{align*}
        \condprobof{\catvariableof{0},\catvariableof{1}}{\catvariableof{2}}
        = \contractionof{
            \condprobof{\catvariableof{0}}{\catvariableof{2}},\condprobof{\catvariableof{1}}{\catvariableof{2}}
        }{\catvariableof{0},\catvariableof{1},\catvariableof{2}} \, .
    \end{align*}
    In this case we denote $\condindependent{\catvariableof{0}}{\catvariableof{1}}{\catvariableof{2}}$.
\end{definition}

Conditional independence stated in \defref{def:condIndependence} has a close connection with independence stated in \defref{def:independence}.
To be more precise, $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$, if and only if $\catvariableof{0}$ is independent of $\catvariableof{1}$ with respect to any slice $\condprobof{\catvariableof{0},\catvariableof{1}}{\indexedcatvariableof{2}}$ of the conditional distribution $\condprobof{\catvariableof{0},\catvariableof{1}}{\catvariableof{2}}$.

We can further exploit conditional independence to ﬁnd tensor network decompositions of probabilities, as we show as the next corollary.
\begin{corollary}% NEEDED?
    \label{cor:secCriterionCondIndepencence}
    Let $\probat{\catvariableof{0},\catvariableof{1},\catvariableof{2}}$ be a joint distribution.
    If and only if $\catvariableof{0}$ is independent of $\catvariableof{1}$ conditioned on $\catvariableof{2}$ the distribution satisfies
    \begin{align*}
        \probat{\catvariableof{0},\catvariableof{1},\catvariableof{2}}
        = \contractionof{\condprobof{\catvariableof{0}}{\catvariableof{2}},\condprobof{\catvariableof{1}}{\catvariableof{2}},\margprobat{\catvariableof{2}}}{\catvariableof{0},\catvariableof{1},\catvariableof{2}} \, .
    \end{align*}
    In a diagrammatic notation this is depicted by
    \begin{center}
        \input{../tikz_pics/probability_representation/cond_independence_decomposition}
    \end{center}
\end{corollary}

This conditional–independence pattern is the basic local building block that is generalized in Markov networks, which we define in the following.

\begin{definition}[Markov Network]
    \label{def:markovNetwork}
    Let $\tnetof{\graph}$ be a tensor network of non-negative tensors decorating a hypergraph $\graph$.
    Then the Markov Network $\probof{\graph}$ to $\tnetof{\graph}$ is the probability distribution of $\catvariableof{\node}$ defined by the tensor
    \begin{align*}
        \probofat{\graph}{\nodevariables} = \frac{
            \contractionof{\{\hypercoreof{\edge} \wcols \edgein\}}{\nodevariables}
        }{
            \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
        } = \normalizationof{\tnetof{\graph}}{\nodevariables} \, .
    \end{align*}
    We call the denominator
    \begin{align*}
        \partitionfunctionof{\tnetof{\graph}} = \contraction{\{\hypercoreof{\edge} \wcols \edgein\}}
    \end{align*}
    the partition function of the tensor network $\tnetof{\graph}$.
\end{definition}

%% Graphical Models as Tensor Networks: Alternative approach via graph duality
We define graphical models based on hypergraphs, to establish a direct connection with tensor network decorating the hypergraph.
In a more canonical way, Markov Networks are instead defined by graphs, where instead of the edges the cliques are decorated by factor tensors (see for example \cite{koller_probabilistic_2009}).
The probabilistic graphical models are along that alternative dual to tensor networks \cite{robeva_duality_2019,glasser_expressive_2019}.

%% CompActNets as Markov Networks - Check for redundancy!
We can interpret the factors $\hypercorewith$ as activation cores placed on the hyperedges $\edge$ of the graph.
The global activation tensor (and hence the joint distribution) is obtained by contracting this activation network and normalizing by its partition function.

%
While we so far have defined Markov Networks as decomposed probability distributions, we now want to derive assumptions on a distribution assuring that such decompositions exist.
As we will see, the sets of conditional independencies encoded by a hypergraph are captured by its separation properties, as we define next.

\begin{definition}[Separation of Hypergraph]
    A path in a hypergraph is a sequence of nodes $\node_{\atomenumerator}$ for $\atomenumeratorin$, such that for any $\atomenumerator\in[\atomorder-1]$ we find a hyperedge $\edgein$ such that $(\node_{\atomenumerator}, \node_{\atomenumerator+1})\subset \edge$.
    Given disjoint subsets $\nodesa$, $\nodesb$, $\nodesc$ of nodes in a hypergraph $\graph$ we say that $\nodesc$ separates $\nodesa$ and $\nodesb$ with respect to $\graph$, when any path starting at a node in $\nodesa$ and ending in a node in $\nodesb$ contains a node in $\nodesc$.
\end{definition}

To characterize Markov Networks in terms of conditional independencies we need to further define the property of clique-capturing.
This property of clique-capturing established a correspondence of hyperedges with maximal cliques in the more canonical graph-based definition of Markov Networks \cite{koller_probabilistic_2009}.

\begin{definition}[Clique-Capturing Hypergraph]
    \label{def:ccHypergraph}
    We call a hypergraph $\graph$ \emph{clique-capturing}, when each subset $\secnodes\subset\nodes$ is contained in a hyperedge, if for any $\nodea,\nodeb\in\secnodes$ there is a hyperedge $\edgein$ with $\nodea,\nodeb\in\secnodes$.
\end{definition}

Let us now show a characterization of Markov Networks in terms of conditional independencies.

% Characterization
\begin{theorem}[Hammersley-Clifford Factorization Theorem]
    \label{the:factorizationHammersleyClifford}
    Let there be a positive probability distribution $\probwithnodes$ and a clique-capturing hypergraph $\graph=(\nodes,\edges)$.
    Then the following are equivalent:
    \begin{itemize}
        \item[i] The distribution $\probwithnodes$ is representable by a Markov Network on $\graph$, i.e. for each edge $\edgein$ there is a tensor $\hypercoreofat{\edge}{\catvariableof{\edge}}$ such that
        \begin{align*}
            \probwithnodes = \normalizationof{\{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edgein\}}{\nodevariables}
        \end{align*}
        \item[ii] For any subsets $\nodesa,\nodesb,\nodesc\subset\nodes$ such that $\nodesc$ separates $\nodesa$ from $\nodesb$, we have
        \begin{align*}
            \condindependent{\catvariableof{\nodesa}}{\catvariableof{\nodesb}}{\catvariableof{\nodesc}} \, .
        \end{align*}
    \end{itemize}
    %Given a clique-capturing hypergraph $\graph$, the set of positive Markov Networks on the hypergraph coincides with the set of positive probability distributions, such that for each disjoint subsets of variables $\nodesa$, $\nodesb$, $\nodesc$ we have $\catvariableof{\nodesa}$ is independent of $\catvariableof{\nodesb}$ conditioned on $\catvariableof{\nodesc}$, when $\nodesc$ separates $\nodesa$ and $\nodesb$ in the hypergraph. % called d-separation
\end{theorem}
\begin{proof}
    Shown in Appendix~\secref{sec:proofFactorizationTheorems}.
\end{proof}

By \theref{the:factorizationHammersleyClifford} the conditional independence structure of $\probwithnodes$ determines a global tensor network decomposition of $\probwithnodes$.
We refer to this correspondence between independence structure and tensor network factorization as the \emph{\independenceMechanism{}}.
Note, that the assumption of a positive distribution is required (i.e. for all $\shortcatindices$ we have $\probat{\indexedshortcatvariables}>0$).
The assumption of positivity was however not required in our characterization of independencies and conditional independencies by the existence of corresponding tensor decompositions (see \defref{def:independence} and \defref{def:condIndependence}).

\input{../examples/probabilistic_paradigm/coin_toss_hc.tex}

\input{../examples/probabilistic_paradigm/student_hc}

\subsection{The Computation Mechanism: Factorization in presense of Sufficient Statistics}

We now present the \computationMechanism{} of finding tensor network decompositions of probability distributions.

\begin{definition}
    \label{def:sufStatistic}
    Let $\probat{\catvariable,\thirdcatvariable}$ be a joint distribution of the $\catdim$-dimensional variable $\catvariable$ and the $\thirdcatdim$-dimensional variable $\thirdcatvariable$ and let
    \begin{align*}
        \sstat \defcols [\catdim] \rightarrow [\headdim]
    \end{align*}
    be a statistic.
    We are interested in the distribution $\probat{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}=\contractionof{\probat{\catvariable,\thirdcatvariable},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable}}{\catvariable,\thirdcatvariable,\headvariableof{\sstat}}$.
    We say that $\sstat$ is a sufficient statistic for $\thirdcatvariable$ if and only if $\catvariable$ is independent of $\thirdcatvariable$ conditioned on $\headvariableof{\sstat}$.
\end{definition}

Note that the independence in \defref{def:sufStatistic} is true if and only if
\begin{align*}
    \condprobat{\catvariable}{\thirdcatvariable,\headvariableof{\sstat}}
    =   \condprobat{\catvariable}{\headvariableof{\sstat}} \otimes \onesat{\thirdcatvariable} \, .
\end{align*}

\input{../examples/probabilistic_paradigm/sufstat_probability}

\exaref{exa:sufStatProb} hints at a connection between sufficient statistics and decompositions into \CompActNets{}.
More generally, such decompositions are provided by the Fisher-Neyman Factorization Theorem.

\begin{theorem}[Fisher-Neyman Factorization Theorem] % See appendix of CompAct Nets paper!
    \label{the:factorizationFisherNeyman}
    Let $\probtensor$ be a joint distribution of variables $\catvariable,\thirdcatvariable$ with values $\mathrm{val}(\catvariable), \,\mathrm{val}(\thirdcatvariable)$.
    Let there further be a finite set $\mathrm{val}(\headvariableof{\sstat})$.
    Then $\sstat\defcols \mathrm{val}(\catvariable) \rightarrow \mathrm{val}(\headvariableof{\sstat})$ is a sufficient statistic for $\thirdcatvariable$ if and only if there are tensors $\basemeasureat{\catvariable}$ and $\acttensorat{\headvariableof{\sstat},\thirdcatvariable}$ such that
    \begin{align*}
        \probat{\catvariable,\thirdcatvariable}
        = \contractionof{
            \acttensorat{\headvariableof{\sstat},\thirdcatvariable},\bencodingofat{\sstat}{\headvariableof{\sstat},\catvariable},\basemeasureat{\catvariable}
        }{\catvariable,\thirdcatvariable} \, .
    \end{align*}
    We depict this equation diagrammatically by
    \begin{center}
        \input{../tikz_pics/probability_representation/computation_decomposition.tikz}
    \end{center}
\end{theorem}
\begin{proof}
    Shown in more generality in the appendix, see \theref{the:generalFactorizationFisherNeyman}.
\end{proof}

%%
Notice, that the definition of sufficient statistic does not make use of the marginal distribution $\probat{\thirdcatvariable}$.
We therefore can define sufficient statistics also for families of distributions $\condprobat{\catvariable}{\thirdcatvariable}$, with respect to arbitrary non-degenerate marginal distribution $\probat{\thirdcatvariable}$.
We then use the \theref{the:factorizationFisherNeyman} to embed such families in \CompActNets{}.

\begin{corollary}
    Let $\condprobof{\shortcatvariables}{\thirdcatvariable}$ be an arbitrary family of distributions of $\shortcatvariables$, and $\sstat$ a sufficient statistic for $\thirdcatvariable$.
    Then there is a tensor $\basemeasurewith$ and a activation tensors $\acttensorat{\headvariables,\thirdcatvariable}$ such that for any $\thirdcatindex\in\mathrm{val}(\thirdcatvariable)$
    \begin{align*}
        \condprobat{\shortcatvariables}{\thirdcatvariable=\thirdcatindex} \in\cansof{\sstat,\maxgraph,\basemeasure} \, .
    \end{align*}
\end{corollary}

\input{../examples/probabilistic_paradigm/coin_toss_ft}

The Fisher-Neyman Theorem is the fundamental motivation for the \CompActNets{} Architecture:
\begin{itemize}
    \item The decomposition of $\acttensorwith$ is called \emph{activation network}.
    \item The decomposition of $\bencodingofat{\sstat}{\headvariables,\shortcatvariables}$ is called \emph{computation network}.
\end{itemize}

\input{../examples/probabilistic_paradigm/graphical_models_as_cans}

\subsection{Exponential families in case of elementary activation tensors}

% Universal properties
A classical theorem by Pitman-Koopman-Darmois (see \cite{pitman_sufficient_1936}) states, that whenever a family with constant support and a finite sufficient statistic for arbitrary large data sets is in an exponential family.
We now restrict the activation cores to specific elementary tensors, which correspond with further assumptions on the dependence of $\probtensor$ and $\sstat$ made by exponential families.
For a discussion of further universal properties of exponential families, such that the existence of priors and entropy maximizers, see \cite{murphy_probabilistic_2022}.

\begin{definition}[Exponential Family]
    %\alex{was \theref{the:expFamilyTensorRep}}
    Given a base measure $\basemeasure$ and a statistic $\sstat:\facstates\rightarrow\parspace$ we enumerate for each coordinate $\selindexin$ the image $\imageof{\sstatcoordinateof{\selindex}}$ by an interpretation map
    \begin{align*}
        \indexinterpretationof{\selindex} \defcols
        [\cardof{\imageof{\sstatcoordinateof{\selindex}}}] \rightarrow \imageof{\sstatcoordinateof{\selindex}} \, .
    \end{align*}
    For any canonical parameter vector $\canparamwithin$ we build the activation cores $\softactlegwith$ for each coordinate $\headindexof{\selindex}\in[\cardof{\imageof{\sstatcoordinateof{\selindex}}}]$ by
    \begin{align*}
        \softactlegat{\indexedheadvariableof{\selindex}}
        = \expof{\canparamat{\indexedselvariable} \cdot \indexinterpretationofat{\selindex}{\headindexof{\selindex}}} \,
    \end{align*}
    and define the distribution % Could do parametrization to thirdcatvariable to make connection with the family definition..
    \begin{align*}
        \expdistwith =
        \normalizationof{\{\basemeasurewith\} \cup \{\bencodingofat{\sstatcoordinateof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}\cup\{\softactlegwith \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
    We then call the tensor $\probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta}$ with $\valof{\Theta}=\parspace$ and slices for $\canparamin$ by
    \begin{align*}
        \probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}
        = \expdistwith
    \end{align*}
    the exponential family to the statistic $\sstat$ and the base measure $\basemeasure$.
\end{definition}

%% In Elementary Activations
Note that by construction each member of an exponential family is an element in a \CompActNet{} with elementary activation cores, that is
\begin{align*}
    \probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}
    \in \cansof{\sstat,\elgraph,\basemeasure} \, .
\end{align*}

\begin{figure}
    \begin{center}
        \input{../tikz_pics/probability_representation/expdist_elementary}
    \end{center}
    \caption{
        Tensor Network diagram of a member of an exponential family $\probfamilyofat{\sstat,\basemeasure}{\shortcatvariables}{\Theta=\canparam}$ before normalization as an \CompActNet{} with elementary activation, that is an element in $\cansof{\sstat,\elgraph,\basemeasure}$.}\label{fig:expdistElementary}
\end{figure}

%% NEEDED?
\input{../examples/probabilistic_paradigm/two_boolean_cascade}

\subsection{Efficient Contractions by Message Passing}

\alex{Contractions of tensor networks are generally hard to solve.
We here investigate message passing algorithms, which decompose contractions into a sequence of subcontractions, which are passed as messages through the tensor network.}
The resulting algorithm is called Belief Propagation \algoref{alg:treeBeliefPropagation}.
We denote $\dirovedges$ to be all tuples $(\sedge,\redge)$ of hyperedges $\sedge,\redge\in\edges$ such that $\sedge\neq\redge$ and $\sedge\cap\redge\neq\varnothing$.

%\begin{algorithm}[hbt!]
%    \caption{Belief Propagation}\label{alg:treeBeliefPropagation}
%    \begin{algorithmic}
%        \Require Tensor network $\extnet$ on a hypergraph $\graph$
%        \Ensure Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
%        \iosepline
%        \State Initialize $\scheduler$ (differs in implementation)
%        \State Initialize messages
%        \While{$\scheduler$ not empty}
%            \State Take a $(\sedge,\redge)$ pair from $\scheduler$
%            \State Update the message
%            \begin{align*}
%                \messagewith
%                = \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
%                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
%                }{\catvariableof{\sedge\cap \redge}}
%            \end{align*}
%            \State Update $\scheduler$ (differs in implementation)
%        \EndWhile
%        \State \Return Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
%    \end{algorithmic}
%\end{algorithm}

\begin{algorithm}[hbt!]
    \caption{Tree Belief Propagation}\label{alg:treeBeliefPropagation}
    \begin{algorithmic}
        \Require Tensor network $\extnet$ on a hypergraph $\graph$
        \Ensure Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
        \iosepline
        \State Initialize $\scheduler=\{(\secsedge,\sedge) \wcols \secsedge \quad\text{a leaf in } \graph \}]$
%        \State Initialize empty
        \While{$\scheduler$ not empty}
            \State Pop a $(\sedge,\redge)$ pair from $\scheduler$
            \State Compute the message
            \begin{align*}
                \messagewith
                = \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
                }{\catvariableof{\sedge\cap \redge}}
            \end{align*}
            \State Update $\scheduler$ by all messages $(\redge,\thirdsedge)$ which have not yet been sent, if all messages $(\secsedge,\redge)$ to $\secsedge\neq\thirdsedge$ have been sent.
        \EndWhile
        \State \Return Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{the:treeBeliefPropagationExactness}
    The messages in the tree belief propagation are contracted to local marginals, that is for each $\sedge\in\edges$ we have
    \begin{align*}
        \contractionof{\extnet}{\catvariableof{\sedge}}
        \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}\cup
            \{\mesfromtowith{\secsedge}{\sedge} \wcols (\secsedge,\sedge)\in\dirovedges\}}{\catvariableof{\edge}} \, .
    \end{align*}
\end{theorem}

We show \theref{the:treeBeliefPropagationExactness} based on the following lemma.
We denote for each pair $(\sedge,\redge)$ the subset $\preedgeset\subset\edges$ as the subset of edges $\edgein$, for which each path to $\redge$ passes through $\sedge$.
Note, that by construction $\sedge\in\preedgeset$.

\begin{lemma}
    \label{lem:messageAsContraction}
    For any tensor network on a tree hypergraph, \algoref{alg:contractionPropagation} terminates in the tree-based implementation and returns final messages
    \begin{align*}
        \messagewith
        = \contractionof{\{\edgehypercorewith\wcols\edge\in\preedgeset\}}{\catvariableof{\sedge\cap\redge}}
    \end{align*}
\end{lemma}
\begin{proof}
    We show this property by induction over the edge sets $\preedgeset$ to pairs $(\sedge,\redge)\in\dirovedges$, such that $\cardof{\preedgeset}\leq n$.
    Notice, that since always $\sedge\in\preedgeset$ we have $n\geq1$.

    $n=1$: In this case we have $\preedgeset=\{\sedge\}$ and $\sedge$ is a leaf of the tree-hypergraph $\graph$.
    The claimed message property holds thus by definition.

    $n\rightarrow n+1$: Let us assume, that the message obeys the claimed property for edge sets with cardinality up to $n$.
    If there is no edge set with cardinality $n+1$, the property holds also for those with cardinality up to $n+1$.
    If there is an edge set $\preedgeset$ with size $n+1$, we have
    \begin{align*}
        \preedgeset = \{\sedge\} \cup \left(\bigcup_{\secsedge \in\dirovedges} \preedgesetwrt{\secsedge}{\sedge}\right) \, .
    \end{align*}
    The message $\mesfromto{\sedge}{\redge}$ is sent, once all messages $\mesfromto{\secsedge}{\sedge}$ to $(\secsedge,\sedge)\in\dirovedges\{(\redge,\sedge)\}$ arrived.
    By definition we have
    \begin{align*}
        \mesfromtowith{\sedge}{\redge}%{\catvariableof{\sedge\cap \redge}}
        = \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
            \cup \{\mesfromtowith{\secsedge}{\sedge} \wcols (\secsedge,\sedge) \in\dirovedges \ncond \secsedge \neq \redge \}
        }{\catvariableof{\sedge\cap \redge}}
    \end{align*}
    Now we use the induction assumption on $\preedgesetwrt{\secsedge}{\sedge}$ (since its cardinality is at most $n$) and get
    \begin{align*}
        \mesfromtoat{\sedge}{\redge}{\catvariableof{\sedge\cap \redge}}
        &= \contractionof{
            \{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\} \cup
            \left(\bigcup_{(\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge}
                \contractionof{\{\hypercoreofat{\thirdsedge}{\catvariableof{\thirdsedge}} \wcols  \thirdsedge \in \preedgesetwrt{\secsedge}{\sedge}\}}{\catvariableof{\secsedge\cap \sedge}} \right)
        }{\catvariableof{\sedge\cap \redge}} \\
        &= \contractionof{
            \{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\} \cup
            \left(\bigcup_{(\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge} \{\hypercoreofat{\thirdsedge}{\catvariableof{\thirdsedge}} \wcols  \thirdsedge \in \preedgesetwrt{\secsedge}{\sedge}\} \right)
        }{\catvariableof{\sedge\cap \redge}} \\
        &= \contractionof{\{\edgehypercorewith\wcols\edge\in\preedgeset\}}{\catvariableof{\sedge\cap\redge}}
    \end{align*}
    Here we used the commutation of contraction property in the second equation, which is justified by the assumed tree property of the hypergraph.
    Thus, the message property holds also for any edge sets of size $n+1$.

    By induction, the claimed message property therefore holds for all final messages.
\end{proof}

\begin{proof}[Proof of \theref{the:treeBeliefPropagationExactness}]
    Since the hypergraph is by assumption a tree, we can partition $\edges$ into disjoint subsets $\{\sedge\}$ and $\preedgesetwrt{\secsedge}{\sedge}$ for $(\secsedge,\sedge)\in\dirovedges$.
    We then have
    \begin{align*}
        \contractionof{\extnet}{\catvariableof{\sedge}}
        =& \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
            \cup \left\{\contractionof{\hypercoreofat{\edge}{\catvariableof{\edge}}\wcols\edge\in\preedgesetwrt{\secsedge}{\sedge}}{\catvariableof{\edge\cap\secsedge}} \wcols (\secsedge,\sedge)\in\dirovedges \right\}
        }{\catvariableof{\sedge}} \\
        =& \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\} \cup \{\mesfromtowith{\secsedge}{\sedge} \wcols (\secsedge,\sedge)\in\dirovedges\}}{\catvariableof{\sedge}} \, .
    \end{align*}
    Here we used \lemref{lem:messageAsContraction} in the second equation.
\end{proof}

% Example by Student Markov Network
We examplify the usage of \algoref{alg:treeBeliefPropagation} on the Markov Network of \exaref{exa:studentHC}.

\input{../examples/probabilistic_paradigm/student_bp}