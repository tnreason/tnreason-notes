%The unification of the neural and the symbolic paradigms of artificial intelligence is a long-standing challenge.
The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge.
In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different paradigms in tensor decompositions.
In particular, we describe a basis encoding scheme for functions and model neural decompositions by tensor decompositions.
Furthermore, the proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions.
This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes.
The framework enables the definition and training of hybrid logical and probabilistic models, which we call \HybridLogicNetworks{}.
%We further demonstrate the concepts in the accompanying python library \tnreason{}.
The theoretical concepts are accompanied by the \python{} library \tnreason{}, which enables the implementation and practical use of the proposed architectures.

%% Alternative Sentences:

%The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge.
%In this work, we introduce a tensor-network architecture that provides a unified mathematical framework for both paradigms. \alex{We say that there are three paradigms}
%A common treatment of sparsity is enabled through compositionality. \alex{There are more sparsity principles than compositionality}
%At the core of the framework is a basis-encoding scheme representing functions as tensors, which is applied to sufficient statistics and propositional formulas.
%The framework enables the definition and training of hybrid logical and probabilistic models, which we call \HybridLogicNetworks{}, combining hard logical constraints with soft probabilistic activations in a single, interpretable model.
%The framework subsumes graphical models, exponential families, and symbolic knowledge bases.
%Function evaluation and entailment decisions in propositional logic are expressed as tensor contractions.
%They can be tackled by message passing algorithms based on local contractions for large networks.

