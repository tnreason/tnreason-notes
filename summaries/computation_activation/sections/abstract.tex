The unification of the neural and the symbolic paradigms of artificial intelligence is a long-standing challenge.
We in this work introduce a tensor-network formalism, which captures sparsity principles originating in the different paradigms in tensor decompositions.
In particular, we describe a basis encoding scheme for functions and model neural decompositions by tensor decompositions.
This unified treatment identifies tensor-network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logics, as contraction message passing schemes.
The framework enables the definition and training of hybrid logical and probabilistic models, which we call \HybridLogicNetworks{}.
We further demonstrate the concepts in the accompanying python library \tnreason{}.

%We introduce \ComputationActivationNetworks{} (\CompActNets{}), a novel architecture for tensor networks, which is adapted to represent propositional formulas and exponential distributions.

%This work introduces the novel \ComputationActivationNetworks{} (\CompActNets{}), a tensor-network architecture that can represent probabilistic modeling and logical reasoning within a single mathematical framework.
%Propositional formulas and exponential-family distributions are unified in the resulting \HybridLogicNetworks{}.
%Computation or features, which are encoded by basis representations of sufficient statistics or logical formulas, are separated from activation, which assigns probabilistic or boolean values to these features.
%This separation yields sparse and interpretable representations that subsume graphical models, exponential-family factorizations, and logical inference as special cases.
%Probabilistic and logical queries reduce to tensor contractions, providing a common operational substrate for both reasoning paradigms.
%Altogether, \CompActNets{} offer an expressive and intrinsically explainable foundation for neuro-symbolic AI, enabling exact reasoning alongside efficient implementation for learning tasks.