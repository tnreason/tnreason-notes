\section{Notation and Basic Concepts}\label{sec:notation}

We in this section introduce our hypergraph-based tensor network formalism and define the architecture of \CompActNets{} based on this formalism.

\subsection{Tensors}
\label{sec:tensors}

Tensors are multiway arrays and a generalization of vectors and matrices to higher orders.
We will first provide a formal definition as real maps from index sets enumerating the coordinates of vectors, matrices and larger order tensors.

\begin{definition}[Tensor]
    \label{def:tensor}
    For $\atomenumeratorin$, let $\catdimof{\atomenumerator}\in\nn$ and let $\catvariableof{\atomenumerator}$ be variables taking values in $\{0,\ldots,\catdimof{\atomenumerator}-1\}$.
    A tensor $\hypercoreat{\catvariables}$ of order $\catorder$ and with leg dimensions $\catdimof{0},\dots,\catdimof{\atomorder-1}$ is defined through its coordinates
    \begin{align*}
        \hypercoreat{\indexedcatvariableof{0},\ldots,\indexedcatvariableof{\catorder-1}} \in \rr
    \end{align*}
    for index tuples
    \begin{align*}
        \catindexof{0},\ldots,\catindexof{\catorder-1} \in \facstates \, .
    \end{align*}
    Tensors $\hypercoreat{\catvariables}$, also denoted by $\hypercoreat{\shortcatvariables}$, are elements of the tensor space
    \begin{align*}
        \bigotimes_{\atomenumeratorin} \rr^{\catdimof{\atomenumerator}} \,,
    \end{align*}
    which is a linear space, enriched with the operations of coordinate wise summation and scalar multiplication.
    We call a tensor $\hypercoreat{\shortcatvariables}$ boolean, when all coordinates are in $\{0,1\}$, and positive, when all coordinates are greater than $0$.
    To ease the notation, we abbreviate sets as $[\catorder]=\{0,\ldots,\catorder-1\}$, tuples of state indices by $\catindexof{[\catorder]}=\catindexof{0},\ldots,\catindexof{\catorder-1}$ and tuples of variables by $\catvariableof{[\catorder]}=\catvariableof{0},\ldots,\catvariableof{\catorder-1}$.
\end{definition}

We here introduced tensors in a non-canonical way based on categorical variables assigned to its axis.
While coming as syntactic sugar at this point, this will allow us to define contractions without further specification of axes, based on comparisons of shared variables.
%\subsection{Continuous Variables} % Needed for families of distributions
We occasionally also allow for variables $\catvariable$ taking values in infinite sets such as $\rr$, in which case we denote the set of values to a variable by $\valof{\catvariable}$.
%Contractions are then performed by generic integrations with respect to e.g. the Lebesgue measure -> But not needed!

%This notation of tensors opposed to its notation through ordered indices as common in tensor calculus, facilitates writing down contractions along individual legs and other operations.
%Occasionally, when the categorical variables of a tensor are clear from the context, we will omit the notation of the variables. %further abbreviate $\hypercoreat{\catvariables}$ by $\hypercore$.
%\begin{example}[Trivial Tensor]
%    \label{exa:trivialTensor}
%    \alex{Maybe directly do the Dirac tensor (special case of $d=1$ is the trivial tensor) - explains the black dots.}
%    The trivial tensor
%    \begin{align*}
%        \onesat{\shortcatvariables} \in \facspace
%    \end{align*}
%    is defined by all coordinates being $1$, that is for all $\catindices\in\facstates$
%    \begin{align*}
%        \onesat{\indexedshortcatvariables} = 1 \, .
%    \end{align*}
%\end{example}

\begin{example}[Delta Tensor]\label{exa:diracDelta}
    Given a set of variables $\shortcatvariables=\catvariables$, where $\catorder\geq 1$, with identical dimension $\catdim$, the delta tensor is the element
    \begin{align*}
        \dirdeltawith \in \bigotimes_{\catenumeratorin} \rr^{\catdim}
    \end{align*}
    with coordinates
    \begin{align}
        \dirdeltaofat{[\catorder],\catdim}{\indexedshortcatvariables} =
        \begin{cases}
            1 \quad & \ifspace \catindexof{0} = \ldots = \catindexof{\catorder-1} \\
            0 & \text{else}
        \end{cases} \, .
    \end{align}
    We will depict this tensor by black dots, which will sometimes appear auxiliarly in tensor network diagrams (see e.g. \figref{fig:contraction}).
    For $\catorder=1$, the delta tensor is the trivial vector, which coordinates are constant $1$, which we denote by $\onesat{\catvariable}$.
\end{example}

\subsection{Tensor Networks and Contractions}

% Diagrammatic representation in factor graphs
We will use a standard visualization of tensors (dating back to \cite{penrose_spinors_1987}) by blocks with lines depicting the axes of the tensor.
In addition we denote to each axis of the tensor the corresponding variable $\catvariableof{\atomenumerator}$:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/hypercore}
\end{center}
Drawing on the association of variables with nodes and of tensors with hyperedges build by the assigned variables we continue with the definition of tensor networks.

\begin{definition}[Tensor Network]
    \label{def:tensorNetwork}
    Let $\graph=(\nodes,\edges)$ be a hypergraph with nodes decorated by categorical variables $\catvariableof{\node}$ with dimensions $\catdimof{\node} \in \nn$
    and hyperedges $\edge\in\edges$ decorated by core tensors
    \[ \hypercoreofat{\edge}{\catvariableof{\edge}} \in \bigotimes_{\node\in\edge}\rr^{\catdimof{\node}} \, , \]
    where we denote by $\catvariableof{\edge}$ the set of categorical variables $\catvariableof{\node}$ with $\node\in\edge$.
    Then we call the set
    \[ \tnetofat{\graph}{\catvariableof{\nodes}} = \{\hypercoreofat{\edge}{\catvariableof{\edge}}  \wcols \edge\in\edges\} \]
    the Tensor Network of the decorated hypergraph $\graph$.
    The set of tensor networks on $\graph$, such that all tensors have non-negative coordinates, is denoted by $\tnsetof{\graph}$.
\end{definition}

As examples we now present the $\cpformat$ and the $\ttformat$ formats in our hypergraph notation.

\input{../examples/notation/cp_format.tex}

\input{../examples/notation/tt_format.tex}

\subsection{Generic Contractions}

Let us now exploit our graphical approach to tensor networks in the definition of contractions. % of tensor networks as operations to get single tensors from tensor networks. % by summing slices of tensors in a network over common variables.

\begin{definition}
    \label{def:contraction}
    Let $\tnetof{\graph}$ be a tensor network on a decorated hypergraph $\graph=(\nodes,\edges)$.
    For any subset $\secnodes\subset\nodes$ we define the contraction to be the tensor (for an example see \figref{fig:contraction})
    \begin{align}
        \contractionof{\tnetof{\graph}}{\secnodevariables} \in \bigotimes_{\node\in\secnodes} \rr^{\catdimof{\node}}
    \end{align}
    defined coordinatewise by the sum
    \begin{align}
        \contractionof{\tnetof{\graph}}{\indexedcatvariableof{\secnodes}} =
        \sum_{\catindexof{\nodes/\secnodes} \in\,\nodestatesof{\nodes/\secnodes}}
        \left( \prod_{\edge\in\edges}\hypercoreofat{\edge}{\indexedcatvariableof{\edge}} \right) \, .
    \end{align}
    We call $\secnodevariables$ the open variables of the contraction.
\end{definition}

% Open variables
%We here defined contractions formally by only specifying the open variables, where the connectivity of the tensors is clear from the shared variables.
When an open variable $\catvariable$ is not appearing at any tensor in a contraction, we define the contraction as a tensor product with the trivial tensor $\onesat{\catvariable}$.
To ease notation, we will often omit the set notation by brackets $\{\cdot\}$ and specify the tensors to be contracted with the delimiter "," (see e.g. \exaref{exa:matrixProduct}).

\begin{figure}
    \begin{center}
        \input{../tikz_pics/notation_basic_concepts/contraction.tex}
    \end{center}
    \caption{
        Graphical depiction of a tensor network contraction with the open variables $\catvariableof{1},\catvariableof{3}$.
        Open variables are depicted by those without a dot at the end of the line.
    }\label{fig:contraction}
\end{figure}

\begin{example}[Tensor Product]
    The simplest contraction is the tensor product, which maps a pair of two tensors with distinct variables onto a third tensor and has an interpretation by coordinate wise products.
    Such a contraction corresponds with a tensor network of two tensors with disjoint variables.
    Let there be two tensors
    \begin{align*}
        \hypercoreat{\shortcatvariables} \in \facspace  \andspace \sechypercoreat{\secshortcatvariables} \in \secfacspace
    \end{align*}
    with different categorical variables assigned to its axes.
    Then their tensor product is the map
    \begin{align*}
        \contractionof{\hypercoreat{\shortcatvariables},\sechypercoreat{\secshortcatvariables}}{\shortcatvariables,\secshortcatvariables}
        \in \left(\facspace\right) \otimes \left(\secfacspace \right)
        %:  \left(\facstates\right) \times \left(\secfacstates\right) \rightarrow \rr
    \end{align*}
    defined coordinatewise for tuples of $\catindices\in\facstates$ and $\seccatindices\in\secfacstates$ as
    \begin{align*}
        & \contractionof{\hypercoreat{\shortcatvariables},\sechypercoreat{\secshortcatvariables}}{\indexedcatvariables,\indexedseccatvariables} \\
        &\quad\quad \coloneqq  \hypercoreat{\indexedcatvariables}\cdot \sechypercoreat{\indexedseccatvariables} \, .
    \end{align*}
\end{example}

\subsection{Normalizations}

%% Directionality by Normalization
\begin{definition}
    \label{def:normalization}
    The normalization of a tensor $\hypercorewithnodes$ on incoming nodes $\innodes\subset\nodes$ and outgoing nodes $\outnodes\subset\nodes/\innodes$ is the tensor $\normalizationofwrt{\hypercorewithnodes}{\catvariableof{\outnodes}}{\catvariableof{\innodes}}$ defined for $\catindexof{\innodes}$ as
    \begin{align*}
        \normalizationofwrt{\hypercorewithnodes}{\catvariableof{\outnodes}}{\indexedcatvariableof{\innodes}}
        = \begin{cases}
              \frac{\contractionof{\hypercore}{\catvariableof{\outnodes},\indexedcatvariableof{\innodes}}}{\contractionof{\hypercore}{\indexedcatvariableof{\innodes}}} & \ifspace \contractionof{\hypercore}{\indexedcatvariableof{\innodes}} \neq 0 \\
              \frac{1}{\prod_{\node\in\outnodes}\catdimof{\node}}\onesat{\catvariableof{\outnodes}} & \text{else}
        \end{cases} \, .
    \end{align*}
    We say that $\hypercorewithnodes$ is normalized with incoming nodes $\incomingnodes\subset\nodes$, if
    \begin{align*}
        \hypercorewithnodes
        = \normalizationofwrt{\hypercorewithnodes}{\catvariableof{\nodes/\innodes}}{\catvariableof{\innodes}} \, .
    \end{align*}
\end{definition}

%% Diagrammatic notation
In our graphical tensor notation, we depict normalized tensors by directed hyperedges (a), which are decorated by directed tensors (b), for example:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/directed_core.tex}
\end{center}

\subsection{Function encoding and \ComputationActivationNetworks{}}

Towards presenting the function encoding schemes we define one-hot encodings mapping the states of variables to basis tensors.

\begin{definition}[One-hot encodings to Factored Representations]
    \label{def:onehotenc}
    To any variable $\catvariable$ taking values in $[\catdim]$ the one-hot encoding of any state $\catindex\in[\catdim]$ is the vector with coordinates
    \begin{align}
        \onehotmapofat{\catindex}{\catvariable=\tilde{\catindex}}
        \coloneqq \begin{cases}
                      1 & \ifspace \catindex=\tilde{\catindex} \\
                      0 & \text{else} \, .
        \end{cases}
    \end{align}
    To any tuple $(\catvariables)$ of variables taking values in $\facstates$ the one-hot encoding of a state tuple $\shortcatindices=(\catindexof{0},\ldots,\catindexof{\catorder-1})$ is the tensor product
    \begin{align*}
        \onehotmapofat{\shortcatindices}{\shortcatvariables}
        \coloneqq \bigotimes_{\catenumeratorin} \onehotmapofat{\catindexof{\atomenumerator}}{\catvariableof{\atomenumerator}} \, .
    \end{align*}
\end{definition}

%The basis vectors $\onehotmapofat{\catindex}{\catvariable}$ are tensors of order $1$ and leg dimension $\catdim$ of the structure
%\begin{align}
%    \onehotmapofat{\catindex}{\catvariable}
%    = \begin{bmatrix}
%          0 & \cdots & 0 & 1 & 0 & \cdots & 0
%    \end{bmatrix}^T \, ,
%\end{align}
%where the $1$ is at the $\catindex$-th coordinate of the vector.

We now use one-hot encodings to encode functions between state sets.

\begin{definition}[Basis encoding of maps between state sets]
    \label{def:functionRepresentation}
    Let there be two systems with factored representations by variables $\shortcatvariables$ and $\headvariables$, and a map
    \begin{align*}
        \statesetfunction\defcols\facstates \rightarrow  \headstates
    \end{align*}
    between these state sets.
    Then the basis encoding of $\statesetfunction$ is a tensor
    \begin{align*}
        \bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables}
        \in \left(\secfacspace\right) \otimes \left(\facspace\right)
    \end{align*}
    defined by
    \begin{align*}
        & \bencodingofat{\statesetfunction}{\headvariables,\shortcatvariables}
        \quad = \sum_{\catindices\in\facstates}
        \onehotmapofat{\statesetfunctionev{\shortcatindices}}{\headvariables} \otimes  \onehotmapofat{\shortcatindices}{\shortcatvariables} \, .
    \end{align*}
\end{definition}
Basis encodings are normalized tensors and are thus depicted as decorations of directed edges in hypergraphs:
\begin{center}
    \input{../tikz_pics/notation_basic_concepts/bencoding}
\end{center}

% Image enumeration
We further generalize basis encodings to arbitrary functions between finite sets, by the use of image enumeration maps.
Given an arbitrary set $\arbset$ we say a map
\begin{align*}
    \indexinterpretation \defcols
    [\cardof{\arbset}] \rightarrow \arbset
\end{align*}
is an enumeration map for $\arbset$.
Given a function $\exfunction:\inset\rightarrow\outset$ between arbitrary sets, and enumerating maps $\indexinterpretationof{\insymbol}$ and $\indexinterpretationof{\outsymbol}$ for both sets we define the basis encoding of $\exfunction$ as
\begin{align*}
    \bencodingofat{\exfunction}{\indvariableof{\outsymbol},\indvariableof{\insymbol}}
    = \sum_{\arbelement\in\inset} \onehotmapofat{\invindexinterpretationofat{\outsymbol}{\exfunction(\arbelement)}}{\indvariableof{\outsymbol}}
    \otimes \onehotmapofat{\invindexinterpretationofat{\insymbol}{\arbelement}}{\indvariableof{\insymbol}} \, ,
\end{align*}
where $\indvariableof{\insymbol},\indvariableof{\outsymbol}$ are variables taking values in $[\cardof{\inset}]$ and $[\cardof{\outset}]$.
%
Based on these concepts the main architecture can be defined.

\begin{definition}[\ComputationActivationNetwork{} (\CompActNets{})]
    \label{def:realizableStatDistributions}
    Given a function $\sstat : \facstates \rightarrow \parspace$, and a hypergraph $\graph=(\nodes,\edges)$ with nodes $[\seldim]\subset\nodes$ containing the image coordinates of $\sstat$, we define the by $\sstat$ computable and by $\graph$ activated family of distributions by
    \begin{align*}
        \realizabledistsof{\sstat,\graph}
        = \left\{ \normalizationof{\bencodingofat{\sstat}{\headvariables,\shortcatvariables},\contractionof{\acttensor}{\headvariables} %\} \cup \{\hypercoreofat{\edge}{\sstatcatof{\edge}} \wcols \edgein \}
        }{\shortcatvariables}
              \wcols \acttensorat{\headvariableof{\nodes}} \in \tnsetof{\graph} \right\} \, .
    \end{align*}
    We refer to any member $\probat{\shortcatvariables}\in\realizabledistsof{\sstat,\graph}$ as a \emph{\ComputationActivationNetwork{}} (\emph{\CompActNet{}}).
    We call $\bencsstatwith$ (and any decomposition of it) as the \emph{computation network} and by $\acttensorat{\headvariableof{\nodes}}$ as the \emph{activation network}.
\end{definition}