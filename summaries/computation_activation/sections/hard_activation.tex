\section{Decompositions based on Propositional Syntax}\label{sec:log_rep}

A tensor-based representation of propositional logic is developed by encoding boolean variables into vectors, defining formulas as boolean tensors, and showing how logical connectives and normal forms can be expressed as tensor contractions.

\subsection{Propositional Semantics by Boolean Tensors}

%We define
%\begin{itemize}
%    \item a \emph{propositional formula} as a boolean-valued tensor
%    \begin{align*}
%        \formulaat{\shortcatvariables} \defcols \atomstates \rightarrow \ozset \subset \rr \, ,
%    \end{align*}
%    \item a \emph{model} of a propositional formula as a state $\shortcatindices \in \atomstates$, which fulfills
%    $% \begin{align*}
%    \formulaat{\indexedshortcatvariables}=1 \, ,
%    $% \end{align*}
%    where we associate $\text{True}\leftrightarrow 1$ and $\text{False}\leftrightarrow 0$,
%    \item and the propositional formula to be \emph{satisfiable}, if there is a model.
%\end{itemize}

\begin{definition}
    \label{def:formulas}
    A \emph{propositional formula} $\formulaat{\shortcatvariables}$ depending on $\atomorder$ boolean variables $\catvariableof{\atomenumerator}$ is a boolean-valued tensor
    \begin{align*}
        \formulaat{\shortcatvariables} \defcols \atomstates \rightarrow \ozset \subset \rr \, .
    \end{align*}
    We call a state $\shortcatindices \in \atomstates$ a \emph{model} of a propositional formula $\formula$, if
    \begin{align*}
        \formulaat{\indexedshortcatvariables}=1 \, ,
    \end{align*}
    where we associate $\mathrm{True}\leftrightarrow 1$ and $\mathrm{False}\leftrightarrow 0$.
    If there is a model to a propositional formula, we say the formula is \emph{satisfiable}.
\end{definition}

\input{../examples/hard_activation/prop_formula_coordinatewise.tex}

%\begin{example}
%    \label{ex:propform}
%    The propositional formula defined for $d=3$ and $x_{[3]} = (\catvariableof{0},\catvariableof{1},\catvariableof{2})\in \atomstates = \{0,1\}\times \{0,1\} \times \{0,1\}$ by
%    \begin{align*}
%        \formulaat{\shortcatvariables = x_{[3]}} = \catvariableof{0} \wedge (\catvariableof{1} \vee \catvariableof{2})
%    \end{align*}
%    is satisfiable, since $\formulaat{\shortcatvariables = (1,1,1)} = 1$, $\formulaat{\shortcatvariables = (1,0,1)} = 1$, and $\formulaat{\shortcatvariables = (1,1,0)} = 1$ and therefore $x=(1,1,1)$, $x=(1,0,1)$, and $x=(1,1,0)$ are models of $\formulaat{\shortcatvariables}$.
%\end{example}

\paragraph{CP decomposition}
Since the tensor $\formulaat{\shortcatvariables}$ is equal to one at index $x_{[d]}$ if and only if $x_{[d]}$ is a model of $\formula$, i.e. fulfills the formula, a propositional formula can be written as the sum over the one-hot encodings of its models.
\begin{center}
    \input{../tikz_pics/logic_representation/formula_direct.tex}
\end{center}
This decomposition corresponds to the CP decomposition of a tensor.

\input{../examples/hard_activation/prop_formula_basCP.tex}

\paragraph{Model counts by contraction}
Each coordiante of the propositional formula is either a $1$ or $0$ encoding if the indexed state is a model of the formula or not.
In this way, the contraction $\contraction{\formula}$ counts the number of models of the propositional formula $\formula$.
One can therefore decide the satisfiability of a formula by checking if $\contraction{\formula}>0$.

\paragraph{Basis encoding}
Representing booleans by elements in $\{0,1\}$ leads to the problem, that negation is an affine transformation and can not be represented by multilinear tensors. %~\cite[Section 4.1.1]{goessmann_tensor-network_2025}.
Therefore, instead of using this \emph{coordinate calculus} an approach based on \emph{basis calculus} is employed, which is explained in this section.
To be able to express different kinds of connectives and finally any propositional formula by multi-linear tensors, booleans are encoded by one-hot encodings as defined in \defref{def:onehotenc}.
Propositional formulas $\formula$ can be expressed in terms of a tensor describing the mapping and its negation by
\begin{align}
    \label{eq:basisencboolean}
    \bencodingofat{\formula}{\indexedheadvariableof{\formula},\indexedshortcatvariables}
    = \begin{cases}
          1 & \ifspace \formulaat{\indexedshortcatvariables} = \headindexof{\formula}\\
          0 & \text{else}
    \end{cases}.
\end{align}
This basis encoding $\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables} \in \{0,1\}^{2\times 2^d}$ then has the form
\begin{align}
    \label{eq:basisencnegsum}
    \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
    = \tbasisat{\headvariableof{\formula}} \otimes \formulaat{\shortcatvariables}
    + \fbasisat{\headvariableof{\formula}} \otimes \lnot\formulaat{\shortcatvariables} \, .
\end{align}
In our graphical notation this property is visualized by
\begin{center}
    \input{../tikz_pics/logic_representation/formula_bencoding.tex}
\end{center}
We further provide a more detailed example in coordinate sensitive notation in the following.
\input{../examples/hard_activation/bencoding_neg_con.tex}

\paragraph{Interpretation as \CompActNets{}}
The propositional formula and its negation can be represented by that tensor by
\begin{align*}
    \formulaat{\shortcatvariables}
    = \contractionof{\tbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables}
    \andspace
    \lnot\formulaat{\shortcatvariables}
    = \contractionof{\fbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables} \, .
\end{align*}
Both $\formula$ and $\lnot\formula$ are thus \ComputationActivationNetworks{} to the statistic $\{\formula\}$ and the hard activation tensor $\tbasisat{\headvariableof{\formula}}$, respectively $\fbasisat{\headvariableof{\formula}}$.
This representation of propositional formulas with respect to basis encoding thus leads to \ComputationActivationNetworks{}, which were also used to describe probability distributions in the last section.
In this way the soft and hard logic can be combined in one framework.

\subsection{Decomposition of Propositional Formulas}

We now show, that the propositional formula allows for a decomposition into connective formulas, its basis encoding decomposes into the basis encodings of the connective formulas.

\begin{lemma}
    \label{lem:formulaDecomp}
    Let $\formulaat{\shortcatvariables}$ be a composition of a $\seldim$-ary connective formula $\exconnective$ and propositional formulas $\formulaofat{\selindex}{\shortcatvariables}$, where $\selindexin$, i.e. for $\shortcatindices\in\atomstates$ we have
    \begin{align*}
        \formulaat{\indexedshortcatvariables}
        = \exconnective\left(\formulaofat{0}{\indexedshortcatvariables}, \dots, \formulaofat{\seldim-1}{\indexedshortcatvariables}\right) \, .
    \end{align*}
    Then we have
    \begin{align*}
        \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
        = \contractionof{
            \{\bencodingofat{\exconnective}{\headvariableof{\formula},\headvariableof{[\seldim]}}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}
        }{\headvariableof{\formula},\shortcatvariables} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    This can be shown on each index $\shortcatindices$.
\end{proof}

For the composition of two propositional formulas $\formulaat{\shortcatvariables}$ and $\secexformula\left[\shortcatvariables\right]$ the composition by some binary connective is pictured by:
\begin{center}
    \input{../tikz_pics/logic_representation/unary_binary_composition}
\end{center}

Let us now define a more generic syntactical decomposition of propositional formulas.

\begin{definition}
    \label{def:formulaDecomposition}
    A syntactical hypergraph is a directed acyclic hypergraph $\graph=(\nodes,\edges)$ such that
    \begin{itemize}
        \item each hyperedge $\edge=(\incomingnodes,\outgoingnodes)$ has exactly one outgoing node, i.e. $\cardof{\outgoingnodes}=1$
        \item each node $\nodein$ carries a boolean variable $\headvariableof{\node}$ and appears at most once as the outgoing node of a hyperedge % well-definedness
        \item each hyperedge $(\incomingnodes,\{\node\})$ with $\incomingnodes\neq\varnothing$ is decorated by a propositional formula
        \begin{align*}
            \connectiveofat{\node}{\headvariableof{\incomingnodes}} \defcols \bigtimes_{\node\in\incomingnodes} [2] \rightarrow [2]
        \end{align*}
        \item the node not appearing as an outgoing node are labeled by $[\atomorder]$
    \end{itemize}
    We say that the syntactical hypergraph is single-rooted, if exactly one node $\secnode$ does not appear as an incoming node of a hyperedge.
    In this case this unique node is called the root node. % head node
    We assign atomic formulas to the nodes $[\atomorder]$ and recursively assign to each further node $\node$ a node formula % connective $\connectiveofat{\node}{\headvariableof{\incomingnodes}}$.
    \begin{align*}
        \formulaofat{\node}{\indexedshortcatvariables}
        = \connectiveofat{\node}{[\formulaofat{\thirdnode}{\indexedshortcatvariables}\wcols\thirdnode\in\incomingnodes]} \quad \forall\shortcatindicesin\, ,
    \end{align*}
    where $\incomingnodes$ are the incoming nodes in the unique hyperedge with outgoing nodes $\{\node\}$.
    We call the formula $\exformulaat{\shortcatvariables}\coloneqq\formulaofat{\secnode}{\shortcatvariables}$ to the root note $\secnode$ the syntactical composition of $\graph$ and $\graph$ is a syntactical decomposition of $\exformula$.
\end{definition}

\begin{theorem}
    \label{the:formulaDecompositionRep}
    For any syntactical hypergraph $\graph$ with composition $\exformula$ we have
    \begin{align*}
        \exformulaat{\shortcatvariables}
        = \breakablecontractionof{
            &\left\{
                 \bencodingofat{\connectiveof{\node}}{\headvariableof{\node},\headvariableof{\incomingnodes}} \wcols (\incomingnodes,\{\node\})\in\edges
            \right\} \cup \\
            & \{\identityat{\headvariableof{\atomenumerator},\catvariableof{\atomenumerator}} \wcols \atomenumeratorin\}
            \cup \{\tbasisat{\headvariableof{\secnode}}\}
        }{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    One can show this theorem by induction over the node formulas of the syntactical hypergraph, from the leafs to the root and iteratively applying \lemref{lem:formulaDecomp}.
\end{proof}

Thus we have a tensor network representation of any propositional formula based on its syntactical decomposition, where the hypergraph of the syntactical decomposition equals the hypergraph of the representing tensor network.

\subsection{Contractions to decide entailment}

We have already seen that the contraction of a propositional formula counts its models.
This allows to define entailment between two propositional formulas as follows.

\begin{definition}[Entailment of propositional formulas]
    \label{def:logicalEntailment}
    Given two propositional formulas $\kb$ and $\exformula$ we say that $\kb$ entails $\exformula$, denoted by $\kb\models\exformula$, if any model of $\kb$ is also a model of $\exformula$, that is
    \begin{align*}
        \contraction{\kb,\lnot\exformula}=0 \, .
    \end{align*}
    If $\kb\models\lnot\exformula$ holds (i.e. $\contraction{\kb,\exformula}$=0), we say that $\kb$ contradicts $\exformula$.
\end{definition}

% Relation to classical definition of entailment
Classically (see e.g. \cite{russell_artificial_2021}) entailment in propositional logics is defined as the the unsatisfiability of $\kb\land\lnot\exformula$.
This is equivalent to \defref{def:logicalEntailment}, since $\contraction{\kb,\lnot\exformula}=0$ is equivalent to $\contraction{\kb \land (\lnot\exformula)}=0$, which is the unsatisfiability of $\kb\land\lnot\exformula$.

Entailment is the central operation of "logical inference", i.e. deduce true statements from known statements.
In the tensor network representation, these entailments can be decided by contracting the whole representing tensor with the statement, that needs to be checked.

\input{../examples/hard_activation/sudoku_entailment}

\subsection{Efficient Representation of Knowledge Bases}

We now investigate the representation of knowledge bases, which are conjunctions
\begin{align*}
    \kbwith
    = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables} \, .
\end{align*}
To show efficient repesentations we will use the following identities.

\begin{lemma}[Computation Network Symmetries]
    \label{lem:comNetSymmetries}
    We have for the $\catorder$-ary $\land$-connective (where $\catorder\in\nn$) and the unary $\lnot$-connective that
    \begin{align*}
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\land}{\headvariable,\shortcatvariables}}{\shortcatvariables}
        =
        \bigotimes_{\catenumeratorin} \tbasisat{\catvariableof{\catenumerator}}
        \andspace
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\lnot}{\headvariable,\catvariable}}{\catvariable}
        =
        \fbasisat{\catvariable} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    Follows directly from the definitions of the basis encodings and the connectives.
\end{proof}

\input{../examples/hard_activation/prop_formula_kb_head_sym}

We use this to decompose knowledge bases into their individual formulas as follows.

\begin{theorem}\label{the:kbDecomposition}
    For any knowledge base $\kbwith = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables}$ it holds that
    \begin{align*}
        \kbwith
        = \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    With \lemref{lem:comNetSymmetries} we have
    \begin{align*}
        \kbwith
        &= \contractionof{\{\tbasisat{\headvariableof{\land}},\bencodingofat{\land}{\headvariableof{\land},\headvariables}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{
            \bigcup_{\selindexin} \{\tbasisat{\headvariableof{\selindex}},\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{proof}



\input{../examples/hard_activation/sudoku_entailment}

% BAD NOTATION!
%Noting that for example for $x_a=x_b=\epsilon_1=[0,1]^\intercal$
%\begin{center}
%    \input{../tikz_pics/logic_representation/and_decomposition}
%\end{center}
%while for all other vectors $x_a,x_b$, all parts of the equations amount to $0$. This yields that a knowledge base consisting of multiple formulas connected by a $\land$ has an efficient representation by decomposing the the tensor into its individual formulas.
%

\subsection{Message-passing for Entailment}

% Infeasible constractions
Since contracting the whole tensor is often infeasible and for instance for the Sudoku example would correspond to solving the whole problem, local contractions can be considered to decide in some cases.
Here a local contraction describes the calculation of contractions along few closely connected legs in the tensor network. Now, if the local contraction of any legs leads to a zero-tensor in the network decomposition, the whole contraction amounts to zero, and the knowledge base entails $f$.

\begin{theorem}[Monotonicity of Propositional Logics]
    \label{the:monotonicityPL}
    If $\seckb\subset\kb$ and $\seckb\models\formula$ then also $\kb\models\formula$.
\end{theorem}
\begin{proof}
    Since $\seckb\models\formula$ it holds that $\contraction{\seckb,\lnot\formula}=0$ and thus  $\contractionof{\seckb,\lnot\formula}{\shortcatvariables}=\zerosat{\shortcatvariables}$.
    Denoting by $\kb/\seckb$ the conjunctions of formulas in $\kb$ not in $\seckb$, we have
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}
        &= \contraction{\kb/\seckb[\shortcatvariables],\seckb,\lnot\formulawith} \\
        &= \contraction{\kb/\seckb[\shortcatvariables],\contractionof{\seckb[\shortcatvariables],\lnot\formulawith}{\shortcatvariables}} \\
        &= \contraction{\kb/\seckb[\shortcatvariables],\zerosat{\shortcatvariables}} \\
        &= 0 \, .
    \end{align*}
\end{proof}

To decide entailment, we can therefore investigate entailment on smaller parts of the knowledge base.
This is sound by the above theorem, but not complete, since it can happen that no smaller part of the knowledge base entails the formula, but the whole knowledge base does.

We can futhermore add entailed formulas to the knowledge base without the latter, as we show next.

\begin{theorem}[Invariance of adding Entailed Formulas]
    \label{the:addingEntailed}
    If $\kb\models\formula$ then % Actually can extend to iff -> Modify the proof for that?
    \begin{align*}
        \kbwith
        = \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We use that $\formulawith+\lnot\formulawith=\onesat{\shortcatvariables}$ and thus
    \begin{align*}
        \kbwith
        &= \contractionof{\kbwith,(\formulawith+\lnot\formulawith)}{\shortcatvariables} \\
        &= \contractionof{\kbwith,\formulawith}{\shortcatvariables}  + \contractionof{\kbwith,\lnot\formulawith}{\shortcatvariables}  \\
        &= \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
\end{proof}

% Interpreting entailment
One can understand entailment as "making the knowledge base more accessible":
Adding deduced statements to a knowledge base does not change the knowledge base as a tensor, but one can interpret it in an easier way.
\theref{the:addingEntailed} justifies this intuition in our tensor network formalism.

% Constraint Propagation
This motivates an message-passing approach to decide entailment by iteratively adding entailed formulas to the knowledge base and checking entailment on smaller parts of the knowledge base.
Let us now refine \algoref{alg:beliefPropagation} to this situation.
Since we are only interested in the support of the contractions, we schedule new messages in the direction $(\sedge,\redge)$, once the support of a message received at $\sedge$ has been changed.
Note that such a scheduling system is guaranteed to converge, since there can only be a finite number of message changes.
We further directly reduce the computation of messages to their support and call the resulting \algoref{alg:constraintPropagation} Constraint Propagation.

\begin{algorithm}[hbt!]
    \caption{Constraint Propagation}\label{alg:constraintPropagation}
    \begin{algorithmic}
        \Require Tensor network $\extnet$ on a hypergraph $\graph$
        %\Ensure Scheduler $\scheduler$
        \iosepline
        \State Initialize a queue $\scheduler = \dirovedges$ of message directions
        \State Initialize messages $\messagewith = \onesat{\catvariableof{\sedge\cap\redge}}$ for $(\sedge,\redge)\in\dirovedges$
        \While{$\scheduler$ not empty}
            \State Pop a $(\sedge,\redge)$ pair from $\scheduler$
            \State Compute
            \begin{align*}
                \hypercoreat{\catvariableof{\sedge\cap \redge}}
                = \nonzeroof{\contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
                }{\catvariableof{\sedge\cap \redge}}}
            \end{align*}
            \If{$\hypercoreat{\catvariableof{\sedge\cap \redge}}\neq\messagewith$}
                \State Update the message: $\messagewith\coloneqq\hypercoreat{\catvariableof{\sedge\cap \redge}}$
                \State Add $\scheduler = \scheduler \cup \{(\redge,\secsedge) \wcols (\redge,\secsedge)\in\dirovedges\}$ % Clear?
            \EndIf
        \EndWhile
        \State \Return Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
    \end{algorithmic}
\end{algorithm}

\input{../examples/hard_activation/sudoku_message_passing.tex}