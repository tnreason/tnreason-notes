\section{\HybridLogicNetworks{}}\label{sec:hln}

Let us now exploit the common formulation of logical formulas and probabilistic models in \CompActNets{} to define hybrid models that combine both aspects.
We call \CompActNets{} \HybridLogicNetworks{} in the special case of Boolean statistics $\sstat$ and elementary activations.

\subsection{Parametrization}

We first introduce \HybridLogicNetworks{}, which can be regarded as a unification of logical and probabilistic models.

\begin{definition}[\HybridLogicNetwork{} (HLN)]
    \label{def:hybridLogicNetwork}
    Given a Boolean statistic $\hlnstat$, we call any element of $\elrealizabledistsof{\hlnstat}$ a \HybridLogicNetwork{}.
    The extended canonical parameter set for $\hlnstat$ is the set
    \begin{align*}
        \hybridparamset\coloneqq
        \{\hardparam\wcols \hardlegset\subset[\seldim]\ncond \headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]\} \times \parspace \, .
    \end{align*}
    For each \HybridLogicNetwork{} $\hlnwith$, we can associate a tuple $\hybridparam$ consisting of a subset $\hardlegset\subset[\seldim]$, a tuple $\headindexof{\hardlegset}\in\bigtimes_{\selindex\in\hardlegset}[2]$, and $\canparamwithin$ such that
    \begin{align*}
        \hlnwith
        = \normalizationof{\hlnstatccwith,\paracttensorwith}{\shortcatvariables}
    \end{align*}
    where the activation core is
    \begin{align*}
        \paracttensorwith = \contractionof{\softacttensorwith,\hardacttensorwith}{\headvariables} \, .
    \end{align*}
\end{definition}

%% Parametrization explanation
We notice that the parametrization by $\hybridparamset$ is one-to-one for any non-vanishing elementary activation tensor up to a scalar factor.
Given an arbitrary elementary activation tensor $\bigotimes_{\selindexin}\acttensorlegwith$, we can always find a corresponding tuple in $\hybridparamset$ by choosing\footnote{Here $\nonzeroof{\cdot}$ is the indicator of non-zero entries acting coordinatewise and $\onesat{\headvariableof{\selindex}}$ is the vector $[1,1]^T$.}
\begin{align*}
    \hardlegset = \{\selindex \wcols \nonzeroof{\acttensorlegwith}\neq\onesat{\headvariableof{\selindex}}\} \, ,
\end{align*}
further for all $\selindex\in\hardlegset$
\begin{align*}
    \headindexof{\selindex}
    = \begin{cases}
          0 & \ifspace \nonzeroof{\acttensorlegwith} = \onehotmapofat{0}{\headvariableof{\selindex}} \\
          1 & \ifspace \nonzeroof{\acttensorlegwith} = \onehotmapofat{1}{\headvariableof{\selindex}} \,
    \end{cases}
\end{align*}
and a parameter vector $\canparamwithin$ defined for all $\selindexin$ as
\begin{align*}
    \canparamat{\indexedselvariable} =
    \begin{cases}
        0 & \ifspace \selindex\in\hardlegset \\
        \lnof{\frac{\acttensorlegat{\headvariableof{\selindex}=1}}{\acttensorlegat{\headvariableof{\selindex}=0}}}
        & \ifspace \selindex\notin\hardlegset \, .
    \end{cases}
\end{align*}
Then we have by construction that there is $\lambda>0$ with
\begin{align*}
    \bigotimes_{\selindexin}\acttensorlegwith
    = \lambda \cdot \paracttensorwith \, .
\end{align*}

Let us demonstrate the utility of \HybridLogicNetworks{} with an example from accounting.

\input{../examples/hybrid_activation/hln_accounting_rep}

\subsection{Parameter estimation in \HybridLogicNetworks{}}\label{sec:paramEst}

%% Likelihood maximization
Let us now briefly discuss how \HybridLogicNetworks{} can be trained on data based on likelihood maximization.
Given a dataset $\dataset$ consisting of $\datanum$ independent and identically distributed samples from an unknown distribution, we want to find a \HybridLogicNetwork{} $\hlnwith$ with a statistic $\sstat=(\formulaof{0},\dots,\formulaof{\seldim-1})$ that minimizes the negative log likelihood
\begin{align*}
    \lossof{\hybridparam} \coloneqq -\frac{1}{\datanum} \sum_{\datindexin} \lnof{\probofat{\hlnparameters}{\shortcatvariables=\shortcatindices^{\datindex}}}
    \, .
\end{align*}
% We notice that this is $\infty$ if and only if there is a data point $\datindexin$ with
% \begin{align*}
%     \hlnformulaat{\shortcatvariables=\shortcatindices^{\datindex}}=0 \, ,
% \end{align*}    
% \janina{where the tuple $(A,y_A)$ denotes the hard logic part of the network, $t=(f_1,\dots,f_L)$, and
% \begin{align*}
%     \hlnformulawith =
%     \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
%     \land
%     \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
% \end{align*} (Moved from theorem in next subsection to here.)}
% If this is not the case, w
We can rewrite the loss using the empirical mean vector $\datameanwith\in\parspace$, which is defined for $\selindexin$ as
\begin{align*}
    \datameanat{\indexedselvariable}
    = \frac{1}{\datanum} \sum_{\datindexin} \formulaofat{\selindex}{\shortcatvariables=\shortcatindices^{\datindex}} \, ,
\end{align*}
by
\begin{align*}
    \lossof{\hybridparam} =
    \contraction{\datameanwith,\canparamwith} - \lnof{\contraction{\paracttensorwith,\bencsstatwith}} \, .
\end{align*}
Since $\hardparam$ influences only the second term, the best hard parameters can be found by
\begin{align*}
    \hardlegset = \{\selindex \wcols \datameanat{\indexedselvariable}\in\ozset\} \andspace
    \headindexof{\selindex} = \datameanat{\indexedselvariable} \quad \text{for} \quad \selindex\in\variableset \, .
\end{align*}
We further optimize the coordinates $\selindex\in[\seldim]\setexcept{\hardlegset}$ of $\canparamwithin$ alternately by the coordinate descent steps
\begin{align*}
    \difofwrt{\lossof{\hybridparam}}{\canparamat{\indexedselvariable}} = 0
    \Leftrightarrow
    \canparamat{\indexedselvariable}
    = \lnof{
        \frac{\meanparamat{\indexedselvariable}}{(1-\meanparamat{\indexedselvariable})}
        \cdot \frac{\hypercoreat{\headvariableof{\selindex}=0}}{\hypercoreat{\headvariableof{\selindex}=1}}
    } \, .
\end{align*}
where
\begin{align*}
    \hypercoreat{\headvariableof{\selindex}}
    = \contractionof{\{\bencodingof{\formulaof{\secselindex}} \wcols \secselindex\in[\seldim]\}
        \cup\{\softactsymbolof{\secselindex,\canparam} \wcols \secselindex\in[\seldim]\ncond\secselindex\neq\selindex\}
        \cup\{\basemeasure\}}{\headvariableof{\selindex}} \, .
\end{align*}
Based on an interpretation of the coordinate descent steps as matching steps for the mean parameters or moments to $\formulaof{\selindex}$, we call this method \emph{alternating moment matching} for \HybridLogicNetworks{} and provide pseudocode for it it in \algoref{alg:AMM_HLN}.
%% Forward inference in inner loop
We notice that, during the coordinate descent steps, computing the marginal probability of the variable $\headvariableof{\selindex}$ with respect to the current network parameters is required.
This is the computational bottleneck of the algorithm and can be approached by various approximate inference methods, e.g., variational inference (see for example the CAMEL method \cite{ganapathi_constrained_2008}).

\begin{algorithm}[hbt!]
    \caption{Alternating Moment Matching for \HybridLogicNetworks{}}\label{alg:AMM_HLN}
    \begin{algorithmic}
        \Require Mean parameter $\datameanwith$
        \Ensure Parameters $\hybridparam$ for the approximating HLN $\expdist$ %, such that $\expdist$ is the (approximative) moment projection of $\empdistribution$ onto $\hlnsetof{\formulaset}$
        \iosepline
        \State Set
        \begin{align*}
            \hardlegset = \Big\{ \selindex \wcols \selindexin \ncond \meanparamat{\indexedselvariable}\in\{0,1\} \Big\}
        \end{align*}
        and a tuple $\headindexof{\hardlegset}$ with $\headindexof{\selindex}=\meanparamat{\indexedselvariable}$ for $\selindex\in\hardlegset$.
        \State Set $\canparamwith=\zerosat{\selvariable}$
        \While{Convergence criterion is not met}
            \ForAll{$\selindex\in[\seldim]\setexcept{\hardlegset}$}
                \State Compute
                \begin{align*}
                    \hypercoreat{\headvariableof{\selindex}}
                    = \contractionof{\{\bencodingof{\formulaof{\secselindex}} \wcols \secselindex\in[\seldim]\}
                        \cup\{\softactsymbolof{\secselindex,\canparam} \wcols \secselindex\in[\seldim]\ncond\secselindex\neq\selindex\}
                        \cup\{\basemeasure\}}{\headvariableof{\selindex}}
                \end{align*}
                \State Set
                \begin{align*}
                    \canparamat{\indexedselvariable} = \lnof{
                        \frac{\meanparamat{\indexedselvariable}}{(1-\meanparamat{\indexedselvariable})}
                        \cdot \frac{\hypercoreat{\headvariableof{\selindex}=0}}{\hypercoreat{\headvariableof{\selindex}=1}}
                    }
                \end{align*}
            \EndFor
        \EndWhile
        \State \Return $(\hardlegset,\headindexof{\hardlegset},\canparamwith)$
    \end{algorithmic}
\end{algorithm}

It can be shown that the algorithm converges if and only if there is a \HybridLogicNetwork{} matching the empirical moments of the data.
For more details we refer to~\cite[Chapter~9]{goessmann_tensor-network_2025}.

\input{../examples/hybrid_activation/hln_accounting_amm}

\subsection{Entailment by \HybridLogicNetworks{}} % Now

Let us now demonstrate a further use of our unified treatment of probabilistic and logical models by investigating a generalized concept of entailment.
Entailment can be generalized to probabilistic models by deciding whether a propositional formula is always satisfied given a probabilistic model.
%Also entailment can be checked for \HybridLogicNetwork{}.
%Assuming a positive probability of all models of the integrated \HardLogicNetwork{}, the entailment can be checked only considering the \HardLogicNetwork{}.
%Here a query is a formula to retrieve information from a given network.

\begin{theorem}\label{the:hybridEntailment}
    Let $\probofat{\hlnparameters}{\shortcatvariables}$ be a \HybridLogicNetwork{} and $\secexformulaat{\shortcatvariables}$ a propositional formula.
    Then $\probof{\hlnparameters}$ probabilistically entails $\secexformula$, that is,
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\secexformulaat{\shortcatvariables}} = 1 \, ,
    \end{align*}
    if and only if
%    Given a query formula $\secexformula$, we have that $\probofat{\hlnparameters}{\shortcatvariables}\models~\secexformula$ if and only if
    \begin{align*}
        \hlnformula \models \secexformula \, ,
    \end{align*}
    where
    \begin{align*}
        \hlnformulawith =
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=1} \enumformulaat{\shortcatvariables}\right)
        \land
        \left(\bigwedge_{\selindex\in\hardlegset\wcols\headindexof{\selindex}=0} \lnot\enumformulaat{\shortcatvariables}\right).
    \end{align*} 
\end{theorem}
\begin{proof}
    We have
    \begin{align*}
        \contraction{\probofat{\hlnparameters}{\shortcatvariables},\secexformulaat{\shortcatvariables}} = 1
    \end{align*}
    if and only if
    \begin{align*}
        \contraction{\left(\onesat{\shortcatvariables}-\nonzeroof{\probofat{\hlnparameters}{\shortcatvariables}}\right),\secexformulaat{\shortcatvariables}} = 0
    \end{align*}\, .
    We notice that $\hlnformulawith$ is the indicator tensor for the support of $\probofat{\hlnparameters}{\shortcatvariables}$.
    It therefore holds that
    \begin{align*}
        &\contraction{\left(\onesat{\shortcatvariables}-\nonzeroof{\probofat{\hlnparameters}{\shortcatvariables}}\right),\secexformulaat{\shortcatvariables}} \\
        &\quad\quad = \contraction{\secexformulaat{\shortcatvariables}} - \contraction{\hlnformulawith,\secexformulaat{\shortcatvariables}} \\
        &\quad\quad = \contraction{\secexformulaat{\shortcatvariables}} - \contraction{\hlnformulawith,\left(\onesat{\shortcatvariables}-\lnot\secexformulaat{\shortcatvariables}\right)} \\
        &\quad\quad = \contraction{\hlnformulawith,\secexformulaat{\shortcatvariables}} \, .
    \end{align*}
    By \defref{def:logicalEntailment} this vanishes if and only if $\hlnformula \models \secexformula$.
\end{proof}

\input{../examples/hybrid_activation/hln_accounting_entailment}

%
%\begin{example}{Entailment for Accounting Logic}
%    \alex{Bad example: Minterms are only entailed by themselves.}
%    To check entailment of the query formula defined by the minterm
%    \begin{align*}
%        g[X_{A_1}=0,X_{A_2}=1,X_F=1] = 1
%    \end{align*}
%    and is set to zero otherwise, entailment can be checked by contracting
%    \begin{align*}
%        \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}] = \catvariableof{A1} \oplus \catvariableof{A2} \otimes \mathbb{I}
%    \end{align*}
%    with $g$ arriving at
%    \begin{align*}
%        &\langle  \hlnformula[\catvariableof{A_1},\catvariableof{A_2},\catvariableof{F}], \lnot g[X_{A_1},X_{A_2},X_F]  \rangle \\
%        &= \sum_{x_{A_1},x_{A_2},x_{F} \in \{0,1\}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f]  (1-g[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f])\\
%        &= \sum_{\substack{x_{A_1},x_{A_2},x_{F} \in \{0,1\}\\ (x_{A_1},x_{A_2},x_{F})\neq (0,1,1)}} \hspace{-2ex}\hlnformula[X_{A_1}=x_{A_1},X_{A_2}=x_{A_2},X_F=x_f] \\
%        &\geq \hlnformula[X_{A_1}=1,X_{A_2}=0,X_F=0] > 0.
%    \end{align*}
%    Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does not entail $g$.
%    In this case, this is due to the fact, that $g$ only assumes one model, despite the formula having multiple models.
%
%    Doing the same calculations for
%    \begin{align*}
%        \secexformulaat{\catvariableof{A1},\catvariableof{A2},\catvariableof{F}}
%        = \lnot\catvariableof{A1} \lor \lnot\catvariableof{A2} \lor \lnot\catvariableof{F}
%    \end{align*}
%    \begin{align*}
%        \tilde g[X_{A_1}=1,X_{A_2}=1,X_F=1] = 0
%    \end{align*}
%    and equal to $1$ everywhere else leads to the constraction being equal to zero.
%    Therefore, $\mathbb{P}^{\mathcal{F},(A,y_A,\theta)}[\catvariableof{[d]}]$ does entail $\tilde g$.
%\end{example}