\section{The Neural Paradigm}

The neural paradigm of artificial intelligence exploits the decomposition of functions into neurons, which are aligned in a directed acyclic graph.
We show in this section how functions decomposeable into neurons can be represented by tensor networks.
To this end we formalize discrete neural models in decomposition graphs and formally proof the corresponding decomposition of their basis encodings.
Particular examples will be presented in the next section by propositional formulas with sparse syntactical descriptions.

\subsection{Function Decomposition}

\begin{lemma}
    \label{lem:formulaDecomp}
    Let $\formulaat{\shortcatvariables}$ be a composition of a $\seldim$-ary connective function $\exconnective$ and functions $\formulaofat{\selindex}{\shortcatvariables}$, where $\selindexin$, i.e. for $\shortcatindices\in\atomstates$ we have
    \begin{align*}
        \formulaat{\indexedshortcatvariables}
        = \exconnective\left(\formulaofat{0}{\indexedshortcatvariables}, \dots, \formulaofat{\seldim-1}{\indexedshortcatvariables}\right) \, .
    \end{align*}
    Then we have
    \begin{align*}
        \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
        = \contractionof{
            \{\bencodingofat{\exconnective}{\headvariableof{\formula},\headvariableof{[\seldim]}}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}
        }{\headvariableof{\formula},\shortcatvariables} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    This can be shown on each index $\shortcatindices$.
\end{proof}

For the composition of two functions $\formulaat{\shortcatvariables}$ and $\secexformula\left[\shortcatvariables\right]$ the composition by some binary connective is pictured by:
\begin{center}
    \input{../tikz_pics/logic_representation/unary_binary_composition}
\end{center}

Let us now define a more generic decomposition of discrete functions.

\begin{definition}
    \label{def:decompositionHypergraph}
    A decomposition hypergraph is a directed acyclic hypergraph $\graph=(\nodes,\edges)$ such that
    \begin{itemize}
        \item Each node $\nodein$ is decorated by a set $\arbsetof{\node}$ of finite cardinality $\catdimof{\node}$, a variable $\catvariableof{\node}$ and an index interpretation function
        \begin{align*}
            \indexinterpretationof{\node} \defcols [\catdimof{\node}] \rightarrow \arbsetof{\node} \, .
        \end{align*}
        \item Each directed hyperedge $(\incomingnodes,\outgoingnodes)$ has at least one outgoing node, i.e. $\outgoingnodes\neq\varnothing$ and is decorated by an activation function % propositional formula
        \begin{align*}
            \secexfunctionof{\node} \defcols
            \bigtimes_{\node\in\incomingnodes} \arbsetof{\node}
            \rightarrow \bigtimes_{\node\in\outgoingnodes} \arbsetof{\node} \, .
        \end{align*}
        \item Each node $\nodein$ appears at most once as an outgoing node. % Well-defined function
        \item The nodes not appearing as an outgoing node are enumerated by $\node^{\insymbol}_{[\atomorder]}$.
        We abbreviate the corresponding variables by $\catvariableof{\node^{\insymbol}_{[\atomorder]}}=\shortcatvariables$. % and labeled by $\nodes^{\insymbol}_{[\atomorder]}$.
        \item The nodes not appearing as an incoming node are enumerated by $\node^{\outsymbol}_{[\seldim]}$.
        We abbreviate the variables by $\catvariableof{\node^{\outsymbol}_{[\selindex]}},\catvariableof{\node^{\insymbol}_{[\atomorder]}}=\headvariables$. %$[\seldim]$ are labeled by $\nodes^{\outsymbol}_{[\seldim]}$.
    \end{itemize}
    We assign for each $\catenumeratorin$ restriction functions
    \begin{align*}
        \restrictionofto{\cdot}{\node^{\insymbol}_\catenumerator}
        \defcols \bigtimes_{\seccatenumerator\in[\catorder]} \arbsetof{\node^{\insymbol}_{\seccatenumerator}} \rightarrow \arbsetof{\node^{\insymbol}_\catenumerator}
        \quad,\quad  \restrictionofto{\catindexof{[\catorder]}}{\catenumerator} = \catindexof{\catenumerator}
    \end{align*}
    to the nodes $\node^{\insymbol}_{[\atomorder]}$ and recursively assign to each further node $\node$ a node function % connective $\connectiveofat{\node}{\headvariableof{\incomingnodes}}$.
    \begin{align*}
        \exfunctionof{\node} \wcols \bigtimes_{\catenumeratorin} \arbsetof{\node^{\insymbol}_\catenumerator} \rightarrow \bigtimes_{\node\in\outgoingnodes} \arbsetof{\node}
        \quad,\quad
        \exfunctionof{\node}(\catindexof{[\catorder]})
        = \secexfunctionof{\node}\left(\bigtimes_{\node\in\incomingnodes}\restrictionofto{\exfunctionof{\edgeof{\node}}(\catindexof{[\catorder]})}{\node}\right) \, ,
    \end{align*}
    where $\edgeof{\node}$ is to each $\node\in\incomingnodes$ the unique hyperedge with outgoing nodes $\{\node\}$.
    We then call the function
    \begin{align*}
        \exfunctionof{\graph} \wcols \bigtimes_{\catenumeratorin} \arbsetof{\node^{\insymbol}_\catenumerator} \rightarrow \bigtimes_{\selindexin} \arbsetof{\node^{\outsymbol}_\selindex}
        \quad,\quad
        \exfunctionof{\graph} = \bigtimes_{\selindexin} \exfunctionof{\node^{\outsymbol}_\selindex}
    \end{align*}
    the composition formula to the decomposition hypergraph $\graph$.
    %We call the formula $\exformulaat{\shortcatvariables}\coloneqq\formulaofat{\secnode}{\shortcatvariables}$ to the root note $\secnode$ the syntactical composition of $\graph$ and $\graph$ is a syntactical decomposition of $\exformula$.
\end{definition}

%We now show, that the propositional formula allows for a decomposition into connective formulas, its basis encoding decomposes into the basis encodings of the connective formulas.

% Neural Paradigm
The neural paradigm in AI can be modelled by the existence of decomposition hypergraphs for functions on large sets.
Let us now show how decomposition hypergraphs enable the sparse representation of composition functions by tensor networks.

\begin{theorem}
    \label{the:functionDecompositionRep}
    For any decomposition hypergraph $\graph$ with composition formula $\exfunctionof{\graph}$ we have
    \begin{align*}
        \bencodingofat{\exfunctionof{\graph}}{\headvariables,\shortcatvariables}
        = \contractionof{\left\{\bencodingofat{\secexfunctionof{\node}}{\catvariableof{\outgoingnodes},\catvariableof{\incomingnodes}} \wcols (\incomingnodes,\outgoingnodes)\in\edges\right\}
        }{\headvariables,\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    One can show this theorem by induction over the node formulas of the syntactical hypergraph, from the leafs to the root and iteratively applying \lemref{lem:formulaDecomp}.
\end{proof}

% Weights
When neurons have tunable parameters, we can discretize those by sets $\arbsetof{\catenumerator}$ and understand them as additional input variables

\input{../examples/neural_paradigm/mary_sum}

\subsection{Directed Message Passing}

\begin{algorithm}[hbt!]
    \caption{Directed Belief Propagation}\label{alg:directedBeliefPropagation}
    \begin{algorithmic}
        \Require Tensor network $\extnet$ on a directed hypergraph $\graph$
        \Ensure Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
        \iosepline
        \State Prepare directed message directions
        \begin{align*}
            \dirovedges = \left\{
                              \big((\innodes_{0},\outnodes_{0}),(\innodes_{1},\outnodes_{1})\big) \wcols
                              \innodes_{0} \cap (\innodes_{1},\outnodes_{1}) = \varnothing
                              \ncond
                              \outnodes_{1} \cap (\innodes_{0},\outnodes_{0}) = \varnothing
                              \ncond
                               \outnodes_{0} \cap \innodes_{1} \neq \varnothing
            \right\}
        \end{align*}
        \State Initialize a message queue $\scheduler=\{(\secsedge,\sedge) \wcols \secsedge \quad\text{has empty incoming nodes} \}]$
        \While{$\scheduler$ not empty}
            \State Pop a $(\sedge,\redge)$ pair from $\scheduler$
            \State Update the message
            \begin{align*}
                \messagewith
                = \contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
                }{\catvariableof{\sedge\cap \redge}}
            \end{align*}
            \State Update $\scheduler$ by all messages $(\redge,\thirdsedge)$ which have not yet been sent, if all messages $(\secsedge,\redge)$ have been sent.
        \EndWhile
        \State \Return Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{the:directedBeliefPropagationExactness}
    \alex{
        If all tensors are boolean and directed, then the messages in directed belief propagation gives the one-hot encodings of the node formulas evaluated at the inverse one-hot encoding at the leafs.
        In particular, the messages are the correct contractions of the subgraph.}
\end{theorem}
\begin{proof}
    By induction over the node formulas.
\end{proof}