\section{Introduction}

% Neural Paradigm
Modern artificial intelligence is dominated by large-scale neural models that excel at various tasks but mostly remain black-boxes.
While these models offer adaptability, the two main concerns when integrating these architectures into safety-critical processes, are reliability and explainability.
% Logical Paradigm
In this regard, the logical tradition of artificial intelligence, historically motivated by the resemblance of human thought to formal logics \cite{mccarthy_programs_1959}, offers explicit structures and human-readable inference.
However, the main problem hindering the success of this classical approach is the inability of classical \firstOrderLogic{} to handle uncertainty or scale to complex real-world data.
% Probabilistic Paradigm
Probabilistic graphical models~\cite{pearl_probabilistic_1988,koller_probabilistic_2009}, provide insights based on encoded variable independences and causality \cite{pearl_causality_2009}. % and logical formulas within exponential families~\cite{wainwright_graphical_2008}.
While probabilistic models and Statistical Relational AI~\cite{nickel_review_2016,getoor_introduction_2019} have improved uncertainty handling, bridging these paradigms remains the central goal of \emph{Neuro-Symbolic AI}~\cite{hochreiter_toward_2022, sarker_neuro-symbolic_2022, colelough_neuro-symbolic_2024}.
Building on early connectionist approaches~\cite{towell_knowledge-based_1994,avila_garcez_connectionist_1999} and aligning with statistical relational learning~\cite{marra_statistical_2024}, the field seeks a single, mathematically coherent framework combining structural clarity with neural adaptability.
%We demonstrate the practical power of this unified representation by benchmarking the framework on constraint satisfaction tasks like Sudoku problems from Sakana AI, confirming its ability to perform exact symbolic inference.
Altough progress has been made, for example with Markov Logic Networks~\cite{richardson_markov_2006}, a fully unified substrate that treats logical and probabilistic inference as instances of the same operation, is still missing. % requires a more flexible linear-algebraic foundation.

%% TNREASON to bridge the gap between logical and probabilistic reasoning
We in this work propose to fill this gap between the probabilistic, neural and logical paradigm with \emph{tensor networks}, in a framework called \tnreason{}. %~\cite{goessmann_tensor-network_2025}.
% Representation
Tensor spaces capture both the semantics of logical formulas (by boolean tensors) and probability distributions (by normalized non-negative tensors).
%Most significantly for the present work, we exploit the algebraic flexibility of tensor networks to bridge the gap between continuous numerical representation and discrete symbolic reasoning.
%To this end, we demonstrate how the fundamental principles of probabilistic and logical representation schemes are captured by tensor network decompositions.
As naive tensors are prone to the curse of dimensionality, we turn to distributed representation schemes by tensor networks.
We show that fundamental sparsity principles of AI, such as conditional independence, existence of sufficient statistics and model decomposition into neurons, are equivalent to tensor network decompositions.
% Reasoning
Moreover, we identify tensor network contraction as fundamental inference instances, which correspond with computation of marginal distributions and the decision of entailment.
This abstraction eliminates the traditional divide between symbolic and numerical representations: logical inference and probabilistic computations become different instances of the same underlying operation.

%% Message Passing Schemes
Efficient schemes to perform inference are known as message passing schemes (appearing as sum-product, belief propagation, etc.).
These schemes underly distributed approaches to contract tensor networks based on efficient local contractions.

%where logical formulas (boolean tensors) and probabilistic distributions (real tensors) are manipulated through the same algebra of tensor contraction.
%We build upon the recently developed \emph{\tnreason{}} framework~\cite{goessmann_tensor-network_2025}, which establishes tensor networks as a unifying foundation

%In this paper,
To capture both logical and probabilistic models, we introduce \emph{\ComputationActivationNetworks{} (\CompActNets{})}, an architecture that organizes this reasoning into two complementary substructures.
The \emph{computation network} encodes the structural relations of a problem, such as logical dependencies or more generic statistics, while the \emph{activation network} assigns semantic or numerical values to these structures.
We generalize the standard tensor network diagrammatic notation to visually represent these components, defining the inference process as a graphical tensor contraction between structure and activation.
Logical inference emerges when activations are boolean, probabilistic inference when they are real-valued, and hybrid reasoning when both coexist.


\subsection{Related works}

%% Neuro-Symbolic
The unification of neural, symbolic and probabilistic approaches to interpretable model architectures has been a long-standing aim.
A central goal is to achieve \emph{intrinsic explainability}.
Unlike post-hoc interpretations—which analyze input influence or fit surrogate models after training~\cite{lipton_mythos_2018,barredo_arrieta_explainable_2020}—the proposed framework encodes symbolic relations that remain directly readable, building explainability into the architecture itself.

%%% Tensor Networks

%To this end,
\emph{Tensor Networks} have recently gained interest as a unifying language for AI, framed by Logical Tensor Networks \cite{badreddine_logic_2022} and Tensor Logic \cite{domingos_tensor_2025}.
Furthermore, the MeLoCoToN approach \cite{ali_explicit_2025} applies tensor network architectures similar to \CompActNets{} in combinatorical optimization problems.
Specifically, tensor networks have emerged as a highly efficient mathematical framework for handling data in high-dimensional spaces, effectively circumventing the "curse of dimensionality" that typically plagues grid-based methods ~\cite{hackbusch_tensor_2012}.
By decomposing high-order tensors into networks of low-rank components, these structures reduce the storage and computational complexity from exponential to polynomial with respect to the dimension ~\cite{oseledets_tensor-train_2011, hackbusch_new_2009, hitchcock_expression_1927}.

%%% History of TN

Historically rooted in quantum many-body physics ~\cite{white_density-matrix_1993}, tensor networks found its first major success with Matrix Product States (MPS), originally developed to efficiently capture the quantum dynamics and ground states of one-dimensional spin chains ~\cite{affleck_rigorous_1987}.
This format remains a standard tool in the field, with recent contributions refining it for tasks such as large-scale stochastic simulations and variational circuit operations ~\cite{sander_large-scale_2025, sander_quantum_2025}.
To address the topological constraints of MPS, the landscape of architectures was subsequently expanded to include Projected Entangled Pair States (PEPS) for two-dimensional lattices and the Multi-scale Entanglement Renormalization Ansatz (MERA), which utilizes a hierarchical geometry to represent scale-invariant critical systems and has recently been adapted for simulating quantum systems ~\cite{orus_tensor_2019, berezutskii_simulating_2025}.
Beyond the quantum realm, these formats have been successfully adapted to applied mathematics, particularly for solving high-dimensional parametric PDEs, sampling problems, modeling complex continuous fields and learning dynamical laws ~\cite{hagemann_sampling_2025, eigel_adaptive_2017, goessmann_tensor_2020}.
Furthermore, they exhibit properties helpful for handling these high-dimensional spaces, such as restricted isometry properties~\cite{goessmann_uniform_2021}.
Recent advancements have demonstrated the efficacy of these methods in capturing multiscale phenomena in fluid dynamics and turbulence, proving that the tensor network formalism offers a robust alternative to classical numerical schemes ~\cite{gourianov_tensor_2025}.




%\subsection{Related Works}

\subsection{Structure of the paper}

The paper is organized as follows.
Section~\ref{sec:notation} introduces the basic notation for categorical variables, tensors, and tensor networks, establishing the formal framework on which all subsequent reasoning structures are defined.
Section~\ref{sec:prob_rep} develops the probabilistic representation of reasoning through soft activation, showing how exponential-family distributions can be expressed as tensor networks based on independence assumptions and sufficient statistics.
Section~\ref{sec:log_rep} turns to hard activation, formulating propositional logic within the same tensor framework and demonstrating how logical inference, entailment, and knowledge bases can be represented by boolean tensors and contractions.
Section~\ref{sec:hyb_rep} unifies these two perspectives in the concept of Hybrid Logic Networks, which integrate hard logical constraints with soft probabilistic activations, thereby forming the core of the computation–activation architecture.
The paper concludes with coding examples in section~\ref{sec:code} that illustrate the expressive power and interpretability of this unified tensor-based reasoning approach.