%\section{Decompositions based on Propositional Syntax}
\section{The logical paradigm}\label{sec:logPar}

A tensor-based representation of propositional logic is developed by defining formulas as boolean valued tensors, and showing how logical connectives and normal forms can be expressed as tensor contractions.

\subsection{Propositional semantics by boolean tensors}

Starting with the introduction of propositional formulas as boolean tensors their decomposition is discussed with respect to a basis encoding.
%The basis encoding representation enables dealing with the negation operation of propositional formulas still in tensor format.
%Furthermore, the propositional formula then fits into the concept of \CompActNets{}.

\begin{definition}
    \label{def:formulas}
    A \emph{propositional formula} $\formulaat{\shortcatvariables}$ depending on $\atomorder$ boolean variables $\catvariableof{\atomenumerator}$ is a tensor
    \begin{align*}
        \formulaat{\shortcatvariables} \in \bigotimes_{\atomenumeratorin} \rr^2
    \end{align*}
    with coordinates in $\ozset$.
    We call a state $\shortcatindices \in \atomstates$ a \emph{model} of a propositional formula $\formula$, if
    \begin{align*}
        \formulaat{\indexedshortcatvariables}=1 \, ,
    \end{align*}
    where we understand $1$ as a representation of $\mathrm{True}$ and $0$ of $\mathrm{False}$.
    If there is a model of a propositional formula, we say the formula is \emph{satisfiable}.
\end{definition}

\input{../examples/logical_paradigm/prop_formula_coordinatewise.tex}

\paragraph{Model counts by contraction}
Each coordinate of a propositional formula is either $1$ or $0$, indicating whether the indexed state is a model of the formula or not.
In this way, the contraction $\contraction{\formula}$ counts the number of models of the propositional formula $\formula$.
One can therefore decide the satisfiability of a formula by testing for $\contraction{\formula}>0$.

\paragraph{CP decomposition}
We can decompose a formula into the sum of the one-hot encodings of its models:
%Since the tensor $\formulaat{\shortcatvariables}$ is equal to one at index $x_{[d]}$ if and only if $x_{[d]}$ is a model of $\formula$, a propositional formula can be written as the sum over the one-hot encodings of its models.
\begin{center}
    \input{../tikz_pics/logic_representation/formula_direct.tex}
\end{center}
As already depicted, one can exploit this summation to find a $\cpformat$ decomposition of the formula.
To this end, we enumerate the models $\shortcatindices^{\decindex}$ of $\formula$ by a decomposition variable $\decvariable$ with values $\decindex\in[\contraction{\formula}]$ and define, for $\catenumeratorin$, cores with slices
\begin{align*}
    \hypercoreofat{\catenumerator}{\catvariableof{\catenumerator},\indexeddecvariable}
    = \onehotmapofat{\catindexof{\catenumerator}^{\decindex}}{\catvariableof{\catenumerator}} \, .
\end{align*}

\input{../examples/logical_paradigm/prop_formula_basCP.tex}

\paragraph{Basis encoding}
Representing booleans by elements in $\{0,1\}$ leads to the problem that the negation is an affine transformation and cannot be represented by multilinear tensors. %~\cite[Section 4.1.1]{goessmann_tensor-network_2025}.
%Therefore, instead of using this \emph{coordinate calculus} scheme an approach based on \emph{basis calculus} is employed, which is explained in this section.
To be able to express different kinds of connectives by contractions, booleans are encoded by one-hot encodings as defined in \defref{def:onehotenc}.
Propositional formulas $\formula$ can then be expressed by their basis encodings
\begin{align*}
    \bencodingofat{\formula}{\indexedheadvariableof{\formula},\indexedshortcatvariables}
    = \begin{cases}
          1 & \ifspace \formulaat{\indexedshortcatvariables} = \headindexof{\formula}\\
          0 & \text{else}
    \end{cases} \, .
\end{align*}
This basis encoding $\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables} \in \{0,1\}^{2\times 2^d}$ encodes the formula itself and its negation in its slices, since
\begin{align*}
    \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
    = \tbasisat{\headvariableof{\formula}} \otimes \formulaat{\shortcatvariables}
    + \fbasisat{\headvariableof{\formula}} \otimes \lnot\formulaat{\shortcatvariables} \, .
\end{align*}
In our graphical notation this property is visualized by
\begin{center}
    \input{../tikz_pics/logic_representation/formula_bencoding.tex}
\end{center}
We further provide a more detailed example in coordinate sensitive notation in the following.
\input{../examples/logical_paradigm/bencoding_neg_con.tex}

\paragraph{Interpretation as \CompActNets{}}
The propositional formula and its negation can be represented by this tensor via
\begin{align*}
    \formulaat{\shortcatvariables}
    = \contractionof{\tbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables}
    \andspace
    \lnot\formulaat{\shortcatvariables}
    = \contractionof{\fbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables} \, .
\end{align*}
Both $\formula$ and $\lnot\formula$ are thus \ComputationActivationNetworks{} to the statistic $\{\formula\}$ and the hard activation tensor $\tbasisat{\headvariableof{\formula}}$, respectively $\fbasisat{\headvariableof{\formula}}$.

\subsection{Syntactic decomposition of propositional formulas}

Propositional formulas of concern often have a syntactic specification as composed functions.
We can therefore apply the neural paradigm to find efficient representations of them.

\begin{definition}[Syntactic decompositions]
    \label{def:syntacticalDecomposition}
    A syntactic decomposition of a propositional formula $\exformula$ is a decomposition hypergraph (see \defref{def:decompositionHypergraph}) such that all nodes are decorated with the dimension $\catdimof{\node}=2$ and the composition function $\exformula$.
\end{definition}

We thus have a tensor network representation of any propositional formula based on its syntactic decomposition, where the hypergraph of the syntactic decomposition equals the hypergraph of the representing tensor network.

\input{../examples/logical_paradigm/prop_formula_syntax}

\subsection{Entailment decision by contractions}

We have already seen that the contraction of a propositional formula counts its models.
This allows to define entailment between two propositional formulas as defined in the following.
To generalize the treatment, we no longer demand that the variables of a formula are of dimension $2$.
We further use $\lnot\formulawith=\oneswith-\formulawith$.

\begin{definition}[Entailment of propositional formulas]
    \label{def:logicalEntailment}
    Given two propositional formulas $\kb$ and $\formula$, we say that $\kb$ entails $\formula$, denoted by $\kb\models\formula$, if any model of $\kb$ is also a model of $\formula$, that is
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}=0 \, .
    \end{align*}
    If $\kb\models\lnot\formula$ holds (i.e. $\contraction{\kbwith,\formulawith}$=0), we say that $\kb$ contradicts $\formula$.
\end{definition}

% Relation to classical definition of entailment
Classically (see e.g. \cite{russell_artificial_2021}) entailment in propositional logics is defined as the unsatisfiability of $\kb\land\lnot\exformula$.
This is equivalent to \defref{def:logicalEntailment} due to the equivalence of $\contraction{\kbwith,\lnot\formulawith}=0$ and $\contraction{(\kb \land (\lnot\exformula))[\shortcatvariables]}=0$, which is the unsatisfiability of $\kb\land\lnot\exformula$.

%Entailment is the central operation of "logical inference", i.e. deduce true statements from known statements.
%In the tensor network representation, these entailments can be decided by contracting the whole representing tensor with the statement, that needs to be checked.

\input{../examples/logical_paradigm/sudoku_entailment}

\subsection{Efficient representation of knowledge bases}

We now investigate the representation of propositional knowledge bases $\kb=\{\formulaof{\selindex}\wcols\selindexin\}$, which are sets of propositional formulas $\formulaof{\selindex}$.
The conjunction of these formulas is the knowledge base formula
\begin{align*}
    \kbwith
    = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables} \, .
\end{align*}
To show efficient representations, we use the following identities.

\begin{lemma}[Computation Network Symmetries]
    \label{lem:comNetSymmetries}
    For the $\catorder$-ary $\land$-connective (where $\catorder\in\nn$) and the unary $\lnot$-connective it holds that
    \begin{align*}
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\land}{\headvariable,\shortcatvariables}}{\shortcatvariables}
        = \bigotimes_{\catenumeratorin} \tbasisat{\catvariableof{\catenumerator}}
        \andspace
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\lnot}{\headvariable,\catvariable}}{\catvariable}
        = \fbasisat{\catvariable} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    Follows directly from the definitions of the basis encodings and the connectives.
\end{proof}

\input{../examples/logical_paradigm/prop_formula_kb_head_sym}

We use this to decompose knowledge bases into their individual formulas as follows.

\begin{theorem}
    \label{the:kbDecomposition}
    For any knowledge base $\kbwith = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables}$ it holds that
    \begin{align*}
        \kbwith
        = \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    With \lemref{lem:comNetSymmetries} we have
    \begin{align*}
        \kbwith
        &= \contractionof{\{\tbasisat{\headvariableof{\land}},\bencodingofat{\land}{\headvariableof{\land},\headvariables}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{
            \bigcup_{\selindexin} \{\tbasisat{\headvariableof{\selindex}},\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{proof}

\input{../examples/logical_paradigm/sudoku_decomposition}

\subsection{Entailment decision by message passing}

% Infeasible contractions
Since contracting the whole tensor network is often infeasible, local contractions can be considered to decide entailment in some cases.
Here, a local contraction describes the calculation of contractions along few closely connected tensors in the network.
Before presenting the resulting Constraint Propagation algorithm, we first show two important properties of local entailment motivating the procedure.

\begin{theorem}[Monotonicity of propositional logics]
    \label{the:monotonicityPL}
    If $\seckb\subset\kb$ and $\seckb\models\formula$ then also $\kb\models\formula$.
\end{theorem}
\begin{proof}
    Since $\seckb\models\formula$ it holds that $\contraction{\seckb[\shortcatvariables],\lnot\formula[\shortcatvariables]}=0$ and thus
    \begin{align*}
        \contractionof{\seckb[\shortcatvariables],\lnot\formula[\shortcatvariables]}{\shortcatvariables}=\zerosat{\shortcatvariables} \, .
    \end{align*}
    Denoting by $\kb/\seckb$ the conjunctions of formulas in $\kb$ not in $\seckb$, we have
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\seckb[\shortcatvariables],\lnot\formulawith} \\
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\contractionof{\seckb[\shortcatvariables],\lnot\formulawith}{\shortcatvariables}} \\
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\zerosat{\shortcatvariables}} \\
        &= 0 \, .
    \end{align*}
\end{proof}
To decide entailment, we can therefore investigate entailment on smaller parts of the knowledge base.
This is sound by the above theorem but not complete since it can happen that no smaller part of the knowledge base entails the formula while the whole knowledge base does.
We can furthermore add entailed formulas to the knowledge base without changing it as is shown next.

\begin{theorem}[Invariance of adding entailed formulas]
    \label{the:addingEntailed}
    If and only if $\kb\models\formula$ we have that
    \begin{align*}
        \kbwith
        = \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We use that $\formulawith+\lnot\formulawith=\onesat{\shortcatvariables}$ and thus
    \begin{align*}
        \kbwith
        &= \contractionof{\kbwith,(\formulawith+\lnot\formulawith)}{\shortcatvariables} \\
        &= \contractionof{\kbwith,\formulawith}{\shortcatvariables}  + \contractionof{\kbwith,\lnot\formulawith}{\shortcatvariables} \,. \\
        %&= \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
    Since $\contractionof{\kbwith,\lnot\formulawith}{\shortcatvariables}$ is boolean, we have that
    \begin{align*}
        \kbwith=\contractionof{\kbwith,\formulawith}{\shortcatvariables}
    \end{align*}
    if and only if $\contraction{\kbwith,\lnot\formulawith}=0$, that is $\kb\models\formula$.
\end{proof}

% Interpreting entailment
The mechanism of \theref{the:addingEntailed} provides us with a means to store entailment information in small-order auxiliary tensors.
%Adding deduced statements to a knowledge base does not change the knowledge base as a tensor, but one can exploit it in smaller contractions.
% Constraint Propagation
One way to exploit this accessibility of local entailment information are message passing schemes similar to \algoref{alg:treeBeliefPropagation} propagating the information.
This approach decides local entailment by iteratively adding entailed formulas to the knowledge base and checking further entailment on neighboring tensors of the knowledge base.
Since for entailment decisions the support of the contractions is sufficient, we can apply non-zero indicators before sending contraction messages.
We then schedule new messages in the direction $(\sedge,\redge)$ once the support of a message received at $\sedge$ has been changed.
Note that such a scheduling system is guaranteed to converge since there can only be a finite number of message changes.
We further directly reduce the computation of messages to their support and call the resulting Constraint Propagation (\algoref{alg:constraintPropagation}).

\begin{algorithm}[hbt!]
    \caption{Constraint Propagation}\label{alg:constraintPropagation}
    \begin{algorithmic}
        \Require Tensor network $\extnet$ on a hypergraph $\graph$
        \Ensure Messages $\{\messagewith\wcols(\sedge,\redge)\in\dirovedges\}$ containing entailment statements
        \iosepline
        \State Initialize a queue $\scheduler = \dirovedges$ of message directions
        \State Initialize messages $\messagewith = \onesat{\catvariableof{\sedge\cap\redge}}$ for $(\sedge,\redge)\in\dirovedges$
        \While{$\scheduler$ not empty}
            \State Pop a $(\sedge,\redge)$ pair from $\scheduler$
            \State Update the message
            \begin{align*}
                \messagewith
                = \nonzeroof{\contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
                }{\catvariableof{\sedge\cap \redge}}}
            \end{align*}
            \If{$\hypercoreat{\catvariableof{\sedge\cap \redge}}\neq\messagewith$}
                \State Update the message: $\messagewith\coloneqq\hypercoreat{\catvariableof{\sedge\cap \redge}}$
                \State Add $\scheduler = \scheduler \cup \{(\redge,\secsedge) \wcols (\redge,\secsedge)\in\dirovedges\}$ % Clear?
            \EndIf
        \EndWhile
        \State \Return Messages $\{\messagewith\wcols(\sedge,\redge)\in\dirovedges\}$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{the:constraintPropagationSoundness}
    All messages during constraint propagation are sound, meaning that for all $(\sedge,\redge)\in\dirovedges$ it holds that
    \begin{align*}
        \nonzeroof{\contractionof{\extnet}{\catvariableof{\sedge\cap\redge}}} \prec \messagewith \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We show this theorem by induction over the \whileSymbol{} loop of \algoref{alg:constraintPropagation}.
    At the first iteration, we have for all messages $\messagewith=\onesat{\catvariableof{\sedge\cap\redge}}$ and thus
    \begin{align}
        \label{eq:nzMessageAddingEquivalence}
        \extnet = \contractionof{\{\extnet\}\cup\{\messagewith\wcols(\sedge,\redge)\in\dirovedges\}}{\nodevariables} \, .
    \end{align}
    By \theref{the:monotonicityPL} we then have for the first message send along the pair $(\sedge,\redge)$ that
    \begin{align*}
        \nonzeroof{\contractionof{\extnet}{\catvariableof{\sedge\cap\redge}}} \prec
        &\nonzeroof{\contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
            \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
        }{\catvariableof{\sedge\cap \redge}}} \\
        &= \messagewith \, .
    \end{align*}

    We now assume that at an arbitrary state of the algorithm the inequality holds for all previously sent messages.
    By \theref{the:addingEntailed} we can contract the messages with the tensor network without changing it and \eqref{eq:nzMessageAddingEquivalence} thus still holds.
    We then conclude with \theref{the:monotonicityPL} that the claimed property also holds for the new message.
\end{proof}

\input{../examples/logical_paradigm/sudoku_message_passing.tex}