%\section{Decompositions based on Propositional Syntax}
\section{The Logical Paradigm}\label{sec:logPar}

A tensor-based representation of propositional logic is developed by encoding boolean variables into vectors, defining formulas as boolean tensors, and showing how logical connectives and normal forms can be expressed as tensor contractions.

\subsection{Propositional Semantics by Boolean Tensors}


\begin{definition}
    \label{def:formulas}
    A \emph{propositional formula} $\formulaat{\shortcatvariables}$ depending on $\atomorder$ boolean variables $\catvariableof{\atomenumerator}$ is a boolean-valued tensor
    \begin{align*}
        \formulaat{\shortcatvariables} \defcols \atomstates \rightarrow \ozset \subset \rr \, .
    \end{align*}
    We call a state $\shortcatindices \in \atomstates$ a \emph{model} of a propositional formula $\formula$, if
    \begin{align*}
        \formulaat{\indexedshortcatvariables}=1 \, ,
    \end{align*}
    where we associate $\mathrm{True}\leftrightarrow 1$ and $\mathrm{False}\leftrightarrow 0$.
    If there is a model to a propositional formula, we say the formula is \emph{satisfiable}.
\end{definition}

\input{../examples/logical_paradigm/prop_formula_coordinatewise.tex}

\paragraph{Model counts by contraction}
Each coordiante of the propositional formula is either a $1$ or $0$ encoding if the indexed state is a model of the formula or not.
In this way, the contraction $\contraction{\formula}$ counts the number of models of the propositional formula $\formula$.
One can therefore decide the satisfiability of a formula by checking if $\contraction{\formula}>0$.

\paragraph{CP decomposition}
Since the tensor $\formulaat{\shortcatvariables}$ is equal to one at index $x_{[d]}$ if and only if $x_{[d]}$ is a model of $\formula$, a propositional formula can be written as the sum over the one-hot encodings of its models.
\begin{center}
    \input{../tikz_pics/logic_representation/formula_direct.tex}
\end{center}
As already depicted one can exploit this summation to find a $\cpformat$ decomposition of the formula.
To this we enumerate the models $\shortcatindices^{\decindex}$ of $\formula$ by a decomposition variable $\decvariable$ with values $\decindex\in[\contraction{\formula}]$ and define for $\catenumeratorin$ cores with slices
\begin{align*}
    \hypercoreofat{\catenumerator}{\catvariableof{\catenumerator},\indexeddecvariable}
    = \onehotmapofat{\catindexof{\catenumerator}^{\decindex}}{\catvariableof{\catenumerator}} \, .
\end{align*}

\input{../examples/logical_paradigm/prop_formula_basCP.tex}

\paragraph{Basis encoding}
Representing booleans by elements in $\{0,1\}$ leads to the problem, that negation is an affine transformation and can not be represented by multilinear tensors. %~\cite[Section 4.1.1]{goessmann_tensor-network_2025}.
Therefore, instead of using this \emph{coordinate calculus} an approach based on \emph{basis calculus} is employed, which is explained in this section.
To be able to express different kinds of connectives and finally any propositional formula by multi-linear tensors, booleans are encoded by one-hot encodings as defined in \defref{def:onehotenc}.
Propositional formulas $\formula$ can be expressed in terms of a tensor describing the mapping and its negation by
\begin{align}
    \label{eq:basisencboolean}
    \bencodingofat{\formula}{\indexedheadvariableof{\formula},\indexedshortcatvariables}
    = \begin{cases}
          1 & \ifspace \formulaat{\indexedshortcatvariables} = \headindexof{\formula}\\
          0 & \text{else}
    \end{cases}.
\end{align}
This basis encoding $\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables} \in \{0,1\}^{2\times 2^d}$ then has the form
\begin{align}
    \label{eq:basisencnegsum}
    \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
    = \tbasisat{\headvariableof{\formula}} \otimes \formulaat{\shortcatvariables}
    + \fbasisat{\headvariableof{\formula}} \otimes \lnot\formulaat{\shortcatvariables} \, .
\end{align}
In our graphical notation this property is visualized by
\begin{center}
    \input{../tikz_pics/logic_representation/formula_bencoding.tex}
\end{center}
We further provide a more detailed example in coordinate sensitive notation in the following.
\input{../examples/logical_paradigm/bencoding_neg_con.tex}

\paragraph{Interpretation as \CompActNets{}}
The propositional formula and its negation can be represented by that tensor by
\begin{align*}
    \formulaat{\shortcatvariables}
    = \contractionof{\tbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables}
    \andspace
    \lnot\formulaat{\shortcatvariables}
    = \contractionof{\fbasisat{\headvariableof{\formula}},\bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}}{\shortcatvariables} \, .
\end{align*}
Both $\formula$ and $\lnot\formula$ are thus \ComputationActivationNetworks{} to the statistic $\{\formula\}$ and the hard activation tensor $\tbasisat{\headvariableof{\formula}}$, respectively $\fbasisat{\headvariableof{\formula}}$.
%This representation of propositional formulas with respect to basis encoding thus leads to \ComputationActivationNetworks{}, which were also used to describe probability distributions in the last section.
%In this way the soft and hard logic can be combined in one framework.

\subsection{Syntactical Decomposition of Propositional Formulas}

Propositional formulas of concern often have a syntactical specification as composed functions.
We can therefore apply the neural paradigm to find efficient representations of them.

\begin{definition}[Syntactical Decompositions]
    \label{def:syntacticalDecomposition}
    A syntactical decomposition of a propositional formula $\exformula$ is a decomposition hypergraph (see \defref{def:decompositionHypergraph}) such that all nodes are decorated with the dimension $\catdimof{\node}=2$ and composition function $\exformula$.
\end{definition}

Thus we have a tensor network representation of any propositional formula based on its syntactical decomposition, where the hypergraph of the syntactical decomposition equals the hypergraph of the representing tensor network.

\input{../examples/logical_paradigm/prop_formula_syntax}

%\begin{lemma}
%    \label{lem:formulaDecomp}
%    Let $\formulaat{\shortcatvariables}$ be a composition of a $\seldim$-ary connective formula $\exconnective$ and propositional formulas $\formulaofat{\selindex}{\shortcatvariables}$, where $\selindexin$, i.e. for $\shortcatindices\in\atomstates$ we have
%    \begin{align*}
%        \formulaat{\indexedshortcatvariables}
%        = \exconnective\left(\formulaofat{0}{\indexedshortcatvariables}, \dots, \formulaofat{\seldim-1}{\indexedshortcatvariables}\right) \, .
%    \end{align*}
%    Then we have
%    \begin{align*}
%        \bencodingofat{\formula}{\headvariableof{\formula},\shortcatvariables}
%        = \contractionof{
%            \{\bencodingofat{\exconnective}{\headvariableof{\formula},\headvariableof{[\seldim]}}\}
%            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables} \wcols \selindexin\}
%        }{\headvariableof{\formula},\shortcatvariables} \, .
%    \end{align*}
%\end{lemma}
%\begin{proof}
%    This can be shown on each index $\shortcatindices$.
%\end{proof}

%\begin{definition}
%    \label{def:formulaDecomposition}
%    A syntactical hypergraph is a directed acyclic hypergraph $\graph=(\nodes,\edges)$ such that
%    \begin{itemize}
%        \item each hyperedge $\edge=(\incomingnodes,\outgoingnodes)$ has exactly one outgoing node, i.e. $\cardof{\outgoingnodes}=1$
%        \item each node $\nodein$ carries a boolean variable $\headvariableof{\node}$ and appears at most once as the outgoing node of a hyperedge % well-definedness
%        \item each hyperedge $(\incomingnodes,\{\node\})$ with $\incomingnodes\neq\varnothing$ is decorated by a propositional formula
%        \begin{align*}
%            \connectiveofat{\node}{\headvariableof{\incomingnodes}} \defcols \bigtimes_{\node\in\incomingnodes} [2] \rightarrow [2]
%        \end{align*}
%        \item the nodes not appearing as an outgoing node are labeled by $[\atomorder]$
%    \end{itemize}
%    We say that the syntactical hypergraph is single-rooted, if exactly one node $\secnode$ does not appear as an incoming node of a hyperedge.
%    In this case this unique node is called the root node. % head node
%    We assign atomic formulas to the nodes $[\atomorder]$ and recursively assign to each further node $\node$ a node formula % connective $\connectiveofat{\node}{\headvariableof{\incomingnodes}}$.
%    \begin{align*}
%        \formulaofat{\node}{\indexedshortcatvariables}
%        = \connectiveofat{\node}{[\formulaofat{\thirdnode}{\indexedshortcatvariables}\wcols\thirdnode\in\incomingnodes]} \quad \forall\shortcatindicesin\, ,
%    \end{align*}
%    where $\incomingnodes$ are the incoming nodes in the unique hyperedge with outgoing nodes $\{\node\}$.
%    We call the formula $\exformulaat{\shortcatvariables}\coloneqq\formulaofat{\secnode}{\shortcatvariables}$ to the root note $\secnode$ the syntactical composition of $\graph$ and $\graph$ is a syntactical decomposition of $\exformula$.
%\end{definition}
%
%%We now show, that the propositional formula allows for a decomposition into connective formulas, its basis encoding decomposes into the basis encodings of the connective formulas.
%
%\begin{theorem}
%    \label{the:formulaDecompositionRep}
%    For any syntactical hypergraph $\graph$ with composition $\exformula$ we have
%    \begin{align*}
%        \exformulaat{\shortcatvariables}
%        = \breakablecontractionof{
%            &\left\{
%                 \bencodingofat{\connectiveof{\node}}{\headvariableof{\node},\headvariableof{\incomingnodes}} \wcols (\incomingnodes,\{\node\})\in\edges
%            \right\} \cup \\
%            & \{\identityat{\headvariableof{\atomenumerator},\catvariableof{\atomenumerator}} \wcols \atomenumeratorin\}
%            \cup \{\tbasisat{\headvariableof{\secnode}}\}
%        }{\shortcatvariables} \, .
%    \end{align*}
%\end{theorem}
%\begin{proof}
%    One can show this theorem by induction over the node formulas of the syntactical hypergraph, from the leafs to the root and iteratively applying \lemref{lem:formulaDecomp}.
%\end{proof}
%
%Thus we have a tensor network representation of any propositional formula based on its syntactical decomposition, where the hypergraph of the syntactical decomposition equals the hypergraph of the representing tensor network.
%

\subsection{Entailment Decision by Contractions}

We have already seen that the contraction of a propositional formula counts its models.
This allows to define entailment between two propositional formulas as follows.
To generalize the treatment, we do not demand any more that the variables of a formula are of dimension $2$.
We further use $\lnot\formulawith=\oneswith-\formulawith$.

\begin{definition}[Entailment of propositional formulas]
    \label{def:logicalEntailment}
    Given two propositional formulas $\kb$ and $\formula$ we say that $\kb$ entails $\formula$, denoted by $\kb\models\formula$, if any model of $\kb$ is also a model of $\formula$, that is
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}=0 \, .
    \end{align*}
    If $\kb\models\lnot\formula$ holds (i.e. $\contraction{\kbwith,\formulawith}$=0), we say that $\kb$ contradicts $\formula$.
\end{definition}

% Relation to classical definition of entailment
Classically (see e.g. \cite{russell_artificial_2021}) entailment in propositional logics is defined as the unsatisfiability of $\kb\land\lnot\exformula$.
This is equivalent to \defref{def:logicalEntailment}, since $\contraction{\kbwith,\lnot\formulawith}=0$ is equivalent to $\contraction{(\kb \land (\lnot\exformula))[\shortcatvariables]}=0$, which is the unsatisfiability of $\kb\land\lnot\exformula$.

%Entailment is the central operation of "logical inference", i.e. deduce true statements from known statements.
%In the tensor network representation, these entailments can be decided by contracting the whole representing tensor with the statement, that needs to be checked.

\input{../examples/logical_paradigm/sudoku_entailment}

\subsection{Efficient Representation of Knowledge Bases}

We now investigate the representation of propositional knowledge bases $\kb=\{\formulaof{\selindex}\wcols\selindexin\}$, which are sets of propositional formulas $\formulaof{\selindex}$.
The conjunction of these formulas is the knowledge base formula
\begin{align*}
    \kbwith
    = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables} \, .
\end{align*}
To show efficient repesentations we will use the following identities.

\begin{lemma}[Computation Network Symmetries]
    \label{lem:comNetSymmetries}
    We have for the $\catorder$-ary $\land$-connective (where $\catorder\in\nn$) and the unary $\lnot$-connective that
    \begin{align*}
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\land}{\headvariable,\shortcatvariables}}{\shortcatvariables}
        = \bigotimes_{\catenumeratorin} \tbasisat{\catvariableof{\catenumerator}}
        \andspace
        \contractionof{\tbasisat{\headvariable},\bencodingofat{\lnot}{\headvariable,\catvariable}}{\catvariable}
        = \fbasisat{\catvariable} \, .
    \end{align*}
\end{lemma}
\begin{proof}
    Follows directly from the definitions of the basis encodings and the connectives.
\end{proof}

\input{../examples/logical_paradigm/prop_formula_kb_head_sym}

We use this to decompose knowledge bases into their individual formulas as follows.

\begin{theorem}
    \label{the:kbDecomposition}
    For any knowledge base $\kbwith = \bigwedge_{\selindexin} \formulaofat{\selindex}{\shortcatvariables}$ it holds that
    \begin{align*}
        \kbwith
        = \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    With \lemref{lem:comNetSymmetries} we have
    \begin{align*}
        \kbwith
        &= \contractionof{\{\tbasisat{\headvariableof{\land}},\bencodingofat{\land}{\headvariableof{\land},\headvariables}\}
            \cup \{\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{
            \bigcup_{\selindexin} \{\tbasisat{\headvariableof{\selindex}},\bencodingofat{\formulaof{\selindex}}{\headvariableof{\selindex},\shortcatvariables}\wcols \selindexin\}}{\shortcatvariables} \\
        &= \contractionof{\{\formulaofat{\selindex}{\shortcatvariables} \wcols \selindexin\}}{\shortcatvariables} \, .
    \end{align*}
\end{proof}

\input{../examples/logical_paradigm/sudoku_decomposition}

\subsection{Entailment Decision by Message-Passing}

% Infeasible contractions
Since contracting the whole tensor network is often infeasible, local contractions can be considered to decide entailment in some cases.
Here a local contraction describes the calculation of contractions along few closely connected tensors in the network.
Before presenting the resulting Constraint Propagation algorithm, we first show two important properties of local entailment motivating the procedure.

\begin{theorem}[Monotonicity of Propositional Logics]
    \label{the:monotonicityPL}
    If $\seckb\subset\kb$ and $\seckb\models\formula$ then also $\kb\models\formula$.
\end{theorem}
\begin{proof}
    Since $\seckb\models\formula$ it holds that $\contraction{\seckb[\shortcatvariables],\lnot\formula[\shortcatvariables]}=0$ and thus
    \begin{align*}
        \contractionof{\seckb[\shortcatvariables],\lnot\formula[\shortcatvariables]}{\shortcatvariables}=\zerosat{\shortcatvariables} \, .
    \end{align*}
    Denoting by $\kb/\seckb$ the conjunctions of formulas in $\kb$ not in $\seckb$, we have
    \begin{align*}
        \contraction{\kbwith,\lnot\formulawith}
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\seckb[\shortcatvariables],\lnot\formulawith} \\
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\contractionof{\seckb[\shortcatvariables],\lnot\formulawith}{\shortcatvariables}} \\
        &= \contraction{(\kb/\seckb)[\shortcatvariables],\zerosat{\shortcatvariables}} \\
        &= 0 \, .
    \end{align*}
\end{proof}
To decide entailment, we can therefore investigate entailment on smaller parts of the knowledge base.
This is sound by the above theorem, but not complete, since it can happen that no smaller part of the knowledge base entails the formula, but the whole knowledge base does.
We can futhermore add entailed formulas to the knowledge base without the latter, as we show next.

\begin{theorem}[Invariance of adding Entailed Formulas]
    \label{the:addingEntailed}
    If and only if $\kb\models\formula$ we have
    \begin{align*}
        \kbwith
        = \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We use that $\formulawith+\lnot\formulawith=\onesat{\shortcatvariables}$ and thus
    \begin{align*}
        \kbwith
        &= \contractionof{\kbwith,(\formulawith+\lnot\formulawith)}{\shortcatvariables} \\
        &= \contractionof{\kbwith,\formulawith}{\shortcatvariables}  + \contractionof{\kbwith,\lnot\formulawith}{\shortcatvariables}  \\
        %&= \contractionof{\kbwith,\formulawith}{\shortcatvariables} \, .
    \end{align*}
    Since $\contractionof{\kbwith,\lnot\formulawith}{\shortcatvariables}$ is boolean, we thus have that
    \begin{align*}
        \kbwith=\contractionof{\kbwith,\formulawith}{\shortcatvariables}
    \end{align*}
    if and only if $\contraction{\kbwith,\lnot\formulawith}=0$, that is $\kb\models\formula$.
\end{proof}

% Interpreting entailment
The mechanism of \theref{the:addingEntailed} provides us with a mean to store entailment information in small order auxiliary tensors.
%Adding deduced statements to a knowledge base does not change the knowledge base as a tensor, but one can exploit it in smaller contractions.
% Constraint Propagation
One way to exploit this accessibility of local entailment information are message passing schemes similar to \algoref{alg:treeBeliefPropagation} propagating the information.
This approach decides local entailment by iteratively adding entailed formulas to the knowledge base and checking further entailment on neighbored tensors of the knowledge base.
Since for entailment decisions the support of the contractions is sufficient, we can apply non-zero indicators before sending contraction messages.
We then schedule new messages in the direction $(\sedge,\redge)$, once the support of a message received at $\sedge$ has been changed.
Note that such a scheduling system is guaranteed to converge, since there can only be a finite number of message changes.
We further directly reduce the computation of messages to their support and call the resulting \algoref{alg:constraintPropagation} Constraint Propagation.

\begin{algorithm}[hbt!]
    \caption{Constraint Propagation}\label{alg:constraintPropagation}
    \begin{algorithmic}
        \Require Tensor network $\extnet$ on a hypergraph $\graph$
        \Ensure Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$ containing entailment statements
        \iosepline
        \State Initialize a queue $\scheduler = \dirovedges$ of message directions
        \State Initialize messages $\messagewith = \onesat{\catvariableof{\sedge\cap\redge}}$ for $(\sedge,\redge)\in\dirovedges$
        \While{$\scheduler$ not empty}
            \State Pop a $(\sedge,\redge)$ pair from $\scheduler$
            \State Update the message
            \begin{align*}
                \messagewith
                = \nonzeroof{\contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
                    \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
                }{\catvariableof{\sedge\cap \redge}}}
            \end{align*}
            \If{$\hypercoreat{\catvariableof{\sedge\cap \redge}}\neq\messagewith$}
                \State Update the message: $\messagewith\coloneqq\hypercoreat{\catvariableof{\sedge\cap \redge}}$
                \State Add $\scheduler = \scheduler \cup \{(\redge,\secsedge) \wcols (\redge,\secsedge)\in\dirovedges\}$ % Clear?
            \EndIf
        \EndWhile
        \State \Return Messages $\{\messagewith\wcols(\secsedge,\sedge)\in\dirovedges\}$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{the:constraintPropagationSoundness}
    All messages during constraint propagation are sound, that is for all $(\sedge,\redge)\in\dirovedges$ it holds that
    \begin{align*}
        \nonzeroof{\contractionof{\extnet}{\catvariableof{\sedge\cap\redge}}} \prec \messagewith \, .
    \end{align*}
\end{theorem}
\begin{proof}
    We show this theorem by induction over the $\mathrm{While}$ loop of \algoref{alg:constraintPropagation}.
    At the first iteration, we have for all messages $\messagewith=\onesat{\catvariableof{\sedge\cap\redge}}$ and thus
    \begin{align}
        \label{eq:nzMessageAddingEquivalence}
        \extnet = \contractionof{\{\extnet\}\cup\{\messagewith\wcols(\sedge,\redge)\in\dirovedges\}}{\nodevariables} \, .
    \end{align}
    By \theref{the:monotonicityPL} we then have for the first message send along the pair $(\sedge,\redge)$ that
    \begin{align*}
        \nonzeroof{\contractionof{\extnet}{\catvariableof{\sedge\cap\redge}}} \prec
        &\nonzeroof{\contractionof{\{\hypercoreofat{\sedge}{\catvariableof{\sedge}}\}
            \cup \{\mesfromtoat{\secsedge}{\sedge}{\catvariableof{\secsedge\cap\sedge}} \wcols (\secsedge,\sedge)\in\dirovedges \ncond \secsedge\neq \redge\}
        }{\catvariableof{\sedge\cap \redge}}} \\
        &= \messagewith \, .
    \end{align*}

    Let us now assume that at an arbitrary state of the algorithm the inequality holds for all previous sent messages.
    By \theref{the:addingEntailed} we can add contract the messages on the tensor network without changing it, and \eqref{eq:nzMessageAddingEquivalence} thus still holds.
    We then conclude with \theref{the:monotonicityPL} that the claimed property also holds for the new message.
\end{proof}

\input{../examples/logical_paradigm/sudoku_message_passing.tex}